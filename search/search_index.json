{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Object Centric Learning Framework (OCLF)","text":""},{"location":"#what-is-oclf","title":"What is OCLF?","text":"<p>OCLF (Object Centric Learning framework) is a framework designed to ease running experiments for object centric learning research, yet is not limited to this use case.  At its heart lies the idea that while code is not typically composable many experiments in machine learning very similar with minor changes and only represent minor changes.</p> <p>One such example is multi-task training where a model might be trained to solve multiple tasks at the same time.  Different ablations of said model would then contain different model components but largely remain the same.</p> <p>OCLF allows for such ablations without creating duplicate code by defining models and experiments in configuration files and allowing their composition in configuration space via hydra.</p>"},{"location":"#quickstart-development-setup","title":"Quickstart - Development setup","text":"<p>Installing OCLF requires at least python3.8. Installation can be done using poetry.  After installing <code>poetry</code>, check out the repo and setup a development environment:</p> <pre><code>git clone https://github.com/amazon-science/object-centric-learning-framework.git\ncd object-centric-learning-framework\npoetry install\n</code></pre> <p>This installs the <code>ocl</code> package and the cli scripts used for running experiments in a poetry managed virtual environment.</p> <p>Next we need to prepare a dataset.  For this follow the steps below to install the dependencies needed for dataset conversion and creation.</p> <pre><code>cd scripts/datasets\npoetry install\nbash download_and_convert.sh movi_c\n</code></pre> <p>This should create a webdataset in the path <code>scripts/datasets/outputs/movi_c</code>.</p> <p>After exposing this dataset to OCLF, a first experiment can be run:</p> <pre><code>cd ../..   # Go back to root folder\nexport DATASET_PREFIX=scripts/datasets/outputs  # Expose dataset path\npoetry run ocl_train +experiment=slot_attention/movi_c # Run training exeriment\n</code></pre> <p>The output of the training run should be stored at <code>outputs/slot_attention/movi_c/&lt;timestamp&gt;</code>.</p> <p>Note</p> <p>For a more detailed guide on how to install, setup, and use OCLF check out the Tutorial</p>"},{"location":"#citation","title":"Citation","text":"<p>If you use OCLF to run experiments in your work please cite it using the bibtex entry below</p> <pre><code>@misc{oclf,\nauthor = {Max Horn and Maximilian Seitzer and Andrii Zadaianchuk and Zixu Zhao and Dominik Zietlow and Florian Wenzel and Tianjun Xiao},\ntitle = {Object Centric Learning Framework (version 0.1)},\nyear  = {2023},\nurl   = {https://github.com/amazon-science/object-centric-learning-framework},\n}\n</code></pre>"},{"location":"#publications","title":"Publications","text":"<p>Experiments for the following publications where run using OCLF. Please feel free to add your own experiments via pull requests and to list them below.</p> <ul> <li>M.Seitzer et al., Bridging the Gap to Real-World Object-Centric Learning     training configurations evaluation configurations</li> </ul>"},{"location":"#license","title":"License","text":"<p>This project is licensed under the Apache-2.0 License.</p>"},{"location":"#contributing","title":"Contributing","text":"<p>We are happy to accept code contributions in the form of pull-requests and kindly ask contributors to follow the guidance provided below and in <code>CONTRIBUTING.md</code>.</p> <p>We are using <code>pre-commit</code> to manage automatic code formatting and linting. For someone who has never worked with pre-commit, this can be a bit unusual. <code>pre-commit</code> works by setting up a Git commit hook that runs before each <code>git commit</code>. The hook executes a set of tests and automatic formatting on all files that are modified by the commit: - If a file does not pass a test, the commit is aborted and you are required to   fix the problems, <code>git add</code> the files and run <code>git commit</code> again. - If a file is automatically formatted, the commit is also aborted. You can   review the proposed changes using <code>git diff</code>, accept them with <code>git add</code> and   run <code>git commit</code> again.</p> <p>It can also make sense to manually run the hooks on all files in the repository (using <code>pre-commit run -a</code>) before committing, to make sure the commit passes. Note that this does not run the hooks on files which are not yet commited to the repository.</p> <p>Important: make sure to run <code>pre-commit</code> within the environment installed by <code>poetry</code>. Otherwise the checks might fail because the tools are not installed, or use different versions from the ones specified in <code>poetry.lock</code>.</p>"},{"location":"api/SUMMARY/","title":"SUMMARY","text":"<ul> <li>ocl<ul> <li>callbacks</li> <li>cli<ul> <li>_config</li> <li>cli_utils</li> <li>compute_dataset_size</li> <li>dump_movi_dataset</li> <li>eval</li> <li>eval_cluster_metrics</li> <li>eval_probing_metrics</li> <li>eval_utils</li> <li>run_bridging_eval</li> <li>train</li> </ul> </li> <li>combined_model</li> <li>conditioning</li> <li>data_decoding</li> <li>datasets</li> <li>decoding</li> <li>feature_extractors<ul> <li>clip</li> <li>misc</li> <li>timm</li> <li>utils</li> </ul> </li> <li>losses</li> <li>matching</li> <li>metrics<ul> <li>bbox</li> <li>dataset</li> <li>diagnosis</li> <li>masks</li> <li>tracking</li> <li>utils</li> </ul> </li> <li>models<ul> <li>savi</li> </ul> </li> <li>neural_networks<ul> <li>convenience</li> <li>extensions</li> <li>feature_pyramid_networks</li> <li>positional_embedding</li> <li>slate</li> <li>wrappers</li> </ul> </li> <li>optimization</li> <li>perceptual_grouping</li> <li>preprocessing</li> <li>scheduling</li> <li>transforms</li> <li>typing</li> <li>utils<ul> <li>bboxes</li> <li>dataset_patches</li> <li>logging</li> <li>masking</li> <li>resizing</li> <li>routing</li> <li>trees</li> <li>windows</li> </ul> </li> <li>visualization_types</li> <li>visualizations</li> </ul> </li> <li>routed</li> </ul>"},{"location":"api/routed/","title":"Routing","text":"<p>Magic routed module which allows dynamically routing.</p> <p>This module is used to wrap classes from arbitrary packages into Routable classes. A routable class is augmented with additional constructor parameters that determine on which elements of a PyTree the methods</p> <ul> <li><code>update</code> (for classes of type torchmetrics.Metric)</li> <li><code>forward</code> (for clases of type torch.nn.Module) or</li> <li><code>__call__</code> (for all other cases)</li> </ul> <p>should be applied.</p> <p>This is acomplished using a simple trick: Instead of passing the individual parameters to methods of the class, the original method is wrapped. This wrapped method, then selects the desired input arguments from a <code>inputs</code> argument and forwards these to the original class implementation of the method.</p> Example <pre><code>import torch\nimport routed\nnon_routed_class = torch.nn.Sigmoid()\nrouted_class = routed.torch.nn.Sigmoid(input_path=\"my_sigmoid_source\")\nexample_tensor = torch.randn(100)\ninputs = {\n\"my_sigmoid_source\": example_tensor\n}\nassert torch.allclose(non_routed_class(example_tensor), routed_class(inputs=inputs))\n</code></pre>"},{"location":"api/routed/#routed.RoutedClass","title":"<code>RoutedClass</code>","text":"<p>Class used to dynamically subclass routed classes.</p> <p>Any subclasses of this class are automatically patched to support routing of input arguments.</p> <p>Attributes:</p> Name Type Description <code>input_mapping</code> <code>Dict[str, List[str]]</code> <p>Mapping from parameters of routed functions to paths in the inputs dict.</p> Source code in <code>routed/__init__.py</code> <pre><code>class RoutedClass:\n\"\"\"Class used to dynamically subclass routed classes.\n    Any subclasses of this class are automatically patched to support routing of input arguments.\n    Attributes:\n        input_mapping: Mapping from parameters of routed functions to paths in the inputs dict.\n    \"\"\"\ninput_mapping: Dict[str, List[str]]\ndef __init__(self, *args, **kwargs):\nself._remove_routed_parameters(kwargs)\nsuper().__init__(*args, **kwargs)\ndef __new__(cls, *args, **kwargs):\n# Patch routed methods.\n# This needs to be done here as they are otherwise not considered methods.\n_routed_methods = _get_routed_methods(cls)\ninput_mapping = {}\nfor method_name in _routed_methods:\norg_method = getattr(cls, method_name)\nfor name in inspect.signature(org_method).parameters:\npath_name = f\"{name}_path\"\nif path_name in kwargs:\ninput_mapping[name] = kwargs[path_name].split(\".\")\nsetattr(cls, method_name, build_routed_method(org_method))\ninstance = super().__new__(cls)\ninstance.input_mapping = input_mapping\nreturn instance\ndef _remove_routed_parameters(self, kwargs: Dict[str, Any]):\nfor param in self.input_mapping:\npath = f\"{param}_path\"\nif path in kwargs:\ndel kwargs[path]\n</code></pre>"},{"location":"api/routed/#routed.WrappedModule","title":"<code>WrappedModule</code>","text":"<p>         Bases: <code>types.ModuleType</code></p> <p>Module which automatically patches all classes within it to support routing.</p> Source code in <code>routed/__init__.py</code> <pre><code>class WrappedModule(types.ModuleType):\n\"\"\"Module which automatically patches all classes within it to support routing.\"\"\"\ndef __init__(self, path: str, module):\nsuper().__init__(path, f\"Module with routed versions of {path}\")\nself.path = path\nself.module = module\ndef __getattr__(self, name):\ntry:\nimported = getattr(self.module, name)\nexcept AttributeError:\nimported = importlib.import_module(f\"{self.path}.{name}\")\nif isinstance(imported, types.ModuleType):\nreturn WrappedModule(f\"{self.path}.{name}\", imported)\nreturn type(f\"{self.path}.Routed{name}\", (RoutedClass, imported), {})\n</code></pre>"},{"location":"api/routed/#routed.build_routed_method","title":"<code>build_routed_method</code>","text":"<p>Pass arguments to a function based on the mapping defined in <code>self.input_mapping</code>.</p> <p>This method supports both filtering for parameters that match the arguments of the wrapped method and passing all arguments defined in <code>input_mapping</code>.  If a non-optional argument is missing this will raise an exception.  Additional arguments can also be passed to the method to override entries in the input dict.  Non-keyword arguments are always directly passed to the method.</p> <p>Parameters:</p> Name Type Description Default <code>method</code> <code>types.MethodType</code> <p>The method to pass the arguments to.</p> required <code>filter_parameters</code> <code>bool</code> <p>Only pass arguments to wrapped method that match the methods signature.  This is practical if different methods require different types of input.</p> <code>True</code> Source code in <code>routed/__init__.py</code> <pre><code>def build_routed_method(\nmethod: types.MethodType, filter_parameters: bool = True\n) -&gt; types.MethodType:\n\"\"\"Pass arguments to a function based on the mapping defined in `self.input_mapping`.\n    This method supports both filtering for parameters that match the arguments of the wrapped\n    method and passing all arguments defined in `input_mapping`.  If a non-optional argument is\n    missing this will raise an exception.  Additional arguments can also be passed to the method\n    to override entries in the input dict.  Non-keyword arguments are always directly passed to\n    the method.\n    Args:\n        method: The method to pass the arguments to.\n        filter_parameters: Only pass arguments to wrapped method that match the methods\n            signature.  This is practical if different methods require different types of input.\n    \"\"\"\n# Run inspection here to reduce compute time when calling method.\nsignature = inspect.signature(method)\nvalid_parameters = list(signature.parameters)  # Returns the parameter names.\nvalid_parameters = valid_parameters[1:]  # Discard \"self\".\n# Keep track of default parameters. For these we should not fail if they are not in\n# the input dict.\nwith_defaults = [\nname\nfor name, param in signature.parameters.items()\nif param.default is not inspect.Parameter.empty\n]\n@functools.wraps(method)\ndef method_with_routing(self: RoutedClass, *args, inputs=None, **kwargs):\nif not inputs:\ninputs = {}\nif self.input_mapping:\nif not inputs:  # Empty dict.\ninputs = kwargs\nrouted_inputs = {}\nfor input_field, input_path in self.input_mapping.items():\nif filter_parameters and input_field not in valid_parameters:\n# Skip parameters that are not the function signature.\ncontinue\nif input_field in kwargs.keys():\n# Skip parameters that are directly provided as kwargs.\ncontinue\ntry:\nelement = tree_utils.get_tree_element(inputs, input_path)\nrouted_inputs[input_field] = element\nexcept ValueError as e:\nif input_field in with_defaults:\ncontinue\nelse:\nraise e\n# Support for additional parameters passed via keyword arguments.\n# TODO(hornmax): This is not ideal as it mixes routing args from the input dict\n# and explicitly passed kwargs and thus could lead to collisions.\nfor name, element in kwargs.items():\nif filter_parameters and name not in valid_parameters:\ncontinue\nelse:\nrouted_inputs[name] = element\nreturn method(self, *args, **routed_inputs)\nelse:\nreturn method(self, *args, **kwargs)\nreturn method_with_routing\n</code></pre>"},{"location":"api/ocl/callbacks/","title":"ocl.callbacks","text":""},{"location":"api/ocl/callbacks/#ocl.callbacks.FreezeParameters","title":"<code>FreezeParameters</code>","text":"<p>         Bases: <code>Callback</code></p> <p>Freeze parameters of model prior to training.</p> Source code in <code>ocl/callbacks.py</code> <pre><code>class FreezeParameters(Callback):\n\"\"\"Freeze parameters of model prior to training.\"\"\"\ndef __init__(self, parameter_groups: List[Dict[str, Any]]):\n\"\"\"Initialize FreezeParameters callback.\n        Args:\n            parameter_groups: Parameter groups that should be frozen.\n                Uses same syntax as [ocl.optimization.OptimizationWrapper][]\n        \"\"\"\nsuper().__init__()\nself.parameter_group_specs = parameter_groups\nfor idx, param_group_spec in enumerate(self.parameter_group_specs):\nif \"params\" not in param_group_spec:\nraise ValueError(f'Parameter group {idx + 1} does not contain key \"params\"')\nparam_spec = param_group_spec[\"params\"]\nif isinstance(param_spec, str):\nparam_group_spec[\"params\"] = [param_spec]\nelif isinstance(param_spec, Iterable):\nparam_group_spec[\"params\"] = list(param_spec)\nelse:\nraise ValueError(\nf'\"params\" for parameter group {idx + 1} is not of type str or iterable'\n)\nif \"predicate\" in param_group_spec:\nif not callable(param_group_spec[\"predicate\"]):\nraise ValueError(f'\"predicate\" for parameter group {idx + 1} is not a callable')\ndef _get_parameters_to_freeze(self, model):\n\"\"\"Build parameter groups from specification.\"\"\"\nparameters_to_freeze = []\nfor param_group_spec in self.parameter_group_specs:\nfor current_params in param_group_spec[\"params\"]:\nparam_path = current_params.split(\".\")\n# Default predicate includes all parameters\npredicate = param_group_spec.get(\"predicate\", lambda name, param: True)\nparam = get_tree_element(model, param_path)\nif isinstance(param, nn.Module):\nparameters_to_freeze.extend(\nparam for name, param in param.named_parameters() if predicate(name, param)\n)\nelif isinstance(param, nn.Parameter):\nparameters_to_freeze.append(param)\nelse:\nraise ValueError(\n\"Object at path {'.'.join(param_path)} is neither nn.Module nor nn.Parameter\"\n)\nreturn parameters_to_freeze\ndef on_fit_start(self, trainer, model: nn.Module):\nparameters_to_freeze = self._get_parameters_to_freeze(model)\nfor param in parameters_to_freeze:\nparam.requires_grad_(False)\n</code></pre>"},{"location":"api/ocl/callbacks/#ocl.callbacks.FreezeParameters.__init__","title":"<code>__init__</code>","text":"<p>Initialize FreezeParameters callback.</p> <p>Parameters:</p> Name Type Description Default <code>parameter_groups</code> <code>List[Dict[str, Any]]</code> <p>Parameter groups that should be frozen. Uses same syntax as ocl.optimization.OptimizationWrapper</p> required Source code in <code>ocl/callbacks.py</code> <pre><code>def __init__(self, parameter_groups: List[Dict[str, Any]]):\n\"\"\"Initialize FreezeParameters callback.\n    Args:\n        parameter_groups: Parameter groups that should be frozen.\n            Uses same syntax as [ocl.optimization.OptimizationWrapper][]\n    \"\"\"\nsuper().__init__()\nself.parameter_group_specs = parameter_groups\nfor idx, param_group_spec in enumerate(self.parameter_group_specs):\nif \"params\" not in param_group_spec:\nraise ValueError(f'Parameter group {idx + 1} does not contain key \"params\"')\nparam_spec = param_group_spec[\"params\"]\nif isinstance(param_spec, str):\nparam_group_spec[\"params\"] = [param_spec]\nelif isinstance(param_spec, Iterable):\nparam_group_spec[\"params\"] = list(param_spec)\nelse:\nraise ValueError(\nf'\"params\" for parameter group {idx + 1} is not of type str or iterable'\n)\nif \"predicate\" in param_group_spec:\nif not callable(param_group_spec[\"predicate\"]):\nraise ValueError(f'\"predicate\" for parameter group {idx + 1} is not a callable')\n</code></pre>"},{"location":"api/ocl/callbacks/#ocl.callbacks.RestoreParameterSubset","title":"<code>RestoreParameterSubset</code>","text":"<p>         Bases: <code>Callback</code></p> <p>Restore a subset of parameters using a checkpoint form a different model.</p> Source code in <code>ocl/callbacks.py</code> <pre><code>class RestoreParameterSubset(Callback):\n\"\"\"Restore a subset of parameters using a checkpoint form a different model.\"\"\"\ndef __init__(self, checkpoint_file: str, target_path: str, source_path: Optional[str] = None):\n\"\"\"Initialize RestoreParameterSubset callback.\n        Args:\n            checkpoint_file: File from which the model weights should be loaded.\n            target_path: The path in the model where the model weights should be\n                restored.  This should follow a dot separated syntax, such a `encoder.layer1`.\n            source_path: The path in the checkpoint_file that should be used to restore weights.\n                If none provided assumes to be the same as `target_path`.\n        \"\"\"\nself.checkpoint_file = checkpoint_file\nself.target_path = target_path\nself.source_path = source_path if source_path else self.target_path\ndef on_fit_start(self, trainer, model: nn.Module):\nif model.global_step != 0:\n# Don't restore when we are resuming training.\nrank_zero_warn(\"Not restoring parameter subset as training is being resumed\")\nreturn\n# Get parameters from state dict\nstate_dict = torch.load(self.checkpoint_file, map_location=model.device)[\"state_dict\"]\n# Add offset of 1 to remove potential dot.\noffset_keys = len(self.source_path) + 1\nstate_dict = {\nkey[offset_keys:]: value\nfor key, value in state_dict.items()\nif key.startswith(self.source_path)\n}\n# Get module from model\nmodel_component: nn.Module = get_tree_element(model, self.target_path.split(\".\"))\nresult = model_component.load_state_dict(state_dict)\nif len(result.missing_keys):\nrank_zero_warn(\nf\"Mismatch between state dict and model. Missing keys: {result.missing_keys}\"\n)\nif len(result.unexpected_keys):\nrank_zero_warn(\nf\"Mismatch between state dict and model. Unexpected keys: {result.missing_keys}\"\n)\nrank_zero_info(f\"Restored subset of model parameters from {self.checkpoint_file}\")\n</code></pre>"},{"location":"api/ocl/callbacks/#ocl.callbacks.RestoreParameterSubset.__init__","title":"<code>__init__</code>","text":"<p>Initialize RestoreParameterSubset callback.</p> <p>Parameters:</p> Name Type Description Default <code>checkpoint_file</code> <code>str</code> <p>File from which the model weights should be loaded.</p> required <code>target_path</code> <code>str</code> <p>The path in the model where the model weights should be restored.  This should follow a dot separated syntax, such a <code>encoder.layer1</code>.</p> required <code>source_path</code> <code>Optional[str]</code> <p>The path in the checkpoint_file that should be used to restore weights. If none provided assumes to be the same as <code>target_path</code>.</p> <code>None</code> Source code in <code>ocl/callbacks.py</code> <pre><code>def __init__(self, checkpoint_file: str, target_path: str, source_path: Optional[str] = None):\n\"\"\"Initialize RestoreParameterSubset callback.\n    Args:\n        checkpoint_file: File from which the model weights should be loaded.\n        target_path: The path in the model where the model weights should be\n            restored.  This should follow a dot separated syntax, such a `encoder.layer1`.\n        source_path: The path in the checkpoint_file that should be used to restore weights.\n            If none provided assumes to be the same as `target_path`.\n    \"\"\"\nself.checkpoint_file = checkpoint_file\nself.target_path = target_path\nself.source_path = source_path if source_path else self.target_path\n</code></pre>"},{"location":"api/ocl/callbacks/#ocl.callbacks.UpdateHyperparameterScheduling","title":"<code>UpdateHyperparameterScheduling</code>","text":"<p>         Bases: <code>Callback</code></p> <p>Callback to update hyperparameter schedulers found <code>ocl.scheduling</code>.</p> Source code in <code>ocl/callbacks.py</code> <pre><code>class UpdateHyperparameterScheduling(Callback):\n\"\"\"Callback to update hyperparameter schedulers found `ocl.scheduling`.\"\"\"\ndef __init__(self):\nself._hyperparameter_schedulers: List[HPScheduler] = []\ndef on_fit_start(self, trainer, pl_module):\ndel trainer\nself._hyperparameter_schedulers = list(\nmap(\nlambda a: a[1],\nwalk_tree_with_paths(pl_module, instance_check=lambda t: isinstance(t, HPScheduler)),\n)\n)\n# Set global step to 0 for pretraining evaluation routine.\nself._update_schedulers(0)\ndef _update_schedulers(self, step):\nif len(self._hyperparameter_schedulers) == 0:\nrank_zero_warn(\n\"UpdateHyperparameterScheduling: \"\n\"No schedulable hyperparameters where found in model.\"\n)\nfor hparam in self._hyperparameter_schedulers:\nhparam.update_global_step(step)\ndef on_train_batch_start(self, trainer, pl_module, batch, batch_idx):\ndel trainer, batch, batch_idx\nglobal_step = pl_module.global_step\nself._update_schedulers(global_step)\n</code></pre>"},{"location":"api/ocl/callbacks/#ocl.callbacks.SetEpochEnvironmentVariable","title":"<code>SetEpochEnvironmentVariable</code>","text":"<p>         Bases: <code>Callback</code></p> <p>Sets environment variable <code>EPOCH</code> which is used by ocl.transforms.SampleSlices.</p> Source code in <code>ocl/callbacks.py</code> <pre><code>class SetEpochEnvironmentVariable(Callback):\n\"\"\"Sets environment variable `EPOCH` which is used by [ocl.transforms.SampleSlices][].\"\"\"\ndef on_train_epoch_start(self, trainer, pl_module):\nos.environ[\"EPOCH\"] = str(pl_module.current_epoch)\n</code></pre>"},{"location":"api/ocl/combined_model/","title":"ocl.combined_model","text":"<p>Implementation of combined model.</p>"},{"location":"api/ocl/combined_model/#ocl.combined_model.CombinedModel","title":"<code>CombinedModel</code>","text":"<p>         Bases: <code>pl.LightningModule</code></p> <p>Core pytorch lightning model used for training, loss compuation and visualization.</p> Source code in <code>ocl/combined_model.py</code> <pre><code>class CombinedModel(pl.LightningModule):\n\"\"\"Core pytorch lightning model used for training, loss compuation and visualization.\"\"\"\ndef __init__(\nself,\nmodels: Union[Dict[str, Any], nn.Module],\noptimizers: Dict[str, Union[OptimizationWrapper, Callable]],\nlosses: Dict[str, Any],\nvisualizations: Dict[str, VisualizationMethod],\ntraining_metrics: Optional[Dict[str, torchmetrics.Metric]] = None,\nevaluation_metrics: Optional[Dict[str, torchmetrics.Metric]] = None,\nvis_log_frequency: int = 100,\n):\n\"\"\"Initialize combined model.\n        Args:\n            models: The model to run the forward pass.  If a dict is provieded the\n                modules of the dict are wrapped with [ocl.utils.routing.Combined][].\n            optimizers: Dictionary of partial optimizer objects or OptimizationWrappers.\n            losses: Dictionary of losses. The key is used to track the loss value during\n                logging, the sum of all losses is used to optimize the model.\n            visualizations: Visualizations for visualizing and monitoring training progress.\n            training_metrics: Metrics to evaluate during training.\n            evaluation_metrics: Metrics to evaluate during validation and testing.\n            vis_log_frequency: Frequency in optimization steps when to run visualizations.\n        \"\"\"\nsuper().__init__()\nif isinstance(models, Dict):\nmodels = Combined(**models)\nself.models = models\nself.optimizers = optimizers\nself.losses = torch.nn.ModuleDict(losses)\nself.visualizations = visualizations\nself.vis_log_frequency = vis_log_frequency\nself.return_outputs_on_validation = False\nif training_metrics is None:\ntraining_metrics = {}\nself.training_metrics = torch.nn.ModuleDict(training_metrics)\nif evaluation_metrics is None:\nevaluation_metrics = {}\nself.evaluation_metrics = torch.nn.ModuleDict(evaluation_metrics)\ndef _build_optimizer(self, optimizer: Union[OptimizationWrapper, Callable]):\nif isinstance(optimizer, OptimizationWrapper):\nreturn optimizer(self)\n# Support using a partial of a standard pytorch optimizer.\nreturn optimizer(self.parameters())\ndef configure_optimizers(self):\nreturn [self._build_optimizer(self.optimizers[name]) for name in sorted(self.optimizers)]\ndef forward(self, input_data: dict):\n# Maybe we should use something like a read only dict to prevent existing keys from being\n# overwritten.\ndata: Dict[str, Any]\ndata = {\n\"input\": input_data,\n# TODO(hornmax): Figure out if there is a better way to acces multi-gpu operations.\n\"model\": self,\n}\nreturn self.models(inputs=data)\ndef _compute_losses(self, inputs, phase=\"train\"):\nquantities_to_log = {}\n# We write additional loss outputs directly into the inputs dict, and thus do not need to\n# return them.\noutputs = inputs[\"losses\"] = {}\nfor name, loss in self.losses.items():\nout = loss(inputs=inputs)\nif isinstance(out, tuple):\n# Additional outputs that should be logged for later access.\n# Some visualizations require having access to loss quantities, thus we need to save\n# them for later here.\nout, additional_outputs = out\noutputs[name] = additional_outputs\nquantities_to_log[f\"{phase}/{name}\"] = out\nlosses = []\nfor loss in quantities_to_log.values():\nlosses.append(loss)\ntotal_loss = torch.stack(losses).sum()\n# Log total loss only if there is more than one task\nif len(losses) &gt; 1:\nquantities_to_log[f\"{phase}/loss_total\"] = total_loss\nreturn total_loss, quantities_to_log\ndef predict_step(self, batch, batch_idx):\noutputs = self(batch)\n# Remove things not needed in prediction output.\ndel outputs[\"model\"]\nreturn outputs\ndef training_step(self, batch, batch_idx):\nbatch_size = batch[\"batch_size\"]\noutputs = self(batch)\ntotal_loss, quantities_to_log = self._compute_losses(outputs)\nquantities_to_log.update(self._compute_metrics(outputs, self.training_metrics))\nself.log_dict(quantities_to_log, on_step=True, on_epoch=False, batch_size=batch_size)\nif self.trainer.global_step % self.vis_log_frequency == 0:\nself._log_visualizations(outputs)\nreturn total_loss\ndef validation_step(self, batch, batch_idx):\nbatch_size = batch[\"batch_size\"]\noutputs = self(batch)\ntotal_loss, quantities_to_log = self._compute_losses(outputs, phase=\"val\")\nquantities_to_log.update(\nself._compute_metrics(outputs, self.evaluation_metrics, phase=\"val\")\n)\nself.log_dict(\nquantities_to_log, on_step=False, on_epoch=True, prog_bar=True, batch_size=batch_size\n)\nif batch_idx == 0:\nself._log_visualizations(outputs, phase=\"val\")\nif self.return_outputs_on_validation:\nreturn outputs  # Used for saving model outputs during eval\nelse:\nreturn None\ndef _compute_metrics(self, outputs, metric_fns, phase=\"train\"):\nmetrics = {}\nif len(metric_fns) &gt; 0:\nfor metric_name, metric in metric_fns.items():\nif phase == \"val\":\n# Call update instead of forward to avoid unnecessary metric compute on batch.\nmetric.update(**outputs)\nelse:\nmetric(**outputs)\nmetrics[f\"{phase}/{metric_name}\"] = metric\nreturn metrics\ndef _log_visualizations(self, outputs, phase=\"train\"):\nif self.logger is None:\nreturn\nlogger_experiment = self.logger.experiment\nvisualizations = {}\nfor name, vis in self.visualizations.items():\nvisualizations[name] = vis(inputs=outputs)\nvisualization_iterator = walk_tree_with_paths(\nvisualizations, path=None, instance_check=lambda t: isinstance(t, Visualization)\n)\nfor path, vis in visualization_iterator:\ntry:\nstr_path = \".\".join(path)\nvis.add_to_experiment(\nexperiment=logger_experiment,\ntag=f\"{phase}/{str_path}\",\nglobal_step=self.trainer.global_step,\n)\nexcept AttributeError:\n# The logger does not support the right data format.\npass\n</code></pre>"},{"location":"api/ocl/combined_model/#ocl.combined_model.CombinedModel.__init__","title":"<code>__init__</code>","text":"<p>Initialize combined model.</p> <p>Parameters:</p> Name Type Description Default <code>models</code> <code>Union[Dict[str, Any], nn.Module]</code> <p>The model to run the forward pass.  If a dict is provieded the modules of the dict are wrapped with ocl.utils.routing.Combined.</p> required <code>optimizers</code> <code>Dict[str, Union[OptimizationWrapper, Callable]]</code> <p>Dictionary of partial optimizer objects or OptimizationWrappers.</p> required <code>losses</code> <code>Dict[str, Any]</code> <p>Dictionary of losses. The key is used to track the loss value during logging, the sum of all losses is used to optimize the model.</p> required <code>visualizations</code> <code>Dict[str, VisualizationMethod]</code> <p>Visualizations for visualizing and monitoring training progress.</p> required <code>training_metrics</code> <code>Optional[Dict[str, torchmetrics.Metric]]</code> <p>Metrics to evaluate during training.</p> <code>None</code> <code>evaluation_metrics</code> <code>Optional[Dict[str, torchmetrics.Metric]]</code> <p>Metrics to evaluate during validation and testing.</p> <code>None</code> <code>vis_log_frequency</code> <code>int</code> <p>Frequency in optimization steps when to run visualizations.</p> <code>100</code> Source code in <code>ocl/combined_model.py</code> <pre><code>def __init__(\nself,\nmodels: Union[Dict[str, Any], nn.Module],\noptimizers: Dict[str, Union[OptimizationWrapper, Callable]],\nlosses: Dict[str, Any],\nvisualizations: Dict[str, VisualizationMethod],\ntraining_metrics: Optional[Dict[str, torchmetrics.Metric]] = None,\nevaluation_metrics: Optional[Dict[str, torchmetrics.Metric]] = None,\nvis_log_frequency: int = 100,\n):\n\"\"\"Initialize combined model.\n    Args:\n        models: The model to run the forward pass.  If a dict is provieded the\n            modules of the dict are wrapped with [ocl.utils.routing.Combined][].\n        optimizers: Dictionary of partial optimizer objects or OptimizationWrappers.\n        losses: Dictionary of losses. The key is used to track the loss value during\n            logging, the sum of all losses is used to optimize the model.\n        visualizations: Visualizations for visualizing and monitoring training progress.\n        training_metrics: Metrics to evaluate during training.\n        evaluation_metrics: Metrics to evaluate during validation and testing.\n        vis_log_frequency: Frequency in optimization steps when to run visualizations.\n    \"\"\"\nsuper().__init__()\nif isinstance(models, Dict):\nmodels = Combined(**models)\nself.models = models\nself.optimizers = optimizers\nself.losses = torch.nn.ModuleDict(losses)\nself.visualizations = visualizations\nself.vis_log_frequency = vis_log_frequency\nself.return_outputs_on_validation = False\nif training_metrics is None:\ntraining_metrics = {}\nself.training_metrics = torch.nn.ModuleDict(training_metrics)\nif evaluation_metrics is None:\nevaluation_metrics = {}\nself.evaluation_metrics = torch.nn.ModuleDict(evaluation_metrics)\n</code></pre>"},{"location":"api/ocl/conditioning/","title":"ocl.conditioning","text":"<p>Implementation of conditioning approaches for slots.</p>"},{"location":"api/ocl/conditioning/#ocl.conditioning.RandomConditioning","title":"<code>RandomConditioning</code>","text":"<p>         Bases: <code>nn.Module</code></p> <p>Random conditioning with potentially learnt mean and stddev.</p> Source code in <code>ocl/conditioning.py</code> <pre><code>class RandomConditioning(nn.Module):\n\"\"\"Random conditioning with potentially learnt mean and stddev.\"\"\"\ndef __init__(\nself,\nobject_dim: int,\nn_slots: int,\nlearn_mean: bool = True,\nlearn_std: bool = True,\nmean_init: Callable[[torch.Tensor], None] = torch.nn.init.xavier_uniform_,\nlogsigma_init: Callable[[torch.Tensor], None] = nn.init.xavier_uniform_,\n):\nsuper().__init__()\nself.n_slots = n_slots\nself.object_dim = object_dim\nif learn_mean:\nself.slots_mu = nn.Parameter(torch.zeros(1, 1, object_dim))\nelse:\nself.register_buffer(\"slots_mu\", torch.zeros(1, 1, object_dim))\nif learn_std:\nself.slots_logsigma = nn.Parameter(torch.zeros(1, 1, object_dim))\nelse:\nself.register_buffer(\"slots_logsigma\", torch.zeros(1, 1, object_dim))\nwith torch.no_grad():\nmean_init(self.slots_mu)\nlogsigma_init(self.slots_logsigma)\ndef forward(self, batch_size: int) -&gt; ocl.typing.ConditioningOutput:\nmu = self.slots_mu.expand(batch_size, self.n_slots, -1)\nsigma = self.slots_logsigma.exp().expand(batch_size, self.n_slots, -1)\nreturn mu + sigma * torch.randn_like(mu)\n</code></pre>"},{"location":"api/ocl/conditioning/#ocl.conditioning.LearntConditioning","title":"<code>LearntConditioning</code>","text":"<p>         Bases: <code>nn.Module</code></p> <p>Conditioning with a learnt set of slot initializations, similar to DETR.</p> Source code in <code>ocl/conditioning.py</code> <pre><code>class LearntConditioning(nn.Module):\n\"\"\"Conditioning with a learnt set of slot initializations, similar to DETR.\"\"\"\ndef __init__(\nself,\nobject_dim: int,\nn_slots: int,\nslot_init: Optional[Callable[[torch.Tensor], None]] = None,\n):\n\"\"\"Initialize LearntConditioning.\n        Args:\n            object_dim: Dimensionality of the conditioning vector to generate.\n            n_slots: Number of conditioning vectors to generate.\n            slot_init: Callable used to initialize individual slots.\n        \"\"\"\nsuper().__init__()\nself.n_slots = n_slots\nself.object_dim = object_dim\nself.slots = nn.Parameter(torch.zeros(1, n_slots, object_dim))\nif slot_init is None:\nslot_init = nn.init.normal_\nwith torch.no_grad():\nslot_init(self.slots)\ndef forward(self, batch_size: int) -&gt; ocl.typing.ConditioningOutput:\n\"\"\"Generate conditioining vectors for `batch_size` instances.\n        Args:\n            batch_size: Number of instances to create conditioning vectors for.\n        Returns:\n            The conditioning vectors.\n        \"\"\"\nreturn self.slots.expand(batch_size, -1, -1)\n</code></pre>"},{"location":"api/ocl/conditioning/#ocl.conditioning.LearntConditioning.__init__","title":"<code>__init__</code>","text":"<p>Initialize LearntConditioning.</p> <p>Parameters:</p> Name Type Description Default <code>object_dim</code> <code>int</code> <p>Dimensionality of the conditioning vector to generate.</p> required <code>n_slots</code> <code>int</code> <p>Number of conditioning vectors to generate.</p> required <code>slot_init</code> <code>Optional[Callable[[torch.Tensor], None]]</code> <p>Callable used to initialize individual slots.</p> <code>None</code> Source code in <code>ocl/conditioning.py</code> <pre><code>def __init__(\nself,\nobject_dim: int,\nn_slots: int,\nslot_init: Optional[Callable[[torch.Tensor], None]] = None,\n):\n\"\"\"Initialize LearntConditioning.\n    Args:\n        object_dim: Dimensionality of the conditioning vector to generate.\n        n_slots: Number of conditioning vectors to generate.\n        slot_init: Callable used to initialize individual slots.\n    \"\"\"\nsuper().__init__()\nself.n_slots = n_slots\nself.object_dim = object_dim\nself.slots = nn.Parameter(torch.zeros(1, n_slots, object_dim))\nif slot_init is None:\nslot_init = nn.init.normal_\nwith torch.no_grad():\nslot_init(self.slots)\n</code></pre>"},{"location":"api/ocl/conditioning/#ocl.conditioning.LearntConditioning.forward","title":"<code>forward</code>","text":"<p>Generate conditioining vectors for <code>batch_size</code> instances.</p> <p>Parameters:</p> Name Type Description Default <code>batch_size</code> <code>int</code> <p>Number of instances to create conditioning vectors for.</p> required <p>Returns:</p> Type Description <code>ocl.typing.ConditioningOutput</code> <p>The conditioning vectors.</p> Source code in <code>ocl/conditioning.py</code> <pre><code>def forward(self, batch_size: int) -&gt; ocl.typing.ConditioningOutput:\n\"\"\"Generate conditioining vectors for `batch_size` instances.\n    Args:\n        batch_size: Number of instances to create conditioning vectors for.\n    Returns:\n        The conditioning vectors.\n    \"\"\"\nreturn self.slots.expand(batch_size, -1, -1)\n</code></pre>"},{"location":"api/ocl/conditioning/#ocl.conditioning.RandomConditioningWithQMCSampling","title":"<code>RandomConditioningWithQMCSampling</code>","text":"<p>         Bases: <code>RandomConditioning</code></p> <p>Random gaussian conditioning using Quasi-Monte Carlo (QMC) samples.</p> Source code in <code>ocl/conditioning.py</code> <pre><code>class RandomConditioningWithQMCSampling(RandomConditioning):\n\"\"\"Random gaussian conditioning using Quasi-Monte Carlo (QMC) samples.\"\"\"\ndef __init__(\nself,\nobject_dim: int,\nn_slots: int,\nlearn_mean: bool = True,\nlearn_std: bool = True,\nmean_init: Callable[[torch.Tensor], None] = torch.nn.init.xavier_uniform_,\nlogsigma_init: Callable[[torch.Tensor], None] = torch.nn.init.xavier_uniform_,\n):\n\"\"\"Initialize RandomConditioningWithQMCSampling.\n        Args:\n            object_dim: Dimensionality of the conditioning vector to generate.\n            n_slots: Number of conditioning vectors to generate.\n            learn_mean: Learn the mean vector of sampling distribution.\n            learn_std: Learn the std vector for sampling distribution.\n            mean_init: Callable to initialize mean vector.\n            logsigma_init: Callable to initialize logsigma.\n        \"\"\"\nsuper().__init__(\nobject_dim=object_dim,\nn_slots=n_slots,\nlearn_mean=learn_mean,\nlearn_std=learn_std,\nmean_init=mean_init,\nlogsigma_init=logsigma_init,\n)\nimport scipy.stats  # Import lazily because scipy takes some time to import\nself.randn_rng = scipy.stats.qmc.MultivariateNormalQMC(mean=np.zeros(object_dim))\ndef _randn(self, *args: Tuple[int]) -&gt; torch.Tensor:\nn_elements = np.prod(args)\n# QMC sampler needs to sample powers of 2 numbers at a time\nn_elements_rounded2 = 2 ** int(np.ceil(np.log2(n_elements)))\nz = self.randn_rng.random(n_elements_rounded2)[:n_elements]\nreturn torch.from_numpy(z).view(*args, -1)\ndef forward(self, batch_size: int) -&gt; ocl.typing.ConditioningOutput:\n\"\"\"Generate conditioning vectors for `batch_size` instances.\n        Args:\n            batch_size: Number of instances to create conditioning vectors for.\n        Returns:\n            The conditioning vectors.\n        \"\"\"\nmu = self.slots_mu.expand(batch_size, self.n_slots, -1)\nsigma = self.slots_logsigma.exp().expand(batch_size, self.n_slots, -1)\nz = self._randn(batch_size, self.n_slots).to(mu, non_blocking=True)\nreturn mu + sigma * z\n</code></pre>"},{"location":"api/ocl/conditioning/#ocl.conditioning.RandomConditioningWithQMCSampling.__init__","title":"<code>__init__</code>","text":"<p>Initialize RandomConditioningWithQMCSampling.</p> <p>Parameters:</p> Name Type Description Default <code>object_dim</code> <code>int</code> <p>Dimensionality of the conditioning vector to generate.</p> required <code>n_slots</code> <code>int</code> <p>Number of conditioning vectors to generate.</p> required <code>learn_mean</code> <code>bool</code> <p>Learn the mean vector of sampling distribution.</p> <code>True</code> <code>learn_std</code> <code>bool</code> <p>Learn the std vector for sampling distribution.</p> <code>True</code> <code>mean_init</code> <code>Callable[[torch.Tensor], None]</code> <p>Callable to initialize mean vector.</p> <code>torch.nn.init.xavier_uniform_</code> <code>logsigma_init</code> <code>Callable[[torch.Tensor], None]</code> <p>Callable to initialize logsigma.</p> <code>torch.nn.init.xavier_uniform_</code> Source code in <code>ocl/conditioning.py</code> <pre><code>def __init__(\nself,\nobject_dim: int,\nn_slots: int,\nlearn_mean: bool = True,\nlearn_std: bool = True,\nmean_init: Callable[[torch.Tensor], None] = torch.nn.init.xavier_uniform_,\nlogsigma_init: Callable[[torch.Tensor], None] = torch.nn.init.xavier_uniform_,\n):\n\"\"\"Initialize RandomConditioningWithQMCSampling.\n    Args:\n        object_dim: Dimensionality of the conditioning vector to generate.\n        n_slots: Number of conditioning vectors to generate.\n        learn_mean: Learn the mean vector of sampling distribution.\n        learn_std: Learn the std vector for sampling distribution.\n        mean_init: Callable to initialize mean vector.\n        logsigma_init: Callable to initialize logsigma.\n    \"\"\"\nsuper().__init__(\nobject_dim=object_dim,\nn_slots=n_slots,\nlearn_mean=learn_mean,\nlearn_std=learn_std,\nmean_init=mean_init,\nlogsigma_init=logsigma_init,\n)\nimport scipy.stats  # Import lazily because scipy takes some time to import\nself.randn_rng = scipy.stats.qmc.MultivariateNormalQMC(mean=np.zeros(object_dim))\n</code></pre>"},{"location":"api/ocl/conditioning/#ocl.conditioning.RandomConditioningWithQMCSampling.forward","title":"<code>forward</code>","text":"<p>Generate conditioning vectors for <code>batch_size</code> instances.</p> <p>Parameters:</p> Name Type Description Default <code>batch_size</code> <code>int</code> <p>Number of instances to create conditioning vectors for.</p> required <p>Returns:</p> Type Description <code>ocl.typing.ConditioningOutput</code> <p>The conditioning vectors.</p> Source code in <code>ocl/conditioning.py</code> <pre><code>def forward(self, batch_size: int) -&gt; ocl.typing.ConditioningOutput:\n\"\"\"Generate conditioning vectors for `batch_size` instances.\n    Args:\n        batch_size: Number of instances to create conditioning vectors for.\n    Returns:\n        The conditioning vectors.\n    \"\"\"\nmu = self.slots_mu.expand(batch_size, self.n_slots, -1)\nsigma = self.slots_logsigma.exp().expand(batch_size, self.n_slots, -1)\nz = self._randn(batch_size, self.n_slots).to(mu, non_blocking=True)\nreturn mu + sigma * z\n</code></pre>"},{"location":"api/ocl/conditioning/#ocl.conditioning.SlotwiseLearntConditioning","title":"<code>SlotwiseLearntConditioning</code>","text":"<p>         Bases: <code>nn.Module</code></p> <p>Random conditioning with learnt mean and stddev for each slot.</p> <p>Removes permutation equivariance compared to the original slot attention conditioning.</p> Source code in <code>ocl/conditioning.py</code> <pre><code>class SlotwiseLearntConditioning(nn.Module):\n\"\"\"Random conditioning with learnt mean and stddev for each slot.\n    Removes permutation equivariance compared to the original slot attention conditioning.\n    \"\"\"\ndef __init__(\nself,\nobject_dim: int,\nn_slots: int,\nmean_init: Callable[[torch.Tensor], None] = torch.nn.init.normal_,\nlogsigma_init: Callable[[torch.Tensor], None] = torch.nn.init.xavier_uniform_,\n):\n\"\"\"Initialize SlotwiseLearntConditioning.\n        Args:\n            object_dim: Dimensionality of the conditioning vector to generate.\n            n_slots: Number of conditioning vectors to generate.\n            mean_init: Callable to initialize mean vector.\n            logsigma_init: Callable to initialize logsigma.\n        \"\"\"\nsuper().__init__()\nself.n_slots = n_slots\nself.object_dim = object_dim\nself.slots_mu = nn.Parameter(torch.zeros(1, n_slots, object_dim))\nself.slots_logsigma = nn.Parameter(torch.zeros(1, n_slots, object_dim))\nwith torch.no_grad():\nmean_init(self.slots_mu)\nlogsigma_init(self.slots_logsigma)\ndef forward(self, batch_size: int) -&gt; ocl.typing.ConditioningOutput:\n\"\"\"Generate conditioning vectors for `batch_size` instances.\n        Args:\n            batch_size: Number of instances to create conditioning vectors for.\n        Returns:\n            The conditioning vectors.\n        \"\"\"\nmu = self.slots_mu.expand(batch_size, -1, -1)\nsigma = self.slots_logsigma.exp().expand(batch_size, -1, -1)\nreturn mu + sigma * torch.randn_like(mu)\n</code></pre>"},{"location":"api/ocl/conditioning/#ocl.conditioning.SlotwiseLearntConditioning.__init__","title":"<code>__init__</code>","text":"<p>Initialize SlotwiseLearntConditioning.</p> <p>Parameters:</p> Name Type Description Default <code>object_dim</code> <code>int</code> <p>Dimensionality of the conditioning vector to generate.</p> required <code>n_slots</code> <code>int</code> <p>Number of conditioning vectors to generate.</p> required <code>mean_init</code> <code>Callable[[torch.Tensor], None]</code> <p>Callable to initialize mean vector.</p> <code>torch.nn.init.normal_</code> <code>logsigma_init</code> <code>Callable[[torch.Tensor], None]</code> <p>Callable to initialize logsigma.</p> <code>torch.nn.init.xavier_uniform_</code> Source code in <code>ocl/conditioning.py</code> <pre><code>def __init__(\nself,\nobject_dim: int,\nn_slots: int,\nmean_init: Callable[[torch.Tensor], None] = torch.nn.init.normal_,\nlogsigma_init: Callable[[torch.Tensor], None] = torch.nn.init.xavier_uniform_,\n):\n\"\"\"Initialize SlotwiseLearntConditioning.\n    Args:\n        object_dim: Dimensionality of the conditioning vector to generate.\n        n_slots: Number of conditioning vectors to generate.\n        mean_init: Callable to initialize mean vector.\n        logsigma_init: Callable to initialize logsigma.\n    \"\"\"\nsuper().__init__()\nself.n_slots = n_slots\nself.object_dim = object_dim\nself.slots_mu = nn.Parameter(torch.zeros(1, n_slots, object_dim))\nself.slots_logsigma = nn.Parameter(torch.zeros(1, n_slots, object_dim))\nwith torch.no_grad():\nmean_init(self.slots_mu)\nlogsigma_init(self.slots_logsigma)\n</code></pre>"},{"location":"api/ocl/conditioning/#ocl.conditioning.SlotwiseLearntConditioning.forward","title":"<code>forward</code>","text":"<p>Generate conditioning vectors for <code>batch_size</code> instances.</p> <p>Parameters:</p> Name Type Description Default <code>batch_size</code> <code>int</code> <p>Number of instances to create conditioning vectors for.</p> required <p>Returns:</p> Type Description <code>ocl.typing.ConditioningOutput</code> <p>The conditioning vectors.</p> Source code in <code>ocl/conditioning.py</code> <pre><code>def forward(self, batch_size: int) -&gt; ocl.typing.ConditioningOutput:\n\"\"\"Generate conditioning vectors for `batch_size` instances.\n    Args:\n        batch_size: Number of instances to create conditioning vectors for.\n    Returns:\n        The conditioning vectors.\n    \"\"\"\nmu = self.slots_mu.expand(batch_size, -1, -1)\nsigma = self.slots_logsigma.exp().expand(batch_size, -1, -1)\nreturn mu + sigma * torch.randn_like(mu)\n</code></pre>"},{"location":"api/ocl/data_decoding/","title":"ocl.data_decoding","text":"<p>Code to decode input data from file streams.</p> <p>The code in this file is adapted from the torchdata and webdatasets packages. It implements an extension based decoder, which selects a different decoding function based on the extension.  In contrast to the implementations in torchdata and webdatasets, the extension is removed from the data field after decoding.  This ideally makes the output format invariant to the exact decoding strategy.</p> Example <p>image.jpg will be decoded into a numpy array and will be accessable in the field <code>image</code>. image.npy.gz will be decoded into a numpy array which can also be accessed under <code>image</code>.</p>"},{"location":"api/ocl/data_decoding/#ocl.data_decoding.ExtensionBasedDecoder","title":"<code>ExtensionBasedDecoder</code>","text":"<p>Decode key/data based on extension using a list of handlers.</p> <p>The input fields are assumed to be instances of StreamWrapper, which wrap an underlying file like object.</p> Source code in <code>ocl/data_decoding.py</code> <pre><code>class ExtensionBasedDecoder:\n\"\"\"Decode key/data based on extension using a list of handlers.\n    The input fields are assumed to be instances of\n    [StreamWrapper][torchdata.datapipes.utils.StreamWrapper],\n    which wrap an underlying file like object.\n    \"\"\"\ndef __init__(self, *handler: Callable[[str, StreamWrapper], Optional[Any]]):\nself.handlers = list(handler) if handler else []\ndef decode1(self, name, data):\nif not data:\nreturn data\nnew_name, extension = os.path.splitext(name)\nif not extension:\nreturn name, data\nfor f in self.handlers:\nresult = f(extension, data)\nif result is not None:\n# Remove decoded part of name.\ndata = result\nname = new_name\n# Try to decode next part of name.\nnew_name, extension = os.path.splitext(name)\nif extension == \"\":\n# Stop decoding if there are no further extensions to be handled.\nbreak\nreturn name, data\ndef decode(self, data: dict):\nresult = {}\nif data is not None:\nfor k, v in data.items():\nif k[0] == \"_\":\nif isinstance(v, StreamWrapper):\ndata_bytes = v.file_obj.read()\nv.autoclose()\nv = data_bytes\nif isinstance(v, bytes):\nv = v.decode(\"utf-8\")\nresult[k] = v\ncontinue\ndecoded_key, decoded_data = self.decode1(k, v)\nresult[decoded_key] = decoded_data\nreturn result\ndef __call__(self, data: Dict[str, Any]) -&gt; Dict[str, Any]:\n\"\"\"Decode input dictionary.\"\"\"\nreturn self.decode(data)\n</code></pre>"},{"location":"api/ocl/data_decoding/#ocl.data_decoding.ExtensionBasedDecoder.__call__","title":"<code>__call__</code>","text":"<p>Decode input dictionary.</p> Source code in <code>ocl/data_decoding.py</code> <pre><code>def __call__(self, data: Dict[str, Any]) -&gt; Dict[str, Any]:\n\"\"\"Decode input dictionary.\"\"\"\nreturn self.decode(data)\n</code></pre>"},{"location":"api/ocl/datasets/","title":"ocl.datasets","text":"<p>Implementation of datasets.</p>"},{"location":"api/ocl/datasets/#ocl.datasets.WebdatasetDataModule","title":"<code>WebdatasetDataModule</code>","text":"<p>         Bases: <code>pl.LightningDataModule</code></p> <p>Webdataset Data Module.</p> Source code in <code>ocl/datasets.py</code> <pre><code>class WebdatasetDataModule(pl.LightningDataModule):\n\"\"\"Webdataset Data Module.\"\"\"\ndef __init__(\nself,\ntrain_shards: Optional[Union[str, List[str]]] = None,\nval_shards: Optional[Union[str, List[str]]] = None,\ntest_shards: Optional[Union[str, List[str]]] = None,\nbatch_size: int = 32,\neval_batch_size: Optional[int] = None,\ntrain_transforms: Optional[Dict[str, Transform]] = None,\neval_transforms: Optional[Dict[str, Transform]] = None,\nnum_workers: int = 2,\ntrain_size: Optional[int] = None,\nval_size: Optional[int] = None,\ntest_size: Optional[int] = None,\nshuffle_train: bool = True,\nshuffle_buffer_size: Optional[int] = None,\nuse_autopadding: bool = False,\n):\n\"\"\"Initialize WebdatasetDataModule.\n        Args:\n            train_shards: Shards associated with training split. Supports braceexpand notation.\n            val_shards: Shards associated with validation split. Supports braceexpand notation.\n            test_shards: Shards associated with test split. Supports braceexpand notation.\n            batch_size: Batch size to use for training.\n            eval_batch_size: Batch size to use for evaluation (i.e. on validation and test split).\n                If `None` use same value as during training.\n            train_transforms: Transforms to apply during training. We use a dict here to make\n                composition of configurations with hydra more easy.\n            eval_transforms: Transforms to apply during evaluation. We use a dict here to make\n                composition of configurations with hydra more easy.\n            num_workers: Number of workers to run in parallel.\n            train_size: Number of instance in the train split (used for progressbar).\n            val_size: Number of instance in the validation split (used for progressbar).\n            test_size: Number of instance in the test split (used for progressbar).\n            shuffle_train: Shuffle training split. Only used to speed up operations on train split\n                unrelated to training. Should typically be left at `False`.\n            shuffle_buffer_size: Buffer size to use for shuffling. If `None` uses `4*batch_size`.\n            use_autopadding: Enable autopadding of instances with different dimensions.\n        \"\"\"\nsuper().__init__()\nif shuffle_buffer_size is None:\n# Ensure that data is shuffled umong at least 4 batches.\n# This should give us a good amount of diversity while also\n# ensuring that we don't need to long to start training.\n# TODO: Ideally, this should also take into account that\n# dataset might be smaller that the shuffle buffer size.\n# As this should not typically occur and we cannot know\n# the number of workers ahead of time we ignore this for now.\nshuffle_buffer_size = batch_size * 4\nif train_shards is None and val_shards is None and test_shards is None:\nraise ValueError(\"No split was specified. Need to specify at least one split.\")\nself.train_shards = train_shards\nself.val_shards = val_shards\nself.test_shards = test_shards\nself.train_size = train_size\nself.val_size = val_size\nself.test_size = test_size\nself.batch_size = batch_size\nself.eval_batch_size = eval_batch_size if eval_batch_size is not None else batch_size\nself.num_workers = num_workers\nself.shuffle_train = shuffle_train\nself.shuffle_buffer_size = shuffle_buffer_size\nself.train_transforms = _get_sorted_values(train_transforms) if train_transforms else []\nself.eval_transforms = _get_sorted_values(eval_transforms) if eval_transforms else []\nif use_autopadding:\nself.collate_fn = collate_with_autopadding\nelse:\nself.collate_fn = collate_with_batch_size\ndef _create_webdataset(\nself,\nuri_expression: Union[str, List[str]],\nshuffle=False,\nn_datapoints: Optional[int] = None,\nkeys_to_keep: Tuple[str] = tuple(),\ntransforms: Sequence[Callable[[IterDataPipe], IterDataPipe]] = tuple(),\n):\nif isinstance(uri_expression, str):\nuri_expression = [uri_expression]\n# Get shards for current worker.\nshard_uris = list(\nchain.from_iterable(\nbraceexpand.braceexpand(single_expression) for single_expression in uri_expression\n)\n)\ndatapipe = torchdata.datapipes.iter.IterableWrapper(shard_uris, deepcopy=False)\ndatapipe = datapipe.shuffle(buffer_size=len(shard_uris)).sharding_filter()\nif shard_uris[0].startswith(\"s3://\") and USE_AWS_SDK:\n# S3 specific implementation is much faster than fsspec.\ndatapipe = datapipe.load_files_by_s3()\nelse:\ndatapipe = datapipe.open_files_by_fsspec(mode=\"rb\")\ndatapipe = datapipe.load_from_tar().webdataset()\n# Discard unneeded properties of the elements prior to shuffling and decoding.\ndatapipe = datapipe.map(partial(_filter_keys, keys_to_keep=keys_to_keep))\nif shuffle:\ndatapipe = datapipe.shuffle(buffer_size=self.shuffle_buffer_size)\n# Decode files and remove extensions from input as we already decoded the elements. This\n# makes our pipeline invariant to the exact encoding used in the dataset.\ndatapipe = datapipe.map(default_decoder)\n# Apply element wise transforms.\nfor transform in transforms:\ndatapipe = transform(datapipe)\nreturn torchdata.datapipes.iter.LengthSetter(datapipe, n_datapoints)\ndef _create_dataloader(self, dataset, batch_transforms, size, batch_size, partial_batches):\n# Don't return partial batches during training as these give the partial samples a higher\n# weight in the optimization than the other samples of the dataset.\n# Apply batch transforms.\ndataset = dataset.batch(\nbatch_size,\ndrop_last=not partial_batches,\n).collate(collate_fn=self.collate_fn)\nfor transform in batch_transforms:\ndataset = transform(dataset)\ndataloader = DataLoader(dataset, num_workers=self.num_workers, batch_size=None)\nreturn dataloader\ndef train_data_iterator(self):\nif self.train_shards is None:\nraise ValueError(\"Can not create train_data_iterator. No training split was specified.\")\ntransforms = self.train_transforms\nreturn self._create_webdataset(\nself.train_shards,\nshuffle=self.shuffle_train,\nn_datapoints=self.train_size,\nkeys_to_keep=_collect_fields(transforms),\ntransforms=_get_single_element_transforms(transforms),\n)\ndef train_dataloader(self):\nreturn self._create_dataloader(\ndataset=self.train_data_iterator(),\nbatch_transforms=_get_batch_transforms(self.train_transforms),\nsize=self.train_size,\nbatch_size=self.batch_size,\npartial_batches=False,\n)\ndef val_data_iterator(self):\nif self.val_shards is None:\nraise ValueError(\"Can not create val_data_iterator. No val split was specified.\")\ntransforms = self.eval_transforms\nreturn self._create_webdataset(\nself.val_shards,\nshuffle=False,\nn_datapoints=self.val_size,\nkeys_to_keep=_collect_fields(transforms),\ntransforms=_get_single_element_transforms(transforms),\n)\ndef val_dataloader(self):\nreturn self._create_dataloader(\ndataset=self.val_data_iterator(),\nbatch_transforms=_get_batch_transforms(self.eval_transforms),\nsize=self.val_size,\nbatch_size=self.eval_batch_size,\npartial_batches=True,\n)\ndef test_data_iterator(self):\nif self.test_shards is None:\nraise ValueError(\"Can not create test_data_iterator. No test split was specified.\")\nreturn self._create_webdataset(\nself.test_shards,\nshuffle=False,\nn_datapoints=self.test_size,\nkeys_to_keep=_collect_fields(self.eval_transforms),\ntransforms=_get_single_element_transforms(self.eval_transforms),\n)\ndef test_dataloader(self):\nreturn self._create_dataloader(\ndataset=self.test_data_iterator(),\nbatch_transforms=_get_batch_transforms(self.eval_transforms),\nsize=self.test_size,\nbatch_size=self.eval_batch_size,\npartial_batches=True,\n)\n</code></pre>"},{"location":"api/ocl/datasets/#ocl.datasets.WebdatasetDataModule.__init__","title":"<code>__init__</code>","text":"<p>Initialize WebdatasetDataModule.</p> <p>Parameters:</p> Name Type Description Default <code>train_shards</code> <code>Optional[Union[str, List[str]]]</code> <p>Shards associated with training split. Supports braceexpand notation.</p> <code>None</code> <code>val_shards</code> <code>Optional[Union[str, List[str]]]</code> <p>Shards associated with validation split. Supports braceexpand notation.</p> <code>None</code> <code>test_shards</code> <code>Optional[Union[str, List[str]]]</code> <p>Shards associated with test split. Supports braceexpand notation.</p> <code>None</code> <code>batch_size</code> <code>int</code> <p>Batch size to use for training.</p> <code>32</code> <code>eval_batch_size</code> <code>Optional[int]</code> <p>Batch size to use for evaluation (i.e. on validation and test split). If <code>None</code> use same value as during training.</p> <code>None</code> <code>train_transforms</code> <code>Optional[Dict[str, Transform]]</code> <p>Transforms to apply during training. We use a dict here to make composition of configurations with hydra more easy.</p> <code>None</code> <code>eval_transforms</code> <code>Optional[Dict[str, Transform]]</code> <p>Transforms to apply during evaluation. We use a dict here to make composition of configurations with hydra more easy.</p> <code>None</code> <code>num_workers</code> <code>int</code> <p>Number of workers to run in parallel.</p> <code>2</code> <code>train_size</code> <code>Optional[int]</code> <p>Number of instance in the train split (used for progressbar).</p> <code>None</code> <code>val_size</code> <code>Optional[int]</code> <p>Number of instance in the validation split (used for progressbar).</p> <code>None</code> <code>test_size</code> <code>Optional[int]</code> <p>Number of instance in the test split (used for progressbar).</p> <code>None</code> <code>shuffle_train</code> <code>bool</code> <p>Shuffle training split. Only used to speed up operations on train split unrelated to training. Should typically be left at <code>False</code>.</p> <code>True</code> <code>shuffle_buffer_size</code> <code>Optional[int]</code> <p>Buffer size to use for shuffling. If <code>None</code> uses <code>4*batch_size</code>.</p> <code>None</code> <code>use_autopadding</code> <code>bool</code> <p>Enable autopadding of instances with different dimensions.</p> <code>False</code> Source code in <code>ocl/datasets.py</code> <pre><code>def __init__(\nself,\ntrain_shards: Optional[Union[str, List[str]]] = None,\nval_shards: Optional[Union[str, List[str]]] = None,\ntest_shards: Optional[Union[str, List[str]]] = None,\nbatch_size: int = 32,\neval_batch_size: Optional[int] = None,\ntrain_transforms: Optional[Dict[str, Transform]] = None,\neval_transforms: Optional[Dict[str, Transform]] = None,\nnum_workers: int = 2,\ntrain_size: Optional[int] = None,\nval_size: Optional[int] = None,\ntest_size: Optional[int] = None,\nshuffle_train: bool = True,\nshuffle_buffer_size: Optional[int] = None,\nuse_autopadding: bool = False,\n):\n\"\"\"Initialize WebdatasetDataModule.\n    Args:\n        train_shards: Shards associated with training split. Supports braceexpand notation.\n        val_shards: Shards associated with validation split. Supports braceexpand notation.\n        test_shards: Shards associated with test split. Supports braceexpand notation.\n        batch_size: Batch size to use for training.\n        eval_batch_size: Batch size to use for evaluation (i.e. on validation and test split).\n            If `None` use same value as during training.\n        train_transforms: Transforms to apply during training. We use a dict here to make\n            composition of configurations with hydra more easy.\n        eval_transforms: Transforms to apply during evaluation. We use a dict here to make\n            composition of configurations with hydra more easy.\n        num_workers: Number of workers to run in parallel.\n        train_size: Number of instance in the train split (used for progressbar).\n        val_size: Number of instance in the validation split (used for progressbar).\n        test_size: Number of instance in the test split (used for progressbar).\n        shuffle_train: Shuffle training split. Only used to speed up operations on train split\n            unrelated to training. Should typically be left at `False`.\n        shuffle_buffer_size: Buffer size to use for shuffling. If `None` uses `4*batch_size`.\n        use_autopadding: Enable autopadding of instances with different dimensions.\n    \"\"\"\nsuper().__init__()\nif shuffle_buffer_size is None:\n# Ensure that data is shuffled umong at least 4 batches.\n# This should give us a good amount of diversity while also\n# ensuring that we don't need to long to start training.\n# TODO: Ideally, this should also take into account that\n# dataset might be smaller that the shuffle buffer size.\n# As this should not typically occur and we cannot know\n# the number of workers ahead of time we ignore this for now.\nshuffle_buffer_size = batch_size * 4\nif train_shards is None and val_shards is None and test_shards is None:\nraise ValueError(\"No split was specified. Need to specify at least one split.\")\nself.train_shards = train_shards\nself.val_shards = val_shards\nself.test_shards = test_shards\nself.train_size = train_size\nself.val_size = val_size\nself.test_size = test_size\nself.batch_size = batch_size\nself.eval_batch_size = eval_batch_size if eval_batch_size is not None else batch_size\nself.num_workers = num_workers\nself.shuffle_train = shuffle_train\nself.shuffle_buffer_size = shuffle_buffer_size\nself.train_transforms = _get_sorted_values(train_transforms) if train_transforms else []\nself.eval_transforms = _get_sorted_values(eval_transforms) if eval_transforms else []\nif use_autopadding:\nself.collate_fn = collate_with_autopadding\nelse:\nself.collate_fn = collate_with_batch_size\n</code></pre>"},{"location":"api/ocl/datasets/#ocl.datasets.DummyDataModule","title":"<code>DummyDataModule</code>","text":"<p>         Bases: <code>pl.LightningDataModule</code></p> <p>Dataset providing dummy data for testing.</p> Source code in <code>ocl/datasets.py</code> <pre><code>class DummyDataModule(pl.LightningDataModule):\n\"\"\"Dataset providing dummy data for testing.\"\"\"\ndef __init__(\nself,\ndata_shapes: Dict[str, List[int]],\ndata_types: Dict[str, str],\nbatch_size: int = 4,\neval_batch_size: Optional[int] = None,\ntrain_transforms: Optional[Dict[str, Transform]] = None,\neval_transforms: Optional[Dict[str, Transform]] = None,\ntrain_size: Optional[int] = None,\nval_size: Optional[int] = None,\ntest_size: Optional[int] = None,\n# Remaining args needed for compatibility with other datamodules\ntrain_shards: Optional[str] = None,\nval_shards: Optional[str] = None,\ntest_shards: Optional[str] = None,\nnum_workers: Optional[int] = None,\n):\n\"\"\"Initialize DummyDataModule.\n        Args:\n            data_shapes: Mapping field names to shapes of tensors.\n            data_types: Mapping from field names to types of tensors. One of `image`, `binary`,\n                `uniform`, `categorical_{n_categories}` or `mask`.\n            batch_size: Batch size to use for training.\n            eval_batch_size: Batch size to use for evaluation (i.e. on validation and test split).\n                If `None` use same value as during training.\n            train_transforms: Transforms to apply during training.\n            eval_transforms: Transforms to apply during evaluation.\n            train_size: Number of instance in the train split (used for progressbar).\n            val_size: Number of instance in the validation split (used for progressbar).\n            test_size: Number of instance in the test split (used for progressbar).\n            train_shards: Kept for compatibility with WebdatasetDataModule. Has no effect.\n            val_shards: Kept for compatibility with WebdatasetDataModule. Has no effect.\n            test_shards: Kept for compatibility with WebdatasetDataModule. Has no effect.\n            num_workers: Kept for compatibility with WebdatasetDataModule. Has no effect.\n        \"\"\"\nsuper().__init__()\nself.data_shapes = {key: list(shape) for key, shape in data_shapes.items()}\nself.data_types = data_types\nself.batch_size = batch_size\nself.eval_batch_size = eval_batch_size if eval_batch_size is not None else batch_size\nself.train_transforms = _get_sorted_values(train_transforms) if train_transforms else []\nself.eval_transforms = _get_sorted_values(eval_transforms) if eval_transforms else []\nself.train_size = train_size\nif self.train_size is None:\nself.train_size = 3 * batch_size + 1\nself.val_size = val_size\nif self.val_size is None:\nself.val_size = 2 * batch_size\nself.test_size = test_size\nif self.test_size is None:\nself.test_size = 2 * batch_size\n@staticmethod\ndef _get_random_data_for_dtype(dtype: str, shape: List[int]):\nif dtype == \"image\":\nreturn np.random.randint(0, 256, size=shape, dtype=np.uint8)\nelif dtype == \"binary\":\nreturn np.random.randint(0, 2, size=shape, dtype=bool)\nelif dtype == \"uniform\":\nreturn np.random.rand(*shape).astype(np.float32)\nelif dtype.startswith(\"categorical_\"):\nbounds = [int(b) for b in dtype.split(\"_\")[1:]]\nif len(bounds) == 1:\nlower, upper = 0, bounds[0]\nelse:\nlower, upper = bounds\nnp_dtype = np.uint8 if upper &lt;= 256 else np.uint64\nreturn np.random.randint(lower, upper, size=shape, dtype=np_dtype)\nelif dtype.startswith(\"mask\"):\ncategories = shape[1]\nnp_dtype = np.uint8 if categories &lt;= 256 else np.uint64\nslot_per_pixel = np.random.randint(\n0, categories, size=shape[:1] + shape[2:], dtype=np_dtype\n)\nreturn (\nnp.eye(categories)[slot_per_pixel.reshape(-1)]\n.reshape(shape[:1] + shape[2:] + [categories])\n.transpose([0, 4, 1, 2, 3])\n)\nelse:\nraise ValueError(f\"Unsupported dtype `{dtype}`\")\ndef _create_dataset(\nself,\nn_datapoints: int,\ntransforms: Sequence[Callable[[Any], Any]],\n):\nclass NumpyDataset(torch.utils.data.IterableDataset):\ndef __init__(self, data: Dict[str, np.ndarray], size: int):\nsuper().__init__()\nself.data = data\nself.size = size\ndef __iter__(self):\nfor i in range(self.size):\nelem = {key: value[i] for key, value in self.data.items()}\nelem[\"__key__\"] = str(i)\nyield elem\ndata = {}\nfor key, shape in self.data_shapes.items():\ndata[key] = self._get_random_data_for_dtype(self.data_types[key], [n_datapoints] + shape)\ndataset = torchdata.datapipes.iter.IterableWrapper(NumpyDataset(data, n_datapoints))\nfor transform in transforms:\ndataset = transform(dataset)\nreturn dataset\ndef _create_dataloader(self, dataset, batch_size):\nreturn torch.utils.data.DataLoader(\ndataset, batch_size=batch_size, num_workers=0, collate_fn=collate_with_autopadding\n)\ndef train_dataloader(self):\ndataset = self._create_dataset(\nself.train_size, _get_single_element_transforms(self.train_transforms)\n)\nreturn self._create_dataloader(dataset, self.batch_size)\ndef val_dataloader(self):\ndataset = self._create_dataset(\nself.val_size, _get_single_element_transforms(self.eval_transforms)\n)\nreturn self._create_dataloader(dataset, self.eval_batch_size)\ndef test_dataloader(self):\ndataset = self._create_dataset(\nself.test_size, _get_single_element_transforms(self.eval_transforms)\n)\nreturn self._create_dataloader(dataset, self.eval_batch_size)\n</code></pre>"},{"location":"api/ocl/datasets/#ocl.datasets.DummyDataModule.__init__","title":"<code>__init__</code>","text":"<p>Initialize DummyDataModule.</p> <p>Parameters:</p> Name Type Description Default <code>data_shapes</code> <code>Dict[str, List[int]]</code> <p>Mapping field names to shapes of tensors.</p> required <code>data_types</code> <code>Dict[str, str]</code> <p>Mapping from field names to types of tensors. One of <code>image</code>, <code>binary</code>, <code>uniform</code>, <code>categorical_{n_categories}</code> or <code>mask</code>.</p> required <code>batch_size</code> <code>int</code> <p>Batch size to use for training.</p> <code>4</code> <code>eval_batch_size</code> <code>Optional[int]</code> <p>Batch size to use for evaluation (i.e. on validation and test split). If <code>None</code> use same value as during training.</p> <code>None</code> <code>train_transforms</code> <code>Optional[Dict[str, Transform]]</code> <p>Transforms to apply during training.</p> <code>None</code> <code>eval_transforms</code> <code>Optional[Dict[str, Transform]]</code> <p>Transforms to apply during evaluation.</p> <code>None</code> <code>train_size</code> <code>Optional[int]</code> <p>Number of instance in the train split (used for progressbar).</p> <code>None</code> <code>val_size</code> <code>Optional[int]</code> <p>Number of instance in the validation split (used for progressbar).</p> <code>None</code> <code>test_size</code> <code>Optional[int]</code> <p>Number of instance in the test split (used for progressbar).</p> <code>None</code> <code>train_shards</code> <code>Optional[str]</code> <p>Kept for compatibility with WebdatasetDataModule. Has no effect.</p> <code>None</code> <code>val_shards</code> <code>Optional[str]</code> <p>Kept for compatibility with WebdatasetDataModule. Has no effect.</p> <code>None</code> <code>test_shards</code> <code>Optional[str]</code> <p>Kept for compatibility with WebdatasetDataModule. Has no effect.</p> <code>None</code> <code>num_workers</code> <code>Optional[int]</code> <p>Kept for compatibility with WebdatasetDataModule. Has no effect.</p> <code>None</code> Source code in <code>ocl/datasets.py</code> <pre><code>def __init__(\nself,\ndata_shapes: Dict[str, List[int]],\ndata_types: Dict[str, str],\nbatch_size: int = 4,\neval_batch_size: Optional[int] = None,\ntrain_transforms: Optional[Dict[str, Transform]] = None,\neval_transforms: Optional[Dict[str, Transform]] = None,\ntrain_size: Optional[int] = None,\nval_size: Optional[int] = None,\ntest_size: Optional[int] = None,\n# Remaining args needed for compatibility with other datamodules\ntrain_shards: Optional[str] = None,\nval_shards: Optional[str] = None,\ntest_shards: Optional[str] = None,\nnum_workers: Optional[int] = None,\n):\n\"\"\"Initialize DummyDataModule.\n    Args:\n        data_shapes: Mapping field names to shapes of tensors.\n        data_types: Mapping from field names to types of tensors. One of `image`, `binary`,\n            `uniform`, `categorical_{n_categories}` or `mask`.\n        batch_size: Batch size to use for training.\n        eval_batch_size: Batch size to use for evaluation (i.e. on validation and test split).\n            If `None` use same value as during training.\n        train_transforms: Transforms to apply during training.\n        eval_transforms: Transforms to apply during evaluation.\n        train_size: Number of instance in the train split (used for progressbar).\n        val_size: Number of instance in the validation split (used for progressbar).\n        test_size: Number of instance in the test split (used for progressbar).\n        train_shards: Kept for compatibility with WebdatasetDataModule. Has no effect.\n        val_shards: Kept for compatibility with WebdatasetDataModule. Has no effect.\n        test_shards: Kept for compatibility with WebdatasetDataModule. Has no effect.\n        num_workers: Kept for compatibility with WebdatasetDataModule. Has no effect.\n    \"\"\"\nsuper().__init__()\nself.data_shapes = {key: list(shape) for key, shape in data_shapes.items()}\nself.data_types = data_types\nself.batch_size = batch_size\nself.eval_batch_size = eval_batch_size if eval_batch_size is not None else batch_size\nself.train_transforms = _get_sorted_values(train_transforms) if train_transforms else []\nself.eval_transforms = _get_sorted_values(eval_transforms) if eval_transforms else []\nself.train_size = train_size\nif self.train_size is None:\nself.train_size = 3 * batch_size + 1\nself.val_size = val_size\nif self.val_size is None:\nself.val_size = 2 * batch_size\nself.test_size = test_size\nif self.test_size is None:\nself.test_size = 2 * batch_size\n</code></pre>"},{"location":"api/ocl/datasets/#ocl.datasets.collate_with_batch_size","title":"<code>collate_with_batch_size</code>","text":"<p>Default pytorch collate function with additional <code>batch_size</code> output for dict input.</p> Source code in <code>ocl/datasets.py</code> <pre><code>def collate_with_batch_size(batch: List[Dict[str, torch.Tensor]]) -&gt; Dict[str, torch.Tensor]:\n\"\"\"Default pytorch collate function with additional `batch_size` output for dict input.\"\"\"\nif isinstance(batch[0], collections.abc.Mapping):\nout = torch_collate.default_collate(batch)\nout[\"batch_size\"] = len(batch)\nreturn out\nreturn torch_collate.default_collate(batch)\n</code></pre>"},{"location":"api/ocl/datasets/#ocl.datasets.collate_with_autopadding","title":"<code>collate_with_autopadding</code>","text":"<p>Collate function that takes a batch of data and stacks it with a batch dimension.</p> <p>In contrast to torch's collate function, this function automatically pads tensors of different sizes with zeros such that they can be stacked.</p> <p>Adapted from https://github.com/pytorch/pytorch/blob/master/torch/utils/data/_utils/collate.py.</p> Source code in <code>ocl/datasets.py</code> <pre><code>def collate_with_autopadding(batch: List[Dict[str, torch.Tensor]]) -&gt; Dict[str, torch.Tensor]:\n\"\"\"Collate function that takes a batch of data and stacks it with a batch dimension.\n    In contrast to torch's collate function, this function automatically pads tensors of different\n    sizes with zeros such that they can be stacked.\n    Adapted from https://github.com/pytorch/pytorch/blob/master/torch/utils/data/_utils/collate.py.\n    \"\"\"\nelem = batch[0]\nelem_type = type(elem)\nif isinstance(elem, torch.Tensor):\nout = None\n# As most tensors will not need padding to be stacked, we first try to stack them normally\n# and pad only if normal padding fails. This avoids explicitly checking whether all tensors\n# have the same shape before stacking.\ntry:\nif torch.utils.data.get_worker_info() is not None:\n# If we're in a background process, concatenate directly into a\n# shared memory tensor to avoid an extra copy\nnumel = sum(x.numel() for x in batch)\nif len(batch) * elem.numel() != numel:\n# Check whether resizing will fail because tensors have unequal sizes to avoid\n# a memory allocation. This is a sufficient but not necessary condition, so it\n# can happen that this check will not trigger when padding is necessary.\nraise RuntimeError()\nstorage = elem.storage()._new_shared(numel)\nout = elem.new(storage).resize_(len(batch), *elem.shape)\nreturn torch.stack(batch, 0, out=out)\nexcept RuntimeError:\n# Stacking did not work. Try to pad tensors to the same dimensionality.\nif not all(x.ndim == elem.ndim for x in batch):\nraise ValueError(\"Tensors in batch have different number of dimensions.\")\nshapes = [x.shape for x in batch]\nmax_dims = [max(shape[idx] for shape in shapes) for idx in range(elem.ndim)]\npaddings = []\nfor shape in shapes:\npadding = []\n# torch.nn.functional.pad wants padding from last to first dim, so go in reverse\nfor idx in reversed(range(len(shape))):\npadding.append(0)\npadding.append(max_dims[idx] - shape[idx])\npaddings.append(padding)\nbatch_padded = [\ntorch.nn.functional.pad(x, pad, mode=\"constant\", value=0.0)\nfor x, pad in zip(batch, paddings)\n]\nif torch.utils.data.get_worker_info() is not None:\n# If we're in a background process, concatenate directly into a\n# shared memory tensor to avoid an extra copy\nnumel = sum(x.numel() for x in batch_padded)\nstorage = elem.storage()._new_shared(numel)\nout = elem.new(storage).resize_(len(batch_padded), *batch_padded[0].shape)\nreturn torch.stack(batch_padded, 0, out=out)\nelif (\nelem_type.__module__ == \"numpy\"\nand elem_type.__name__ != \"str_\"\nand elem_type.__name__ != \"string_\"\n):\nif elem_type.__name__ == \"ndarray\" or elem_type.__name__ == \"memmap\":\n# array of string classes and object\nif torch_collate.np_str_obj_array_pattern.search(elem.dtype.str) is not None:\nraise TypeError(torch_collate.default_collate_err_msg_format.format(elem.dtype))\nreturn collate_with_autopadding([torch.as_tensor(b) for b in batch])\nelif elem.shape == ():  # scalars\nreturn torch.as_tensor(batch)\nelif isinstance(elem, float):\nreturn torch.tensor(batch, dtype=torch.float64)\nelif isinstance(elem, int):\nreturn torch.tensor(batch)\nelif isinstance(elem, str):\nreturn batch\nelif isinstance(elem, collections.abc.Mapping):\nout = {key: collate_with_autopadding([d[key] for d in batch]) for key in elem}\nout[\"batch_size\"] = len(batch)\ntry:\nreturn elem_type(out)\nexcept TypeError:\n# The mapping type may not support `__init__(iterable)`.\nreturn out\nelif isinstance(elem, tuple) and hasattr(elem, \"_fields\"):  # namedtuple\nreturn elem_type(*(collate_with_autopadding(samples) for samples in zip(*batch)))\nelif isinstance(elem, collections.abc.Sequence):\n# check to make sure that the elements in batch have consistent size\nit = iter(batch)\nelem_size = len(next(it))\nif not all(len(elem) == elem_size for elem in it):\nraise RuntimeError(\"each element in list of batch should be of equal size\")\ntransposed = list(zip(*batch))  # It may be accessed twice, so we use a list.\nif isinstance(elem, tuple):\nreturn [\ncollate_with_autopadding(samples) for samples in transposed\n]  # Backwards compatibility.\nelse:\ntry:\nreturn elem_type([collate_with_autopadding(samples) for samples in transposed])\nexcept TypeError:\n# The sequence type may not support `__init__(iterable)` (e.g., `range`).\nreturn [collate_with_autopadding(samples) for samples in transposed]\nraise TypeError(torch_collate.default_collate_err_msg_format.format(elem_type))\n</code></pre>"},{"location":"api/ocl/decoding/","title":"ocl.decoding","text":"<p>Implementation of different types of decoders.</p>"},{"location":"api/ocl/decoding/#ocl.decoding.StyleGANv2Decoder","title":"<code>StyleGANv2Decoder</code>","text":"<p>         Bases: <code>nn.Module</code></p> <p>CNN decoder as used in StyleGANv2 and GIRAFFE.</p> Source code in <code>ocl/decoding.py</code> <pre><code>class StyleGANv2Decoder(nn.Module):\n\"\"\"CNN decoder as used in StyleGANv2 and GIRAFFE.\"\"\"\ndef __init__(\nself,\nobject_feature_dim: int,\noutput_dim: int = 4,\nmin_features=32,\ninput_size: int = 8,\noutput_size: int = 128,\nactivation_fn: str = \"leaky_relu\",\nleaky_relu_slope: float = 0.2,\n):\nsuper().__init__()\ninput_size_log2 = math.log2(input_size)\nassert math.floor(input_size_log2) == input_size_log2, \"Input size needs to be power of 2\"\noutput_size_log2 = math.log2(output_size)\nassert math.floor(output_size_log2) == output_size_log2, \"Output size needs to be power of 2\"\nn_blocks = int(output_size_log2 - input_size_log2)\nself.convs = nn.ModuleList()\ncur_dim = object_feature_dim\nfor _ in range(n_blocks):\nnext_dim = max(cur_dim // 2, min_features)\nself.convs.append(nn.Conv2d(cur_dim, next_dim, 3, stride=1, padding=1))\ncur_dim = next_dim\nself.skip_convs = nn.ModuleList()\ncur_dim = object_feature_dim\nfor _ in range(n_blocks + 1):\nself.skip_convs.append(nn.Conv2d(cur_dim, output_dim, 1, stride=1))\ncur_dim = max(cur_dim // 2, min_features)\nnn.init.zeros_(self.skip_convs[-1].bias)\nif activation_fn == \"relu\":\nself.activation_fn = nn.ReLU(inplace=True)\nelif activation_fn == \"leaky_relu\":\nself.activation_fn = nn.LeakyReLU(leaky_relu_slope, inplace=True)\nelse:\nraise ValueError(f\"Unknown activation function {activation_fn}\")\ndef forward(self, inp):\noutput = self.skip_convs[0](inp)\nfeatures = inp\nfor conv, skip_conv in zip(self.convs, self.skip_convs[1:]):\nfeatures = F.interpolate(features, scale_factor=2, mode=\"nearest-exact\")\nfeatures = conv(features)\nfeatures = self.activation_fn(features)\noutput = F.interpolate(\noutput, scale_factor=2, mode=\"bilinear\", align_corners=False, antialias=True\n)\noutput = output + skip_conv(features)\nreturn output\n</code></pre>"},{"location":"api/ocl/decoding/#ocl.decoding.SlotAttentionDecoder","title":"<code>SlotAttentionDecoder</code>","text":"<p>         Bases: <code>nn.Module</code></p> <p>Decoder used in the original slot attention paper.</p> Source code in <code>ocl/decoding.py</code> <pre><code>class SlotAttentionDecoder(nn.Module):\n\"\"\"Decoder used in the original slot attention paper.\"\"\"\ndef __init__(\nself,\ndecoder: nn.Module,\nfinal_activation: Union[str, Callable] = \"identity\",\npositional_embedding: Optional[nn.Module] = None,\n):\nsuper().__init__()\nself.initial_conv_size = (8, 8)\nself.decoder = decoder\nself.final_activation = get_activation_fn(final_activation)\nself.positional_embedding = positional_embedding\nif positional_embedding:\nself.register_buffer(\"grid\", build_grid_of_positions(self.initial_conv_size))\ndef forward(self, object_features: torch.Tensor):\nassert object_features.dim() &gt;= 3  # Image or video data.\ninitial_shape = object_features.shape[:-1]\nobject_features = object_features.flatten(0, -2)\nobject_features = (\nobject_features.unsqueeze(-1).unsqueeze(-1).expand(-1, -1, *self.initial_conv_size)\n)\nif self.positional_embedding:\nobject_features = self.positional_embedding(object_features, self.grid.unsqueeze(0))\n# Apply deconvolution and restore object dimension.\noutput = self.decoder(object_features)\noutput = output.unflatten(0, initial_shape)\n# Split out alpha channel and normalize over slots.\n# The decoder is assumed to output tensors in CNN order, i.e. * x C x H x W.\nrgb, alpha = output.split([3, 1], dim=-3)\nrgb = self.final_activation(rgb)\nalpha = alpha.softmax(dim=-4)\nreturn ReconstructionOutput(\n# Combine rgb weighted according to alpha channel.\nreconstruction=(rgb * alpha).sum(-4),\nobject_reconstructions=rgb,\nmasks=alpha.squeeze(-3),\n)\n</code></pre>"},{"location":"api/ocl/decoding/#ocl.decoding.SlotAttentionAmodalDecoder","title":"<code>SlotAttentionAmodalDecoder</code>","text":"<p>         Bases: <code>nn.Module</code></p> <p>Decoder used in the original slot attention paper.</p> Source code in <code>ocl/decoding.py</code> <pre><code>class SlotAttentionAmodalDecoder(nn.Module):\n\"\"\"Decoder used in the original slot attention paper.\"\"\"\ndef __init__(\nself,\ndecoder: nn.Module,\nfinal_activation: Union[str, Callable] = \"identity\",\npositional_embedding: Optional[nn.Module] = None,\n):\nsuper().__init__()\nself.initial_conv_size = (8, 8)\nself.decoder = decoder\nself.final_activation = get_activation_fn(final_activation)\nself.positional_embedding = positional_embedding\nif positional_embedding:\nself.register_buffer(\"grid\", build_grid_of_positions(self.initial_conv_size))\ndef rescale_mask(self, mask):\nmax = torch.max(mask)\nmin = torch.min(mask)\nmask_new = (mask - min) / (max - min)\nreturn mask_new\ndef forward(self, object_features: torch.Tensor):\nassert object_features.dim() &gt;= 3  # Image or video data.\ninitial_shape = object_features.shape[:-1]\nobject_features_ori = object_features.clone()\nobject_features = object_features.flatten(0, -2)\nobject_features = (\nobject_features.unsqueeze(-1).unsqueeze(-1).expand(-1, -1, *self.initial_conv_size)\n)\nif self.positional_embedding:\nobject_features = self.positional_embedding(object_features, self.grid.unsqueeze(0))\n# Apply deconvolution and restore object dimension.\noutput = self.decoder(object_features)\noutput = output.unflatten(0, initial_shape)\n# Split out alpha channel and normalize over slots.\n# The decoder is assumed to output tensors in CNN order, i.e. * x C x H x W.\nrgb, alpha = output.split([3, 1], dim=-3)\nrgb = self.final_activation(rgb)\nalpha1 = alpha.softmax(dim=-4)  # visible masks\nalpha2 = alpha.sigmoid()  # amodal masks\nmasks_vis = torch.zeros(alpha1.shape).to(alpha1.device)\nfor b in range(object_features_ori.shape[0]):\nindex = torch.sum(object_features_ori[b], dim=-1).nonzero(as_tuple=True)[0]\nmasks_vis[b][index] = alpha1[b][index]\nfor i in index:\nmasks_vis[b][i] = self.rescale_mask(alpha1[b][i])\nreturn ReconstructionAmodalOutput(\n# Combine rgb weighted according to alpha channel.\nreconstruction=(rgb * alpha1).sum(-4),\nobject_reconstructions=rgb,\nmasks=alpha2.squeeze(-3),\nmasks_vis=alpha1.squeeze(-3),\nmasks_eval=masks_vis.squeeze(-3),\n)\n</code></pre>"},{"location":"api/ocl/decoding/#ocl.decoding.PatchDecoder","title":"<code>PatchDecoder</code>","text":"<p>         Bases: <code>nn.Module</code></p> <p>Decoder that takes object representations and reconstructs patches.</p> <p>Parameters:</p> Name Type Description Default <code>object_dim</code> <code>int</code> <p>Dimension of objects representations.</p> required <code>output_dim</code> <code>int</code> <p>Dimension of each patch.</p> required <code>num_patches</code> <code>int</code> <p>Number of patches P to reconstruct.</p> required <code>decoder</code> <code>Callable[[int, int], nn.Module]</code> <p>Function that returns backbone to use for decoding. Function takes input and output dimensions and should return module that takes inputs of shape (B * K), P, N, and produce outputs of shape (B * K), P, M, where K is the number of objects, N is the number of input dimensions and M the number of output dimensions.</p> required <code>decoder_input_dim</code> <code>Optional[int]</code> <p>Input dimension to decoder backbone. If specified, a linear transformation from object to decoder dimension is added. If not specified, the object dimension is used and no linear transform is added.</p> <code>None</code> Source code in <code>ocl/decoding.py</code> <pre><code>class PatchDecoder(nn.Module):\n\"\"\"Decoder that takes object representations and reconstructs patches.\n    Args:\n        object_dim: Dimension of objects representations.\n        output_dim: Dimension of each patch.\n        num_patches: Number of patches P to reconstruct.\n        decoder: Function that returns backbone to use for decoding. Function takes input and output\n            dimensions and should return module that takes inputs of shape (B * K), P, N, and produce\n            outputs of shape (B * K), P, M, where K is the number of objects, N is the number of\n            input dimensions and M the number of output dimensions.\n        decoder_input_dim: Input dimension to decoder backbone. If specified, a linear\n            transformation from object to decoder dimension is added. If not specified, the object\n            dimension is used and no linear transform is added.\n    \"\"\"\ndef __init__(\nself,\nobject_dim: int,\noutput_dim: int,\nnum_patches: int,\ndecoder: Callable[[int, int], nn.Module],\ndecoder_input_dim: Optional[int] = None,\nupsample_target: Optional[float] = None,\nresize_mode: str = \"bilinear\",\n):\nsuper().__init__()\nself.output_dim = output_dim\nself.num_patches = num_patches\nself.upsample_target = upsample_target\nself.resize_mode = resize_mode\nif decoder_input_dim is not None:\nself.inp_transform = nn.Linear(object_dim, decoder_input_dim, bias=True)\nnn.init.xavier_uniform_(self.inp_transform.weight)\nnn.init.zeros_(self.inp_transform.bias)\nelse:\nself.inp_transform = None\ndecoder_input_dim = object_dim\nself.decoder = decoder(decoder_input_dim, output_dim + 1)\nself.pos_embed = nn.Parameter(torch.randn(1, num_patches, decoder_input_dim) * 0.02)\ndef forward(\nself,\nobject_features: torch.Tensor,\ntarget: Optional[torch.Tensor] = None,\nimage: Optional[torch.Tensor] = None,\n):\nassert object_features.dim() &gt;= 3  # Image or video data.\nif self.upsample_target is not None and target is not None:\ntarget = (\nresize_patches_to_image(\ntarget.detach().transpose(-2, -1),\nscale_factor=self.upsample_target,\nresize_mode=self.resize_mode,\n)\n.flatten(-2, -1)\n.transpose(-2, -1)\n)\ninitial_shape = object_features.shape[:-1]\nobject_features = object_features.flatten(0, -2)\nif self.inp_transform is not None:\nobject_features = self.inp_transform(object_features)\nobject_features = object_features.unsqueeze(1).expand(-1, self.num_patches, -1)\n# Simple learned additive embedding as in ViT\nobject_features = object_features + self.pos_embed\noutput = self.decoder(object_features)\noutput = output.unflatten(0, initial_shape)\n# Split out alpha channel and normalize over slots.\ndecoded_patches, alpha = output.split([self.output_dim, 1], dim=-1)\nalpha = alpha.softmax(dim=-3)\nreconstruction = torch.sum(decoded_patches * alpha, dim=-3)\nmasks = alpha.squeeze(-1)\nif image is not None:\nmasks_as_image = resize_patches_to_image(\nmasks, size=image.shape[-1], resize_mode=\"bilinear\"\n)\nelse:\nmasks_as_image = None\nreturn PatchReconstructionOutput(\nreconstruction=reconstruction,\nmasks=alpha.squeeze(-1),\nmasks_as_image=masks_as_image,\ntarget=target if target is not None else None,\n)\n</code></pre>"},{"location":"api/ocl/decoding/#ocl.decoding.AutoregressivePatchDecoder","title":"<code>AutoregressivePatchDecoder</code>","text":"<p>         Bases: <code>nn.Module</code></p> <p>Decoder that takes object representations and reconstructs patches autoregressively.</p> <p>Parameters:</p> Name Type Description Default <code>object_dim</code> <code>int</code> <p>Dimension of objects representations.</p> required <code>output_dim</code> <code>int</code> <p>Dimension of each patch.</p> required <code>num_patches</code> <code>int</code> <p>Number of patches P to reconstruct.</p> required <code>decoder</code> <code>Callable[[int, int], nn.Module]</code> <p>Function that returns backbone to use for decoding. Function takes input and output dimensions and should return module that takes autoregressive targets of shape B, P, M, conditioning of shape B, K, N, masks of shape P, P, and produces outputs of shape B, P, M, where K is the number of objects, N is the number of input dimensions and M the number of output dimensions.</p> required <code>decoder_cond_dim</code> <code>Optional[int]</code> <p>Dimension of conditioning input of decoder backbone. If specified, a linear transformation from object to decoder dimension is added. If not specified, the object dimension is used and no linear transform is added.</p> <code>None</code> Source code in <code>ocl/decoding.py</code> <pre><code>class AutoregressivePatchDecoder(nn.Module):\n\"\"\"Decoder that takes object representations and reconstructs patches autoregressively.\n    Args:\n        object_dim: Dimension of objects representations.\n        output_dim: Dimension of each patch.\n        num_patches: Number of patches P to reconstruct.\n        decoder: Function that returns backbone to use for decoding. Function takes input and output\n            dimensions and should return module that takes autoregressive targets of shape B, P, M,\n            conditioning of shape B, K, N, masks of shape P, P, and produces outputs of shape\n            B, P, M, where K is the number of objects, N is the number of input dimensions and M the\n            number of output dimensions.\n        decoder_cond_dim: Dimension of conditioning input of decoder backbone. If specified, a linear\n            transformation from object to decoder dimension is added. If not specified, the object\n            dimension is used and no linear transform is added.\n    \"\"\"\ndef __init__(\nself,\nobject_dim: int,\noutput_dim: int,\nnum_patches: int,\ndecoder: Callable[[int, int], nn.Module],\ndecoder_dim: Optional[int] = None,\ndecoder_cond_dim: Optional[int] = None,\nupsample_target: Optional[float] = None,\nresize_mode: str = \"bilinear\",\nuse_decoder_masks: bool = False,\nuse_bos_token: bool = True,\nuse_input_transform: bool = False,\nuse_input_norm: bool = False,\nuse_output_transform: bool = False,\nuse_positional_embedding: bool = False,\n):\nsuper().__init__()\nself.output_dim = output_dim\nself.num_patches = num_patches\nself.upsample_target = upsample_target\nself.resize_mode = resize_mode\nself.use_decoder_masks = use_decoder_masks\nif decoder_dim is None:\ndecoder_dim = output_dim\nself.decoder = decoder(decoder_dim, decoder_dim)\nif use_bos_token:\nself.bos_token = nn.Parameter(torch.randn(1, 1, output_dim) * output_dim**-0.5)\nelse:\nself.bos_token = None\nif decoder_cond_dim is not None:\nself.cond_transform = nn.Sequential(\nnn.Linear(object_dim, decoder_cond_dim, bias=False),\nnn.LayerNorm(decoder_cond_dim, eps=1e-5),\n)\nnn.init.xavier_uniform_(self.cond_transform[0].weight)\nelse:\ndecoder_cond_dim = object_dim\nself.cond_transform = nn.LayerNorm(decoder_cond_dim, eps=1e-5)\nif use_input_transform:\nself.inp_transform = nn.Sequential(\nnn.Linear(output_dim, decoder_dim, bias=False),\nnn.LayerNorm(decoder_dim, eps=1e-5),\n)\nnn.init.xavier_uniform_(self.inp_transform[0].weight)\nelif use_input_norm:\nself.inp_transform = nn.LayerNorm(decoder_dim, eps=1e-5)\nelse:\nself.inp_transform = None\nif use_output_transform:\nself.outp_transform = nn.Linear(decoder_dim, output_dim)\nnn.init.xavier_uniform_(self.outp_transform.weight)\nnn.init.zeros_(self.outp_transform.bias)\nelse:\nself.outp_transform = None\nif use_positional_embedding:\nself.pos_embed = nn.Parameter(\ntorch.randn(1, num_patches, decoder_dim) * decoder_dim**-0.5\n)\nelse:\nself.pos_embed = None\nmask = torch.triu(torch.full((num_patches, num_patches), float(\"-inf\")), diagonal=1)\nself.register_buffer(\"mask\", mask)\ndef forward(\nself,\nobject_features: torch.Tensor,\nmasks: torch.Tensor,\ntarget: torch.Tensor,\nimage: Optional[torch.Tensor] = None,\nempty_objects: Optional[torch.Tensor] = None,\n) -&gt; PatchReconstructionOutput:\nassert object_features.dim() &gt;= 3  # Image or video data.\nif self.upsample_target is not None and target is not None:\ntarget = (\nresize_patches_to_image(\ntarget.detach().transpose(-2, -1),\nscale_factor=self.upsample_target,\nresize_mode=self.resize_mode,\n)\n.flatten(-2, -1)\n.transpose(-2, -1)\n)\n# Squeeze frames into batch if present.\nobject_features = object_features.flatten(0, -3)\nobject_features = self.cond_transform(object_features)\n# Squeeze frame into batch size if necessary.\ninitial_targets_shape = target.shape[:-2]\ntargets = target.flatten(0, -3)\nif self.bos_token is not None:\nbs = len(object_features)\ninputs = torch.cat((self.bos_token.expand(bs, -1, -1), targets[:, :-1].detach()), dim=1)\nelse:\ninputs = targets\nif self.inp_transform is not None:\ninputs = self.inp_transform(inputs)\nif self.pos_embed is not None:\n# Simple learned additive embedding as in ViT\ninputs = inputs + self.pos_embed\nif empty_objects is not None:\noutputs = self.decoder(\ninputs,\nobject_features,\nself.mask,\nmemory_key_padding_mask=empty_objects,\n)\nelse:\noutputs = self.decoder(inputs, object_features, self.mask)\nif self.use_decoder_masks:\ndecoded_patches, masks = outputs\nelse:\ndecoded_patches = outputs\nif self.outp_transform is not None:\ndecoded_patches = self.outp_transform(decoded_patches)\ndecoded_patches = decoded_patches.unflatten(0, initial_targets_shape)\nif image is not None:\nmasks_as_image = resize_patches_to_image(\nmasks, size=image.shape[-1], resize_mode=\"bilinear\"\n)\nelse:\nmasks_as_image = None\nreturn PatchReconstructionOutput(\nreconstruction=decoded_patches, masks=masks, masks_as_image=masks_as_image, target=target\n)\n</code></pre>"},{"location":"api/ocl/decoding/#ocl.decoding.DensityPredictingSlotAttentionDecoder","title":"<code>DensityPredictingSlotAttentionDecoder</code>","text":"<p>         Bases: <code>nn.Module</code></p> <p>Decoder predicting color and densities along a ray into the scene.</p> Source code in <code>ocl/decoding.py</code> <pre><code>class DensityPredictingSlotAttentionDecoder(nn.Module):\n\"\"\"Decoder predicting color and densities along a ray into the scene.\"\"\"\ndef __init__(\nself,\nobject_dim: int,\ndecoder: nn.Module,\ndepth_positions: int,\nwhite_background: bool = False,\nnormalize_densities_along_slots: bool = False,\ninitial_alpha: Optional[float] = None,\n):\nsuper().__init__()\nself.initial_conv_size = (8, 8)\nself.depth_positions = depth_positions\nself.white_background = white_background\nself.normalize_densities_along_slots = normalize_densities_along_slots\nself.register_buffer(\"grid\", build_grid_of_positions(self.initial_conv_size))\nself.pos_embedding = SoftPositionEmbed(2, object_dim, cnn_channel_order=True)\nself.decoder = decoder\nif isinstance(self.decoder, nn.Sequential) and hasattr(self.decoder[-1], \"bias\"):\nnn.init.zeros_(self.decoder[-1].bias)\nif initial_alpha is not None:\n# Distance between neighboring ray points, currently assumed to be 1\npoint_distance = 1\n# Value added to density output of network before softplus activation. If network outputs\n# are approximately zero, the initial mask value per voxel becomes `initial_alpha`. See\n# https://arxiv.org/abs/2111.11215 for a derivation.\nself.initial_density_offset = math.log((1 - initial_alpha) ** (-1 / point_distance) - 1)\nelse:\nself.initial_density_offset = 0.0\ndef _render_objectwise(self, densities, rgbs):\n\"\"\"Render objects individually.\n        Args:\n            densities: Predicted densities of shape (B, S, Z, H, W), where S is the number of slots\n                and Z is the number of depth positions.\n            rgbs: Predicted color values of shape (B, S, 3, H, W), where S is the number of slots.\n            background: Optional background to render on.\n        \"\"\"\ndensities_objectwise = densities.flatten(0, 1).unsqueeze(2)\nrgbs_objectwise = rgbs.flatten(0, 1).unsqueeze(1)\nrgbs_objectwise = rgbs_objectwise.expand(-1, densities_objectwise.shape[1], -1, -1, -1)\nif self.white_background:\nbackground = torch.full_like(rgbs_objectwise[:, 0], 1.0)  # White color, i.e. 0xFFFFFF\nelse:\nbackground = None\nobject_reconstructions, _, object_masks_per_depth, p_ray_hits_points = volume_rendering(\ndensities_objectwise, rgbs_objectwise, background=background\n)\nobject_reconstructions = object_reconstructions.unflatten(0, rgbs.shape[:2])\nobject_masks_per_depth = object_masks_per_depth.squeeze(2).unflatten(0, rgbs.shape[:2])\np_ray_hits_points = p_ray_hits_points.squeeze(2).unflatten(0, rgbs.shape[:2])\np_ray_hits_points_and_reflects = p_ray_hits_points * object_masks_per_depth\nobject_masks, object_depth_map = p_ray_hits_points_and_reflects.max(2)\nreturn object_reconstructions, object_masks, object_depth_map\ndef forward(self, object_features: torch.Tensor):\n# TODO(hornmax): Adapt this for video data.\n# Reshape object dimension into batch dimension and broadcast.\nbs, n_objects, object_feature_dim = object_features.shape\nobject_features = object_features.view(bs * n_objects, object_feature_dim, 1, 1).expand(\n-1, -1, *self.initial_conv_size\n)\nobject_features = self.pos_embedding(object_features, self.grid.unsqueeze(0))\n# Apply deconvolution and restore object dimension.\noutput = self.decoder(object_features)\noutput = output.view(bs, n_objects, *output.shape[-3:])\n# Split rgb and density channels and transform to appropriate ranges.\nrgbs, densities = output.split([3, self.depth_positions], dim=2)\nrgbs = torch.sigmoid(rgbs)  # B x S x 3 x H x W\ndensities = F.softplus(densities + self.initial_density_offset)  # B x S x Z x H x W\nif self.normalize_densities_along_slots:\ndensities_depthwise_sum = torch.einsum(\"bszhw -&gt; bzhw\", densities).unsqueeze(1)\ndensities_weighted = densities * F.softmax(densities, dim=1)\ndensities_weighted_sum = torch.einsum(\"bszhw -&gt; bzhw\", densities_weighted).unsqueeze(1)\ndensities = densities_weighted * densities_depthwise_sum / densities_weighted_sum\n# Combine densities from different slots by summing over slot dimension\ndensity = torch.einsum(\"bszhw -&gt; bzhw\", densities).unsqueeze(2)\n# Combine colors from different slots by density-weighted mean\nrgb = torch.einsum(\"bszhw, bschw -&gt; bzchw\", densities, rgbs) / density\nif self.white_background:\nbackground = torch.full_like(rgb[:, 0], 1.0)  # White color, i.e. 0xFFFFFF\nelse:\nbackground = None\nreconstruction, _, _, p_ray_hits_point = volume_rendering(\ndensity, rgb, background=background\n)\nif self.training:\n# Get object masks by taking the max density over all depth positions\nmasks = 1 - torch.exp(-densities.detach().max(dim=2).values)\nobject_reconstructions = rgbs.detach() * masks.unsqueeze(2)\nif background is not None:\nmasks = torch.cat((masks, p_ray_hits_point[:, -1:, 0]), dim=1)\nobject_reconstructions = torch.cat(\n(object_reconstructions, background[:, None]), dim=1\n)\nreturn ReconstructionOutput(\nreconstruction=reconstruction,\nobject_reconstructions=object_reconstructions,\nmasks=masks,\n)\nelse:\nobject_reconstructions, object_masks, object_depth_map = self._render_objectwise(\ndensities, rgbs\n)\n# Joint depth map results from taking minimum depth over objects per pixel, whereas\n# joint mask results from the index of the object with minimum depth\ndepth_map, mask_dense = object_depth_map.min(1)\nif background is not None:\nobject_reconstructions = torch.cat(\n(object_reconstructions, background[:, None]), dim=1\n)\n# Assign designated background class wherever the depth map indicates background\nmask_dense[depth_map == self.depth_positions] = n_objects\nn_classes = n_objects + 1\nelse:\nn_classes = n_objects\nmasks = F.one_hot(mask_dense, num_classes=n_classes)\nmasks = masks.squeeze(1).permute(0, 3, 1, 2).contiguous()  # B x C x H x W\nreturn DepthReconstructionOutput(\nreconstruction=reconstruction,\nobject_reconstructions=object_reconstructions,\nmasks=masks,\nmasks_amodal=object_masks,\ndepth_map=depth_map,\nobject_depth_map=object_depth_map,\ndensities=densities,\ncolors=rgbs.unsqueeze(2).expand(-1, -1, self.depth_positions, -1, -1, -1),\n)\n</code></pre>"},{"location":"api/ocl/decoding/#ocl.decoding.DVAEDecoder","title":"<code>DVAEDecoder</code>","text":"<p>         Bases: <code>nn.Module</code></p> <p>VQ Decoder used in the original SLATE paper.</p> Source code in <code>ocl/decoding.py</code> <pre><code>class DVAEDecoder(nn.Module):\n\"\"\"VQ Decoder used in the original SLATE paper.\"\"\"\ndef __init__(\nself,\ndecoder: nn.Module,\npatch_size: int = 4,\n):\nsuper().__init__()\nself.initial_conv_size = (patch_size, patch_size)\nself.decoder = decoder\ndef forward(self, features: Dict[str, torch.Tensor]):\nrgb = self.decoder(features)\nreturn SimpleReconstructionOutput(reconstruction=rgb)\n</code></pre>"},{"location":"api/ocl/decoding/#ocl.decoding.build_grid_of_positions","title":"<code>build_grid_of_positions</code>","text":"<p>Build grid of positions which can be used to create positions embeddings.</p> Source code in <code>ocl/decoding.py</code> <pre><code>def build_grid_of_positions(resolution):\n\"\"\"Build grid of positions which can be used to create positions embeddings.\"\"\"\nranges = [torch.linspace(0.0, 1.0, steps=res) for res in resolution]\ngrid = torch.meshgrid(*ranges, indexing=\"ij\")\ngrid = torch.stack(grid, dim=-1)\ngrid = torch.reshape(grid, [resolution[0], resolution[1], -1])\nreturn grid\n</code></pre>"},{"location":"api/ocl/decoding/#ocl.decoding.get_slotattention_decoder_backbone","title":"<code>get_slotattention_decoder_backbone</code>","text":"<p>Get CNN decoder backbone form the original slot attention paper.</p> Source code in <code>ocl/decoding.py</code> <pre><code>def get_slotattention_decoder_backbone(object_dim: int, output_dim: int = 4):\n\"\"\"Get CNN decoder backbone form the original slot attention paper.\"\"\"\nreturn nn.Sequential(\nnn.ConvTranspose2d(object_dim, 64, 5, stride=2, padding=2, output_padding=1),\nnn.ReLU(inplace=True),\nnn.ConvTranspose2d(64, 64, 5, stride=2, padding=2, output_padding=1),\nnn.ReLU(inplace=True),\nnn.ConvTranspose2d(64, 64, 5, stride=2, padding=2, output_padding=1),\nnn.ReLU(inplace=True),\nnn.ConvTranspose2d(64, 64, 5, stride=2, padding=2, output_padding=1),\nnn.ReLU(inplace=True),\nnn.ConvTranspose2d(64, 64, 5, stride=1, padding=2, output_padding=0),\nnn.ReLU(inplace=True),\nnn.ConvTranspose2d(64, output_dim, 3, stride=1, padding=1, output_padding=0),\n)\n</code></pre>"},{"location":"api/ocl/decoding/#ocl.decoding.get_savi_decoder_backbone","title":"<code>get_savi_decoder_backbone</code>","text":"<p>Get CNN decoder backbone form the slot attention for video paper.</p> Source code in <code>ocl/decoding.py</code> <pre><code>def get_savi_decoder_backbone(\nobject_dim: int,\noutput_dim: int = 4,\nlarger_input_arch: bool = False,\nchannel_multiplier: float = 1,\n):\n\"\"\"Get CNN decoder backbone form the slot attention for video paper.\"\"\"\nchannels = int(64 * channel_multiplier)\nif larger_input_arch:\noutput_stride = 2\noutput_padding = 1\nelse:\noutput_stride = 1\noutput_padding = 0\nreturn nn.Sequential(\nnn.ConvTranspose2d(object_dim, channels, 5, stride=2, padding=2, output_padding=1),\nnn.ReLU(inplace=True),\nnn.ConvTranspose2d(channels, channels, 5, stride=2, padding=2, output_padding=1),\nnn.ReLU(inplace=True),\nnn.ConvTranspose2d(channels, channels, 5, stride=2, padding=2, output_padding=1),\nnn.ReLU(inplace=True),\nnn.ConvTranspose2d(\nchannels, channels, 5, stride=output_stride, padding=2, output_padding=output_padding\n),\nnn.ReLU(inplace=True),\nnn.ConvTranspose2d(\nchannels,\noutput_dim,\n1,\nstride=1,\npadding=0,\noutput_padding=0,\n),\n)\n</code></pre>"},{"location":"api/ocl/decoding/#ocl.decoding.get_dvae_decoder","title":"<code>get_dvae_decoder</code>","text":"<p>Get CNN decoder backbone for DVAE module in SLATE paper.</p> Source code in <code>ocl/decoding.py</code> <pre><code>def get_dvae_decoder(vocab_size: int, output_dim: int = 3):\n\"\"\"Get CNN decoder backbone for DVAE module in SLATE paper.\"\"\"\nconv2d = nn.Conv2d(64, output_dim, 1)\nnn.init.xavier_uniform_(conv2d.weight)\nnn.init.zeros_(conv2d.bias)\nreturn nn.Sequential(\nConv2dBlockWithGroupNorm(vocab_size, 64, 1),\nConv2dBlockWithGroupNorm(64, 64, 3, 1, 1),\nConv2dBlockWithGroupNorm(64, 64, 1, 1),\nConv2dBlockWithGroupNorm(64, 64, 1, 1),\nConv2dBlockWithGroupNorm(64, 64 * 2 * 2, 1),\nnn.PixelShuffle(2),\nConv2dBlockWithGroupNorm(64, 64, 3, 1, 1),\nConv2dBlockWithGroupNorm(64, 64, 1, 1),\nConv2dBlockWithGroupNorm(64, 64, 1, 1),\nConv2dBlockWithGroupNorm(64, 64 * 2 * 2, 1),\nnn.PixelShuffle(2),\nconv2d,\n)\n</code></pre>"},{"location":"api/ocl/decoding/#ocl.decoding.get_dvae_encoder","title":"<code>get_dvae_encoder</code>","text":"<p>Get CNN decoder backbone for DVAE module in SLATE paper.</p> Source code in <code>ocl/decoding.py</code> <pre><code>def get_dvae_encoder(vocab_size: int, patch_size: int = 16, output_dim: int = 3):\n\"\"\"Get CNN decoder backbone for DVAE module in SLATE paper.\"\"\"\nconv2d = nn.Conv2d(64, vocab_size, 1)\nnn.init.xavier_uniform_(conv2d.weight)\nnn.init.zeros_(conv2d.bias)\nreturn nn.Sequential(\nConv2dBlockWithGroupNorm(output_dim, 64, patch_size, patch_size),\nConv2dBlockWithGroupNorm(64, 64, 1, 1),\nConv2dBlockWithGroupNorm(64, 64, 1, 1),\nConv2dBlockWithGroupNorm(64, 64, 1, 1),\nConv2dBlockWithGroupNorm(64, 64, 1, 1),\nConv2dBlockWithGroupNorm(64, 64, 1, 1),\nConv2dBlockWithGroupNorm(64, 64, 1, 1),\nconv2d,\n)\n</code></pre>"},{"location":"api/ocl/decoding/#ocl.decoding.volume_rendering","title":"<code>volume_rendering</code>","text":"<p>Volume render along camera rays (also known as alpha compositing).</p> <p>For each ray, assumes input of Z density and C color channels, corresponding to Z points along the ray from front to back of the scene.</p> <p>Parameters:</p> Name Type Description Default <code>densities</code> <code>torch.Tensor</code> <p>Tensor of shape (B, Z, 1, ...). Non-negative, real valued density values along the ray.</p> required <code>colors</code> <code>torch.Tensor</code> <p>Tensor of shape (B, Z, C, ...). Color values along the ray.</p> required <code>distances</code> <code>Union[float, torch.Tensor]</code> <p>Tensor of shape (B, Z, 1, ...). Optional distances between this ray point and the next. Can also be a single float value. If not given, distances between all points are assumed to be one. The last value corresponds to the distance between the last point and the background.</p> <code>None</code> <code>background</code> <code>torch.Tensor</code> <p>Tensor of shape (B, C, ...). An optional background image that the rendering can be put on.</p> <code>None</code> <p>Returns:</p> Type Description <code>torch.Tensor</code> <ul> <li>Rendered image of shape (B, C, ...)</li> </ul> <code>torch.Tensor</code> <ul> <li>Rendered images along different points of the ray with shape (B, Z, 1, ...), if background is not None, the background is included as the last ray point.</li> </ul> <code>torch.Tensor</code> <ul> <li>The alpha masks for each point of the ray with shape (B, Z, 1, ...)</li> </ul> <code>torch.Tensor</code> <ul> <li>The probabilities of reaching each point of the ray (the transmittances) with shape (B, Z, 1, ...)</li> </ul> Source code in <code>ocl/decoding.py</code> <pre><code>def volume_rendering(\ndensities: torch.Tensor,\ncolors: torch.Tensor,\ndistances: Union[float, torch.Tensor] = None,\nbackground: torch.Tensor = None,\n) -&gt; Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n\"\"\"Volume render along camera rays (also known as alpha compositing).\n    For each ray, assumes input of Z density and C color channels, corresponding to Z points along\n    the ray from front to back of the scene.\n    Args:\n        densities: Tensor of shape (B, Z, 1, ...). Non-negative, real valued density values along\n            the ray.\n        colors: Tensor of shape (B, Z, C, ...). Color values along the ray.\n        distances: Tensor of shape (B, Z, 1, ...). Optional distances between this ray point and\n            the next. Can also be a single float value. If not given, distances between all points\n            are assumed to be one. The last value corresponds to the distance between the last point\n            and the background.\n        background: Tensor of shape (B, C, ...). An optional background image that the rendering can\n            be put on.\n    Returns:\n        - Rendered image of shape (B, C, ...)\n        - Rendered images along different points of the ray with shape (B, Z, 1, ...), if background\n            is not None, the background is included as the last ray point.\n        - The alpha masks for each point of the ray with shape (B, Z, 1, ...)\n        - The probabilities of reaching each point of the ray (the transmittances) with shape\n            (B, Z, 1, ...)\n    \"\"\"\nif distances is None:\ntransmittances = torch.exp(-torch.cumsum(densities, dim=1))\np_ray_reflects = 1.0 - torch.exp(-densities)\nelse:\ndensities_distance_weighted = densities * distances\ntransmittances = torch.exp(-torch.cumsum(densities_distance_weighted, dim=1))\np_ray_reflects = 1.0 - torch.exp(-densities_distance_weighted)\n# First object has 100% probability of being hit as it cannot be occluded by other objects\np_ray_hits_point = torch.cat((torch.ones_like(densities[:, :1]), transmittances), dim=1)\nif background is not None:\nbackground = background.unsqueeze(1)\n# All rays reaching the background reflect\np_ray_reflects = torch.cat((p_ray_reflects, torch.ones_like(p_ray_reflects[:, :1])), dim=1)\ncolors = torch.cat((colors, background), dim=1)\nelse:\np_ray_hits_point = p_ray_hits_point[:, :-1]\nz_images = p_ray_reflects * colors\nimage = (p_ray_hits_point * z_images).sum(dim=1)\nreturn image, z_images, p_ray_reflects, p_ray_hits_point\n</code></pre>"},{"location":"api/ocl/losses/","title":"ocl.losses","text":""},{"location":"api/ocl/losses/#ocl.losses.ReconstructionLoss","title":"<code>ReconstructionLoss</code>","text":"<p>         Bases: <code>nn.Module</code></p> <p>Simple reconstruction loss.</p> Source code in <code>ocl/losses.py</code> <pre><code>class ReconstructionLoss(nn.Module):\n\"\"\"Simple reconstruction loss.\"\"\"\ndef __init__(\nself,\nloss_type: str,\nweight: float = 1.0,\nnormalize_target: bool = False,\n):\n\"\"\"Initialize ReconstructionLoss.\n        Args:\n            loss_type: One of `mse`, `mse_sum`, `l1`, `cosine_loss`, `cross_entropy_sum`.\n            weight: Weight of loss, output is multiplied with this value.\n            normalize_target: Normalize target using mean and std of last dimension\n                prior to computing output.\n        \"\"\"\nsuper().__init__()\nif loss_type == \"mse\":\nself.loss_fn = nn.functional.mse_loss\nelif loss_type == \"mse_sum\":\n# Used for slot_attention and video slot attention.\nself.loss_fn = (\nlambda x1, x2: nn.functional.mse_loss(x1, x2, reduction=\"sum\") / x1.shape[0]\n)\nelif loss_type == \"l1\":\nself.loss_name = \"l1_loss\"\nself.loss_fn = nn.functional.l1_loss\nelif loss_type == \"cosine\":\nself.loss_name = \"cosine_loss\"\nself.loss_fn = lambda x1, x2: -nn.functional.cosine_similarity(x1, x2, dim=-1).mean()\nelif loss_type == \"cross_entropy_sum\":\n# Used for SLATE, average is over the first (batch) dim only.\nself.loss_name = \"cross_entropy_sum_loss\"\nself.loss_fn = (\nlambda x1, x2: nn.functional.cross_entropy(\nx1.reshape(-1, x1.shape[-1]), x2.reshape(-1, x2.shape[-1]), reduction=\"sum\"\n)\n/ x1.shape[0]\n)\nelse:\nraise ValueError(\nf\"Unknown loss {loss_type}. Valid choices are (mse, l1, cosine, cross_entropy).\"\n)\n# If weight is callable use it to determine scheduling otherwise use constant value.\nself.weight = weight\nself.normalize_target = normalize_target\ndef forward(self, input: torch.Tensor, target: torch.Tensor) -&gt; float:\n\"\"\"Compute reconstruction loss.\n        Args:\n            input: Prediction / input tensor.\n            target: Target tensor.\n        Returns:\n            The reconstruction loss.\n        \"\"\"\ntarget = target.detach()\nif self.normalize_target:\nmean = target.mean(dim=-1, keepdim=True)\nvar = target.var(dim=-1, keepdim=True)\ntarget = (target - mean) / (var + 1.0e-6) ** 0.5\nloss = self.loss_fn(input, target)\nreturn self.weight * loss\n</code></pre>"},{"location":"api/ocl/losses/#ocl.losses.ReconstructionLoss.__init__","title":"<code>__init__</code>","text":"<p>Initialize ReconstructionLoss.</p> <p>Parameters:</p> Name Type Description Default <code>loss_type</code> <code>str</code> <p>One of <code>mse</code>, <code>mse_sum</code>, <code>l1</code>, <code>cosine_loss</code>, <code>cross_entropy_sum</code>.</p> required <code>weight</code> <code>float</code> <p>Weight of loss, output is multiplied with this value.</p> <code>1.0</code> <code>normalize_target</code> <code>bool</code> <p>Normalize target using mean and std of last dimension prior to computing output.</p> <code>False</code> Source code in <code>ocl/losses.py</code> <pre><code>def __init__(\nself,\nloss_type: str,\nweight: float = 1.0,\nnormalize_target: bool = False,\n):\n\"\"\"Initialize ReconstructionLoss.\n    Args:\n        loss_type: One of `mse`, `mse_sum`, `l1`, `cosine_loss`, `cross_entropy_sum`.\n        weight: Weight of loss, output is multiplied with this value.\n        normalize_target: Normalize target using mean and std of last dimension\n            prior to computing output.\n    \"\"\"\nsuper().__init__()\nif loss_type == \"mse\":\nself.loss_fn = nn.functional.mse_loss\nelif loss_type == \"mse_sum\":\n# Used for slot_attention and video slot attention.\nself.loss_fn = (\nlambda x1, x2: nn.functional.mse_loss(x1, x2, reduction=\"sum\") / x1.shape[0]\n)\nelif loss_type == \"l1\":\nself.loss_name = \"l1_loss\"\nself.loss_fn = nn.functional.l1_loss\nelif loss_type == \"cosine\":\nself.loss_name = \"cosine_loss\"\nself.loss_fn = lambda x1, x2: -nn.functional.cosine_similarity(x1, x2, dim=-1).mean()\nelif loss_type == \"cross_entropy_sum\":\n# Used for SLATE, average is over the first (batch) dim only.\nself.loss_name = \"cross_entropy_sum_loss\"\nself.loss_fn = (\nlambda x1, x2: nn.functional.cross_entropy(\nx1.reshape(-1, x1.shape[-1]), x2.reshape(-1, x2.shape[-1]), reduction=\"sum\"\n)\n/ x1.shape[0]\n)\nelse:\nraise ValueError(\nf\"Unknown loss {loss_type}. Valid choices are (mse, l1, cosine, cross_entropy).\"\n)\n# If weight is callable use it to determine scheduling otherwise use constant value.\nself.weight = weight\nself.normalize_target = normalize_target\n</code></pre>"},{"location":"api/ocl/losses/#ocl.losses.ReconstructionLoss.forward","title":"<code>forward</code>","text":"<p>Compute reconstruction loss.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>torch.Tensor</code> <p>Prediction / input tensor.</p> required <code>target</code> <code>torch.Tensor</code> <p>Target tensor.</p> required <p>Returns:</p> Type Description <code>float</code> <p>The reconstruction loss.</p> Source code in <code>ocl/losses.py</code> <pre><code>def forward(self, input: torch.Tensor, target: torch.Tensor) -&gt; float:\n\"\"\"Compute reconstruction loss.\n    Args:\n        input: Prediction / input tensor.\n        target: Target tensor.\n    Returns:\n        The reconstruction loss.\n    \"\"\"\ntarget = target.detach()\nif self.normalize_target:\nmean = target.mean(dim=-1, keepdim=True)\nvar = target.var(dim=-1, keepdim=True)\ntarget = (target - mean) / (var + 1.0e-6) ** 0.5\nloss = self.loss_fn(input, target)\nreturn self.weight * loss\n</code></pre>"},{"location":"api/ocl/losses/#ocl.losses.LatentDupplicateSuppressionLoss","title":"<code>LatentDupplicateSuppressionLoss</code>","text":"<p>         Bases: <code>nn.Module</code></p> <p>Latent Dupplicate Suppression Loss.</p> Li et al, Duplicate latent representation suppression <p>for multi-object variational autoencoders, BMVC 2021</p> Source code in <code>ocl/losses.py</code> <pre><code>class LatentDupplicateSuppressionLoss(nn.Module):\n\"\"\"Latent Dupplicate Suppression Loss.\n    Inspired by: Li et al, Duplicate latent representation suppression\n      for multi-object variational autoencoders, BMVC 2021\n    \"\"\"\ndef __init__(\nself,\nweight: float,\neps: float = 1e-08,\n):\n\"\"\"Initialize LatentDupplicateSuppressionLoss.\n        Args:\n            weight: Weight of loss, output is multiplied with this value.\n            eps: Small value to avoid division by zero in cosine similarity computation.\n        \"\"\"\nsuper().__init__()\nself.weight = weight\nself.similarity = nn.CosineSimilarity(dim=-1, eps=eps)\ndef forward(self, grouping: ocl.typing.PerceptualGroupingOutput) -&gt; float:\n\"\"\"Compute latent dupplicate suppression loss.\n        This also takes into account the `is_empty` tensor of\n        [ocl.typing.PerceptualGroupingOutput][].\n        Args:\n            grouping: Grouping to use for loss computation.\n        Returns:\n            The weighted loss.\n        \"\"\"\nif grouping.objects.dim() == 4:\n# Build large tensor of reconstructed video.\nobjects = grouping.objects\nbs, n_frames, n_objects, n_features = objects.shape\noff_diag_indices = torch.triu_indices(\nn_objects, n_objects, offset=1, device=objects.device\n)\nsq_similarities = (\nself.similarity(\nobjects[:, :, off_diag_indices[0], :], objects[:, :, off_diag_indices[1], :]\n)\n** 2\n)\nif grouping.is_empty is not None:\np_not_empty = 1.0 - grouping.is_empty\n# Assume that the probability of of individual objects being present is independent,\n# thus the probability of both being present is the product of the individual\n# probabilities.\np_pair_present = (\np_not_empty[..., off_diag_indices[0]] * p_not_empty[..., off_diag_indices[1]]\n)\n# Use average expected penalty as loss for each frame.\nlosses = (sq_similarities * p_pair_present) / torch.sum(\np_pair_present, dim=-1, keepdim=True\n)\nelse:\nlosses = sq_similarities.mean(dim=-1)\nreturn self.weight * losses.sum() / (bs * n_frames)\nelif grouping.objects.dim() == 3:\n# Build large tensor of reconstructed image.\nobjects = grouping.objects\nbs, n_objects, n_features = objects.shape\noff_diag_indices = torch.triu_indices(\nn_objects, n_objects, offset=1, device=objects.device\n)\nsq_similarities = (\nself.similarity(\nobjects[:, off_diag_indices[0], :], objects[:, off_diag_indices[1], :]\n)\n** 2\n)\nif grouping.is_empty is not None:\np_not_empty = 1.0 - grouping.is_empty\n# Assume that the probability of of individual objects being present is independent,\n# thus the probability of both being present is the product of the individual\n# probabilities.\np_pair_present = (\np_not_empty[..., off_diag_indices[0]] * p_not_empty[..., off_diag_indices[1]]\n)\n# Use average expected penalty as loss for each frame.\nlosses = (sq_similarities * p_pair_present) / torch.sum(\np_pair_present, dim=-1, keepdim=True\n)\nelse:\nlosses = sq_similarities.mean(dim=-1)\nreturn self.weight * losses.sum() / bs\nelse:\nraise ValueError(\"Incompatible input format.\")\n</code></pre>"},{"location":"api/ocl/losses/#ocl.losses.LatentDupplicateSuppressionLoss.__init__","title":"<code>__init__</code>","text":"<p>Initialize LatentDupplicateSuppressionLoss.</p> <p>Parameters:</p> Name Type Description Default <code>weight</code> <code>float</code> <p>Weight of loss, output is multiplied with this value.</p> required <code>eps</code> <code>float</code> <p>Small value to avoid division by zero in cosine similarity computation.</p> <code>1e-08</code> Source code in <code>ocl/losses.py</code> <pre><code>def __init__(\nself,\nweight: float,\neps: float = 1e-08,\n):\n\"\"\"Initialize LatentDupplicateSuppressionLoss.\n    Args:\n        weight: Weight of loss, output is multiplied with this value.\n        eps: Small value to avoid division by zero in cosine similarity computation.\n    \"\"\"\nsuper().__init__()\nself.weight = weight\nself.similarity = nn.CosineSimilarity(dim=-1, eps=eps)\n</code></pre>"},{"location":"api/ocl/losses/#ocl.losses.LatentDupplicateSuppressionLoss.forward","title":"<code>forward</code>","text":"<p>Compute latent dupplicate suppression loss.</p> <p>This also takes into account the <code>is_empty</code> tensor of ocl.typing.PerceptualGroupingOutput.</p> <p>Parameters:</p> Name Type Description Default <code>grouping</code> <code>ocl.typing.PerceptualGroupingOutput</code> <p>Grouping to use for loss computation.</p> required <p>Returns:</p> Type Description <code>float</code> <p>The weighted loss.</p> Source code in <code>ocl/losses.py</code> <pre><code>def forward(self, grouping: ocl.typing.PerceptualGroupingOutput) -&gt; float:\n\"\"\"Compute latent dupplicate suppression loss.\n    This also takes into account the `is_empty` tensor of\n    [ocl.typing.PerceptualGroupingOutput][].\n    Args:\n        grouping: Grouping to use for loss computation.\n    Returns:\n        The weighted loss.\n    \"\"\"\nif grouping.objects.dim() == 4:\n# Build large tensor of reconstructed video.\nobjects = grouping.objects\nbs, n_frames, n_objects, n_features = objects.shape\noff_diag_indices = torch.triu_indices(\nn_objects, n_objects, offset=1, device=objects.device\n)\nsq_similarities = (\nself.similarity(\nobjects[:, :, off_diag_indices[0], :], objects[:, :, off_diag_indices[1], :]\n)\n** 2\n)\nif grouping.is_empty is not None:\np_not_empty = 1.0 - grouping.is_empty\n# Assume that the probability of of individual objects being present is independent,\n# thus the probability of both being present is the product of the individual\n# probabilities.\np_pair_present = (\np_not_empty[..., off_diag_indices[0]] * p_not_empty[..., off_diag_indices[1]]\n)\n# Use average expected penalty as loss for each frame.\nlosses = (sq_similarities * p_pair_present) / torch.sum(\np_pair_present, dim=-1, keepdim=True\n)\nelse:\nlosses = sq_similarities.mean(dim=-1)\nreturn self.weight * losses.sum() / (bs * n_frames)\nelif grouping.objects.dim() == 3:\n# Build large tensor of reconstructed image.\nobjects = grouping.objects\nbs, n_objects, n_features = objects.shape\noff_diag_indices = torch.triu_indices(\nn_objects, n_objects, offset=1, device=objects.device\n)\nsq_similarities = (\nself.similarity(\nobjects[:, off_diag_indices[0], :], objects[:, off_diag_indices[1], :]\n)\n** 2\n)\nif grouping.is_empty is not None:\np_not_empty = 1.0 - grouping.is_empty\n# Assume that the probability of of individual objects being present is independent,\n# thus the probability of both being present is the product of the individual\n# probabilities.\np_pair_present = (\np_not_empty[..., off_diag_indices[0]] * p_not_empty[..., off_diag_indices[1]]\n)\n# Use average expected penalty as loss for each frame.\nlosses = (sq_similarities * p_pair_present) / torch.sum(\np_pair_present, dim=-1, keepdim=True\n)\nelse:\nlosses = sq_similarities.mean(dim=-1)\nreturn self.weight * losses.sum() / bs\nelse:\nraise ValueError(\"Incompatible input format.\")\n</code></pre>"},{"location":"api/ocl/losses/#ocl.losses.CLIPLoss","title":"<code>CLIPLoss</code>","text":"<p>         Bases: <code>nn.Module</code></p> <p>Contrastive CLIP loss.</p> Reference <p>Radford et al., Learning transferable visual models from natural language supervision, ICML 2021</p> Source code in <code>ocl/losses.py</code> <pre><code>class CLIPLoss(nn.Module):\n\"\"\"Contrastive CLIP loss.\n    Reference:\n        Radford et al.,\n        Learning transferable visual models from natural language supervision,\n        ICML 2021\n    \"\"\"\ndef __init__(\nself,\nnormalize_inputs: bool = True,\nlearn_scale: bool = True,\nmax_temperature: Optional[float] = None,\n):\n\"\"\"Initiailize CLIP loss.\n        Args:\n            normalize_inputs: Normalize both inputs based on mean and variance.\n            learn_scale: Learn scaling factor of dot product.\n            max_temperature: Maximum temperature of scaling.\n        \"\"\"\nsuper().__init__()\nself.normalize_inputs = normalize_inputs\nif learn_scale:\nself.logit_scale = nn.Parameter(torch.zeros([]) * log(1 / 0.07))  # Same init as CLIP.\nelse:\nself.register_buffer(\"logit_scale\", torch.zeros([]))  # exp(0) = 1, i.e. no scaling.\nself.max_temperature = max_temperature\ndef forward(\nself,\nfirst: ocl.typing.PooledFeatures,\nsecond: ocl.typing.PooledFeatures,\nmodel: Optional[pl.LightningModule] = None,\n) -&gt; Tuple[float, Dict[str, torch.Tensor]]:\n\"\"\"Compute CLIP loss.\n        Args:\n            first: First tensor.\n            second: Second tensor.\n            model: Pytorch lighting model. This is needed in order to perform\n                multi-gpu / multi-node communication independent of the backend.\n        Returns:\n            - Computed loss\n            - Dict with keys `similarity` (containing local similarities)\n                and `temperature` (containing the current temperature).\n        \"\"\"\n# Collect all representations.\nif self.normalize_inputs:\nfirst = first / first.norm(dim=-1, keepdim=True)\nsecond = second / second.norm(dim=-1, keepdim=True)\ntemperature = self.logit_scale.exp()\nif self.max_temperature:\ntemperature = torch.clamp_max(temperature, self.max_temperature)\nif model is not None and hasattr(model, \"trainer\") and model.trainer.world_size &gt; 1:\n# Running on multiple GPUs.\nglobal_rank = model.global_rank\nall_first_rep, all_second_rep = model.all_gather([first, second], sync_grads=True)\nworld_size, batch_size = all_first_rep.shape[:2]\nlabels = (\ntorch.arange(batch_size, dtype=torch.long, device=first.device)\n+ batch_size * global_rank\n)\n# Flatten the GPU dim into batch.\nall_first_rep = all_first_rep.flatten(0, 1)\nall_second_rep = all_second_rep.flatten(0, 1)\n# Compute inner product for instances on the current GPU.\nlogits_per_first = temperature * first @ all_second_rep.t()\nlogits_per_second = temperature * second @ all_first_rep.t()\n# For visualization purposes, return the cosine similarities on the local batch.\nsimilarities = (\n1\n/ temperature\n* logits_per_first[:, batch_size * global_rank : batch_size * (global_rank + 1)]\n)\n# shape = [local_batch_size, global_batch_size]\nelse:\nbatch_size = first.shape[0]\nlabels = torch.arange(batch_size, dtype=torch.long, device=first.device)\n# When running with only a single GPU we can save some compute time by reusing\n# computations.\nlogits_per_first = temperature * first @ second.t()\nlogits_per_second = logits_per_first.t()\nsimilarities = 1 / temperature * logits_per_first\nreturn (\n(F.cross_entropy(logits_per_first, labels) + F.cross_entropy(logits_per_second, labels))\n/ 2,\n{\"similarities\": similarities, \"temperature\": temperature},\n)\n</code></pre>"},{"location":"api/ocl/losses/#ocl.losses.CLIPLoss.__init__","title":"<code>__init__</code>","text":"<p>Initiailize CLIP loss.</p> <p>Parameters:</p> Name Type Description Default <code>normalize_inputs</code> <code>bool</code> <p>Normalize both inputs based on mean and variance.</p> <code>True</code> <code>learn_scale</code> <code>bool</code> <p>Learn scaling factor of dot product.</p> <code>True</code> <code>max_temperature</code> <code>Optional[float]</code> <p>Maximum temperature of scaling.</p> <code>None</code> Source code in <code>ocl/losses.py</code> <pre><code>def __init__(\nself,\nnormalize_inputs: bool = True,\nlearn_scale: bool = True,\nmax_temperature: Optional[float] = None,\n):\n\"\"\"Initiailize CLIP loss.\n    Args:\n        normalize_inputs: Normalize both inputs based on mean and variance.\n        learn_scale: Learn scaling factor of dot product.\n        max_temperature: Maximum temperature of scaling.\n    \"\"\"\nsuper().__init__()\nself.normalize_inputs = normalize_inputs\nif learn_scale:\nself.logit_scale = nn.Parameter(torch.zeros([]) * log(1 / 0.07))  # Same init as CLIP.\nelse:\nself.register_buffer(\"logit_scale\", torch.zeros([]))  # exp(0) = 1, i.e. no scaling.\nself.max_temperature = max_temperature\n</code></pre>"},{"location":"api/ocl/losses/#ocl.losses.CLIPLoss.forward","title":"<code>forward</code>","text":"<p>Compute CLIP loss.</p> <p>Parameters:</p> Name Type Description Default <code>first</code> <code>ocl.typing.PooledFeatures</code> <p>First tensor.</p> required <code>second</code> <code>ocl.typing.PooledFeatures</code> <p>Second tensor.</p> required <code>model</code> <code>Optional[pl.LightningModule]</code> <p>Pytorch lighting model. This is needed in order to perform multi-gpu / multi-node communication independent of the backend.</p> <code>None</code> <p>Returns:</p> Type Description <code>float</code> <ul> <li>Computed loss</li> </ul> <code>Dict[str, torch.Tensor]</code> <ul> <li>Dict with keys <code>similarity</code> (containing local similarities) and <code>temperature</code> (containing the current temperature).</li> </ul> Source code in <code>ocl/losses.py</code> <pre><code>def forward(\nself,\nfirst: ocl.typing.PooledFeatures,\nsecond: ocl.typing.PooledFeatures,\nmodel: Optional[pl.LightningModule] = None,\n) -&gt; Tuple[float, Dict[str, torch.Tensor]]:\n\"\"\"Compute CLIP loss.\n    Args:\n        first: First tensor.\n        second: Second tensor.\n        model: Pytorch lighting model. This is needed in order to perform\n            multi-gpu / multi-node communication independent of the backend.\n    Returns:\n        - Computed loss\n        - Dict with keys `similarity` (containing local similarities)\n            and `temperature` (containing the current temperature).\n    \"\"\"\n# Collect all representations.\nif self.normalize_inputs:\nfirst = first / first.norm(dim=-1, keepdim=True)\nsecond = second / second.norm(dim=-1, keepdim=True)\ntemperature = self.logit_scale.exp()\nif self.max_temperature:\ntemperature = torch.clamp_max(temperature, self.max_temperature)\nif model is not None and hasattr(model, \"trainer\") and model.trainer.world_size &gt; 1:\n# Running on multiple GPUs.\nglobal_rank = model.global_rank\nall_first_rep, all_second_rep = model.all_gather([first, second], sync_grads=True)\nworld_size, batch_size = all_first_rep.shape[:2]\nlabels = (\ntorch.arange(batch_size, dtype=torch.long, device=first.device)\n+ batch_size * global_rank\n)\n# Flatten the GPU dim into batch.\nall_first_rep = all_first_rep.flatten(0, 1)\nall_second_rep = all_second_rep.flatten(0, 1)\n# Compute inner product for instances on the current GPU.\nlogits_per_first = temperature * first @ all_second_rep.t()\nlogits_per_second = temperature * second @ all_first_rep.t()\n# For visualization purposes, return the cosine similarities on the local batch.\nsimilarities = (\n1\n/ temperature\n* logits_per_first[:, batch_size * global_rank : batch_size * (global_rank + 1)]\n)\n# shape = [local_batch_size, global_batch_size]\nelse:\nbatch_size = first.shape[0]\nlabels = torch.arange(batch_size, dtype=torch.long, device=first.device)\n# When running with only a single GPU we can save some compute time by reusing\n# computations.\nlogits_per_first = temperature * first @ second.t()\nlogits_per_second = logits_per_first.t()\nsimilarities = 1 / temperature * logits_per_first\nreturn (\n(F.cross_entropy(logits_per_first, labels) + F.cross_entropy(logits_per_second, labels))\n/ 2,\n{\"similarities\": similarities, \"temperature\": temperature},\n)\n</code></pre>"},{"location":"api/ocl/losses/#ocl.losses.DETRSegLoss","title":"<code>DETRSegLoss</code>","text":"<p>         Bases: <code>nn.Module</code></p> <p>DETR inspired loss for segmentation.</p> <p>This loss computes a hungarian matching of segmentation masks between a prediction and a target.  The loss is then a linear combination of the CE loss between matched masks and a foreground prediction classification.</p> Reference <p>Carion et al., End-to-End Object Detection with Transformers, ECCV 2020</p> Source code in <code>ocl/losses.py</code> <pre><code>class DETRSegLoss(nn.Module):\n\"\"\"DETR inspired loss for segmentation.\n    This loss computes a hungarian matching of segmentation masks between a prediction and\n    a target.  The loss is then a linear combination of the CE loss between matched masks\n    and a foreground prediction classification.\n    Reference:\n        Carion et al., End-to-End Object Detection with Transformers, ECCV 2020\n    \"\"\"\ndef __init__(\nself,\nloss_weight: float = 1.0,\nignore_background: bool = True,\nforeground_weight: float = 1.0,\nforeground_matching_weight: float = 1.0,\nglobal_loss: bool = True,\n):\n\"\"\"Initialize DETRSegLoss.\n        Args:\n            loss_weight: Loss weight\n            ignore_background: Ignore background masks.\n            foreground_weight: Contribution weight of foreground classification loss.\n            foreground_matching_weight: Contribution weight of foreground classification\n                to matching.\n            global_loss: Use average loss over all instances of all gpus.  This is\n                particularly useful when training with sparse labels.\n        \"\"\"\nsuper().__init__()\nself.loss_weight = loss_weight\nself.ignore_background = ignore_background\nself.foreground_weight = foreground_weight\nself.foreground_matching_weight = foreground_matching_weight\nself.global_loss = global_loss\nself.matcher = CPUHungarianMatcher()\ndef forward(\nself,\ninput_mask: ocl.typing.ObjectFeatureAttributions,\ntarget_mask: ocl.typing.ObjectFeatureAttributions,\nforeground_logits: Optional[torch.Tensor] = None,\nmodel: Optional[pl.LightningModule] = None,\n) -&gt; float:\n\"\"\"Compute DETR segmentation loss.\n        Args:\n            input_mask: Input/predicted masks\n            target_mask: Target masks\n            foreground_logits: Forground prediction logits\n            model: Pytorch lighting model. This is needed in order to perform\n                multi-gpu / multi-node communication independent of the backend.\n        Returns:\n            The computed loss.\n        \"\"\"\ntarget_mask = target_mask.detach() &gt; 0\ndevice = target_mask.device\n# A nan mask is not considered.\nvalid_targets = ~(target_mask.isnan().all(-1).all(-1)).any(-1)\n# Discard first dimension mask as it is background.\nif self.ignore_background:\n# Assume first class in masks is background.\nif len(target_mask.shape) &gt; 4:  # Video data (bs, frame, classes, w, h).\ntarget_mask = target_mask[:, :, 1:]\nelse:  # Image data (bs, classes, w, h).\ntarget_mask = target_mask[:, 1:]\ntargets = target_mask[valid_targets]\npredictions = input_mask[valid_targets]\nif foreground_logits is not None:\nforeground_logits = foreground_logits[valid_targets]\ntotal_loss = torch.tensor(0.0, device=device)\nnum_samples = 0\n# Iterate through each clip. Might think about if parallelable\nfor i, (prediction, target) in enumerate(zip(predictions, targets)):\n# Filter empty masks.\ntarget = target[target.sum(-1).sum(-1) &gt; 0]\n# Compute matching.\ncostMatrixSeg = _compute_detr_seg_const_matrix(\nprediction,\ntarget,\n)\n# We cannot rely on the matched cost for computing the loss due to\n# normalization issues between segmentation component (normalized by\n# number of matches) and classification component (normalized by\n# number of predictions). Thus compute both components separately\n# after deriving the matching matrix.\nif foreground_logits is not None and self.foreground_matching_weight != 0.0:\n# Positive classification component.\nlogits = foreground_logits[i]\ncostMatrixTotal = (\ncostMatrixSeg\n+ self.foreground_weight\n* F.binary_cross_entropy_with_logits(\nlogits, torch.ones_like(logits), reduction=\"none\"\n).detach()\n)\nelse:\ncostMatrixTotal = costMatrixSeg\n# Matcher takes a batch but we are doing this one by one.\nmatching_matrix = self.matcher(costMatrixTotal.unsqueeze(0))[0].squeeze(0)\nn_matches = min(predictions.shape[0], target.shape[0])\nif n_matches &gt; 0:\ninstance_cost = (costMatrixSeg * matching_matrix).sum(-1).sum(-1) / n_matches\nelse:\ninstance_cost = torch.tensor(0.0, device=device)\nif foreground_logits is not None:\nismatched = (matching_matrix &gt; 0).any(-1)\nlogits = foreground_logits[i].squeeze(-1)\ninstance_cost += self.foreground_weight * F.binary_cross_entropy_with_logits(\nlogits, ismatched.float(), reduction=\"mean\"\n)\ntotal_loss += instance_cost\n# Normalize by number of matches.\nnum_samples += 1\nif (\nmodel is not None\nand hasattr(model, \"trainer\")\nand model.trainer.world_size &gt; 1\nand self.global_loss\n):\n# As data is sparsely labeled return the average loss over all GPUs.\n# This should make the loss a mit more smooth.\nall_losses, sample_counts = model.all_gather([total_loss, num_samples], sync_grads=True)\ntotal_count = sample_counts.sum()\nif total_count &gt; 0:\ntotal_loss = all_losses.sum() / total_count\nelse:\ntotal_loss = torch.tensor(0.0, device=device)\nreturn total_loss * self.loss_weight\nelse:\nif num_samples == 0:\n# Avoid division by zero if a batch does not contain any labels.\nreturn torch.tensor(0.0, device=targets.device)\ntotal_loss /= num_samples\ntotal_loss *= self.loss_weight\nreturn total_loss\n</code></pre>"},{"location":"api/ocl/losses/#ocl.losses.DETRSegLoss.__init__","title":"<code>__init__</code>","text":"<p>Initialize DETRSegLoss.</p> <p>Parameters:</p> Name Type Description Default <code>loss_weight</code> <code>float</code> <p>Loss weight</p> <code>1.0</code> <code>ignore_background</code> <code>bool</code> <p>Ignore background masks.</p> <code>True</code> <code>foreground_weight</code> <code>float</code> <p>Contribution weight of foreground classification loss.</p> <code>1.0</code> <code>foreground_matching_weight</code> <code>float</code> <p>Contribution weight of foreground classification to matching.</p> <code>1.0</code> <code>global_loss</code> <code>bool</code> <p>Use average loss over all instances of all gpus.  This is particularly useful when training with sparse labels.</p> <code>True</code> Source code in <code>ocl/losses.py</code> <pre><code>def __init__(\nself,\nloss_weight: float = 1.0,\nignore_background: bool = True,\nforeground_weight: float = 1.0,\nforeground_matching_weight: float = 1.0,\nglobal_loss: bool = True,\n):\n\"\"\"Initialize DETRSegLoss.\n    Args:\n        loss_weight: Loss weight\n        ignore_background: Ignore background masks.\n        foreground_weight: Contribution weight of foreground classification loss.\n        foreground_matching_weight: Contribution weight of foreground classification\n            to matching.\n        global_loss: Use average loss over all instances of all gpus.  This is\n            particularly useful when training with sparse labels.\n    \"\"\"\nsuper().__init__()\nself.loss_weight = loss_weight\nself.ignore_background = ignore_background\nself.foreground_weight = foreground_weight\nself.foreground_matching_weight = foreground_matching_weight\nself.global_loss = global_loss\nself.matcher = CPUHungarianMatcher()\n</code></pre>"},{"location":"api/ocl/losses/#ocl.losses.DETRSegLoss.forward","title":"<code>forward</code>","text":"<p>Compute DETR segmentation loss.</p> <p>Parameters:</p> Name Type Description Default <code>input_mask</code> <code>ocl.typing.ObjectFeatureAttributions</code> <p>Input/predicted masks</p> required <code>target_mask</code> <code>ocl.typing.ObjectFeatureAttributions</code> <p>Target masks</p> required <code>foreground_logits</code> <code>Optional[torch.Tensor]</code> <p>Forground prediction logits</p> <code>None</code> <code>model</code> <code>Optional[pl.LightningModule]</code> <p>Pytorch lighting model. This is needed in order to perform multi-gpu / multi-node communication independent of the backend.</p> <code>None</code> <p>Returns:</p> Type Description <code>float</code> <p>The computed loss.</p> Source code in <code>ocl/losses.py</code> <pre><code>def forward(\nself,\ninput_mask: ocl.typing.ObjectFeatureAttributions,\ntarget_mask: ocl.typing.ObjectFeatureAttributions,\nforeground_logits: Optional[torch.Tensor] = None,\nmodel: Optional[pl.LightningModule] = None,\n) -&gt; float:\n\"\"\"Compute DETR segmentation loss.\n    Args:\n        input_mask: Input/predicted masks\n        target_mask: Target masks\n        foreground_logits: Forground prediction logits\n        model: Pytorch lighting model. This is needed in order to perform\n            multi-gpu / multi-node communication independent of the backend.\n    Returns:\n        The computed loss.\n    \"\"\"\ntarget_mask = target_mask.detach() &gt; 0\ndevice = target_mask.device\n# A nan mask is not considered.\nvalid_targets = ~(target_mask.isnan().all(-1).all(-1)).any(-1)\n# Discard first dimension mask as it is background.\nif self.ignore_background:\n# Assume first class in masks is background.\nif len(target_mask.shape) &gt; 4:  # Video data (bs, frame, classes, w, h).\ntarget_mask = target_mask[:, :, 1:]\nelse:  # Image data (bs, classes, w, h).\ntarget_mask = target_mask[:, 1:]\ntargets = target_mask[valid_targets]\npredictions = input_mask[valid_targets]\nif foreground_logits is not None:\nforeground_logits = foreground_logits[valid_targets]\ntotal_loss = torch.tensor(0.0, device=device)\nnum_samples = 0\n# Iterate through each clip. Might think about if parallelable\nfor i, (prediction, target) in enumerate(zip(predictions, targets)):\n# Filter empty masks.\ntarget = target[target.sum(-1).sum(-1) &gt; 0]\n# Compute matching.\ncostMatrixSeg = _compute_detr_seg_const_matrix(\nprediction,\ntarget,\n)\n# We cannot rely on the matched cost for computing the loss due to\n# normalization issues between segmentation component (normalized by\n# number of matches) and classification component (normalized by\n# number of predictions). Thus compute both components separately\n# after deriving the matching matrix.\nif foreground_logits is not None and self.foreground_matching_weight != 0.0:\n# Positive classification component.\nlogits = foreground_logits[i]\ncostMatrixTotal = (\ncostMatrixSeg\n+ self.foreground_weight\n* F.binary_cross_entropy_with_logits(\nlogits, torch.ones_like(logits), reduction=\"none\"\n).detach()\n)\nelse:\ncostMatrixTotal = costMatrixSeg\n# Matcher takes a batch but we are doing this one by one.\nmatching_matrix = self.matcher(costMatrixTotal.unsqueeze(0))[0].squeeze(0)\nn_matches = min(predictions.shape[0], target.shape[0])\nif n_matches &gt; 0:\ninstance_cost = (costMatrixSeg * matching_matrix).sum(-1).sum(-1) / n_matches\nelse:\ninstance_cost = torch.tensor(0.0, device=device)\nif foreground_logits is not None:\nismatched = (matching_matrix &gt; 0).any(-1)\nlogits = foreground_logits[i].squeeze(-1)\ninstance_cost += self.foreground_weight * F.binary_cross_entropy_with_logits(\nlogits, ismatched.float(), reduction=\"mean\"\n)\ntotal_loss += instance_cost\n# Normalize by number of matches.\nnum_samples += 1\nif (\nmodel is not None\nand hasattr(model, \"trainer\")\nand model.trainer.world_size &gt; 1\nand self.global_loss\n):\n# As data is sparsely labeled return the average loss over all GPUs.\n# This should make the loss a mit more smooth.\nall_losses, sample_counts = model.all_gather([total_loss, num_samples], sync_grads=True)\ntotal_count = sample_counts.sum()\nif total_count &gt; 0:\ntotal_loss = all_losses.sum() / total_count\nelse:\ntotal_loss = torch.tensor(0.0, device=device)\nreturn total_loss * self.loss_weight\nelse:\nif num_samples == 0:\n# Avoid division by zero if a batch does not contain any labels.\nreturn torch.tensor(0.0, device=targets.device)\ntotal_loss /= num_samples\ntotal_loss *= self.loss_weight\nreturn total_loss\n</code></pre>"},{"location":"api/ocl/matching/","title":"ocl.matching","text":"<p>Methods for matching between sets of elements.</p>"},{"location":"api/ocl/matching/#ocl.matching.Matcher","title":"<code>Matcher</code>","text":"<p>         Bases: <code>torch.nn.Module</code></p> <p>Matcher base class to define consistent interface.</p> Source code in <code>ocl/matching.py</code> <pre><code>class Matcher(torch.nn.Module):\n\"\"\"Matcher base class to define consistent interface.\"\"\"\ndef forward(self, C: CostMatrix) -&gt; Tuple[AssignmentMatrix, CostVector]:\npass\n</code></pre>"},{"location":"api/ocl/matching/#ocl.matching.CPUHungarianMatcher","title":"<code>CPUHungarianMatcher</code>","text":"<p>         Bases: <code>Matcher</code></p> <p>Implementaiton of a cpu hungarian matcher using scipy.optimize.linear_sum_assignment.</p> Source code in <code>ocl/matching.py</code> <pre><code>class CPUHungarianMatcher(Matcher):\n\"\"\"Implementaiton of a cpu hungarian matcher using scipy.optimize.linear_sum_assignment.\"\"\"\ndef forward(self, C: CostMatrix) -&gt; Tuple[AssignmentMatrix, CostVector]:\nX = torch.zeros_like(C)\nC_cpu: np.ndarray = C.detach().cpu().numpy()\nfor i, cost_matrix in enumerate(C_cpu):\nrow_ind, col_ind = linear_sum_assignment(cost_matrix)\nX[i][row_ind, col_ind] = 1.0\nreturn X, (C * X).sum(dim=(1, 2))\n</code></pre>"},{"location":"api/ocl/optimization/","title":"ocl.optimization","text":"<p>Convenience functions that allow defining optimization via config.</p>"},{"location":"api/ocl/optimization/#ocl.optimization.OptimizationWrapper","title":"<code>OptimizationWrapper</code>","text":"<p>Optimize (a subset of) the parameters using a optimizer and a LR scheduler.</p> Source code in <code>ocl/optimization.py</code> <pre><code>class OptimizationWrapper:\n\"\"\"Optimize (a subset of) the parameters using a optimizer and a LR scheduler.\"\"\"\ndef __init__(\nself,\noptimizer: Optimizer,\nlr_scheduler: Optional[Callable[[Optimizer], Dict[str, Any]]] = None,\nparameter_groups: Optional[List[Dict[str, Any]]] = None,\n):\n\"\"\"Initialize OptimizationWrapper.\n        Args:\n            optimizer: The oprimizer that should be used to optimize the parameters.\n            lr_scheduler: The LR scheduling callable that should be used.  This should\n                be a callable that returns a dict for updating the optimizer output in\n                pytorch_lightning. See [ocl.scheduling.exponential_decay_with_optional_warmup][]\n                for an example of such a callable.\n            parameter_groups: Define parameter groups which have different optimizer parameters.\n                Each element of the list should at least one of two keys `params` (for defining\n                parameters based on their path in the model) or `predicate` (for defining parameters\n                using a predicate function which returns true if the parameter should be included).\n                For an example on how to use this parameter_groups, see\n                `configs/experiment/examples/parameter_groups.yaml`.\n        \"\"\"\nself.optimizer = optimizer\nself.lr_scheduler = lr_scheduler\nself.parameter_group_specs = parameter_groups\nif self.parameter_group_specs:\nfor idx, param_group_spec in enumerate(self.parameter_group_specs):\nif \"params\" not in param_group_spec:\nraise ValueError(f'Parameter group {idx + 1} does not contain key \"params\"')\nparam_spec = param_group_spec[\"params\"]\nif isinstance(param_spec, str):\nparam_group_spec[\"params\"] = [param_spec]\nelif isinstance(param_spec, Iterable):\nparam_group_spec[\"params\"] = list(param_spec)\nelse:\nraise ValueError(\nf'\"params\" for parameter group {idx + 1} is not of type str or iterable'\n)\nif \"predicate\" in param_group_spec:\nif not callable(param_group_spec[\"predicate\"]):\nraise ValueError(\nf'\"predicate\" for parameter group {idx + 1} is not a callable'\n)\ndef _get_parameter_groups(self, model):\n\"\"\"Build parameter groups from specification.\"\"\"\nif not self.parameter_group_specs:\nreturn model.parameters()\nparameter_groups = []\nfor param_group_spec in self.parameter_group_specs:\nparam_spec = param_group_spec[\"params\"]\n# Default predicate includes all parameters\npredicate = param_group_spec.get(\"predicate\", lambda name, param: True)\nparameters = []\nfor parameter_path in param_spec:\nroot = model\nfor child in parameter_path.split(\".\"):\nroot = getattr(root, child)\nparameters.extend(\nparam for name, param in root.named_parameters() if predicate(name, param)\n)\nparam_group = {\nk: v for k, v in param_group_spec.items() if k not in (\"params\", \"predicate\")\n}\nparam_group[\"params\"] = parameters\nparameter_groups.append(param_group)\nreturn parameter_groups\ndef __call__(self, model: torch.nn.Module):\n\"\"\"Called in configure optimizers.\"\"\"\nparams_or_param_groups = self._get_parameter_groups(model)\noptimizer = self.optimizer(params_or_param_groups)\noutput = {\"optimizer\": optimizer}\nif self.lr_scheduler:\noutput.update(self.lr_scheduler(optimizer))\nreturn output\n</code></pre>"},{"location":"api/ocl/optimization/#ocl.optimization.OptimizationWrapper.__init__","title":"<code>__init__</code>","text":"<p>Initialize OptimizationWrapper.</p> <p>Parameters:</p> Name Type Description Default <code>optimizer</code> <code>Optimizer</code> <p>The oprimizer that should be used to optimize the parameters.</p> required <code>lr_scheduler</code> <code>Optional[Callable[[Optimizer], Dict[str, Any]]]</code> <p>The LR scheduling callable that should be used.  This should be a callable that returns a dict for updating the optimizer output in pytorch_lightning. See ocl.scheduling.exponential_decay_with_optional_warmup for an example of such a callable.</p> <code>None</code> <code>parameter_groups</code> <code>Optional[List[Dict[str, Any]]]</code> <p>Define parameter groups which have different optimizer parameters. Each element of the list should at least one of two keys <code>params</code> (for defining parameters based on their path in the model) or <code>predicate</code> (for defining parameters using a predicate function which returns true if the parameter should be included). For an example on how to use this parameter_groups, see <code>configs/experiment/examples/parameter_groups.yaml</code>.</p> <code>None</code> Source code in <code>ocl/optimization.py</code> <pre><code>def __init__(\nself,\noptimizer: Optimizer,\nlr_scheduler: Optional[Callable[[Optimizer], Dict[str, Any]]] = None,\nparameter_groups: Optional[List[Dict[str, Any]]] = None,\n):\n\"\"\"Initialize OptimizationWrapper.\n    Args:\n        optimizer: The oprimizer that should be used to optimize the parameters.\n        lr_scheduler: The LR scheduling callable that should be used.  This should\n            be a callable that returns a dict for updating the optimizer output in\n            pytorch_lightning. See [ocl.scheduling.exponential_decay_with_optional_warmup][]\n            for an example of such a callable.\n        parameter_groups: Define parameter groups which have different optimizer parameters.\n            Each element of the list should at least one of two keys `params` (for defining\n            parameters based on their path in the model) or `predicate` (for defining parameters\n            using a predicate function which returns true if the parameter should be included).\n            For an example on how to use this parameter_groups, see\n            `configs/experiment/examples/parameter_groups.yaml`.\n    \"\"\"\nself.optimizer = optimizer\nself.lr_scheduler = lr_scheduler\nself.parameter_group_specs = parameter_groups\nif self.parameter_group_specs:\nfor idx, param_group_spec in enumerate(self.parameter_group_specs):\nif \"params\" not in param_group_spec:\nraise ValueError(f'Parameter group {idx + 1} does not contain key \"params\"')\nparam_spec = param_group_spec[\"params\"]\nif isinstance(param_spec, str):\nparam_group_spec[\"params\"] = [param_spec]\nelif isinstance(param_spec, Iterable):\nparam_group_spec[\"params\"] = list(param_spec)\nelse:\nraise ValueError(\nf'\"params\" for parameter group {idx + 1} is not of type str or iterable'\n)\nif \"predicate\" in param_group_spec:\nif not callable(param_group_spec[\"predicate\"]):\nraise ValueError(\nf'\"predicate\" for parameter group {idx + 1} is not a callable'\n)\n</code></pre>"},{"location":"api/ocl/optimization/#ocl.optimization.OptimizationWrapper.__call__","title":"<code>__call__</code>","text":"<p>Called in configure optimizers.</p> Source code in <code>ocl/optimization.py</code> <pre><code>def __call__(self, model: torch.nn.Module):\n\"\"\"Called in configure optimizers.\"\"\"\nparams_or_param_groups = self._get_parameter_groups(model)\noptimizer = self.optimizer(params_or_param_groups)\noutput = {\"optimizer\": optimizer}\nif self.lr_scheduler:\noutput.update(self.lr_scheduler(optimizer))\nreturn output\n</code></pre>"},{"location":"api/ocl/perceptual_grouping/","title":"ocl.perceptual_grouping","text":"<p>Implementations of perceptual grouping algorithms.</p> <p>We denote methods that group input feature together into slots of objects (either unconditionally) or via additional conditioning signals as perceptual grouping modules.</p>"},{"location":"api/ocl/perceptual_grouping/#ocl.perceptual_grouping.SlotAttention","title":"<code>SlotAttention</code>","text":"<p>         Bases: <code>nn.Module</code></p> <p>Implementation of SlotAttention.</p> <p>Based on the slot attention implementation of Phil Wang available at: https://github.com/lucidrains/slot-attention</p> Source code in <code>ocl/perceptual_grouping.py</code> <pre><code>class SlotAttention(nn.Module):\n\"\"\"Implementation of SlotAttention.\n    Based on the slot attention implementation of Phil Wang available at:\n    https://github.com/lucidrains/slot-attention\n    \"\"\"\ndef __init__(\nself,\ndim: int,\nfeature_dim: int,\nkvq_dim: Optional[int] = None,\nn_heads: int = 1,\niters: int = 3,\neps: float = 1e-8,\nff_mlp: Optional[nn.Module] = None,\nuse_projection_bias: bool = False,\nuse_implicit_differentiation: bool = False,\n):\nsuper().__init__()\nself.dim = dim\nself.n_heads = n_heads\nself.iters = iters\nself.eps = eps\nself.use_implicit_differentiation = use_implicit_differentiation\nif kvq_dim is None:\nself.kvq_dim = dim\nelse:\nself.kvq_dim = kvq_dim\nif self.kvq_dim % self.n_heads != 0:\nraise ValueError(\"Key, value, query dimensions must be divisible by number of heads.\")\nself.dims_per_head = self.kvq_dim // self.n_heads\nself.scale = self.dims_per_head**-0.5\nself.to_q = nn.Linear(dim, self.kvq_dim, bias=use_projection_bias)\nself.to_k = nn.Linear(feature_dim, self.kvq_dim, bias=use_projection_bias)\nself.to_v = nn.Linear(feature_dim, self.kvq_dim, bias=use_projection_bias)\nself.gru = nn.GRUCell(self.kvq_dim, dim)\nself.norm_input = nn.LayerNorm(feature_dim)\nself.norm_slots = nn.LayerNorm(dim)\nself.ff_mlp = ff_mlp\ndef step(self, slots, k, v, masks=None):\nbs, n_slots, _ = slots.shape\nslots_prev = slots\nslots = self.norm_slots(slots)\nq = self.to_q(slots).view(bs, n_slots, self.n_heads, self.dims_per_head)\ndots = torch.einsum(\"bihd,bjhd-&gt;bihj\", q, k) * self.scale\nif masks is not None:\n# Masked slots should not take part in the competition for features. By replacing their\n# dot-products with -inf, their attention values will become zero within the softmax.\ndots.masked_fill_(masks.to(torch.bool).view(bs, n_slots, 1, 1), float(\"-inf\"))\nattn = dots.flatten(1, 2).softmax(dim=1)  # Take softmax over slots and heads\nattn = attn.view(bs, n_slots, self.n_heads, -1)\nattn_before_reweighting = attn\nattn = attn + self.eps\nattn = attn / attn.sum(dim=-1, keepdim=True)\nupdates = torch.einsum(\"bjhd,bihj-&gt;bihd\", v, attn)\nslots = self.gru(updates.reshape(-1, self.kvq_dim), slots_prev.reshape(-1, self.dim))\nslots = slots.reshape(bs, -1, self.dim)\nif self.ff_mlp:\nslots = self.ff_mlp(slots)\nreturn slots, attn_before_reweighting.mean(dim=2)\ndef iterate(self, slots, k, v, masks=None):\nfor _ in range(self.iters):\nslots, attn = self.step(slots, k, v, masks)\nreturn slots, attn\ndef forward(\nself, inputs: torch.Tensor, conditioning: torch.Tensor, masks: Optional[torch.Tensor] = None\n):\nb, n, d = inputs.shape\nslots = conditioning\ninputs = self.norm_input(inputs)\nk = self.to_k(inputs).view(b, n, self.n_heads, self.dims_per_head)\nv = self.to_v(inputs).view(b, n, self.n_heads, self.dims_per_head)\nif self.use_implicit_differentiation:\nslots, attn = self.iterate(slots, k, v, masks)\nslots, attn = self.step(slots.detach(), k, v, masks)\nelse:\nslots, attn = self.iterate(slots, k, v, masks)\nreturn slots, attn\n</code></pre>"},{"location":"api/ocl/perceptual_grouping/#ocl.perceptual_grouping.SlotAttentionGrouping","title":"<code>SlotAttentionGrouping</code>","text":"<p>         Bases: <code>nn.Module</code></p> <p>Implementation of SlotAttention for perceptual grouping.</p> Source code in <code>ocl/perceptual_grouping.py</code> <pre><code>class SlotAttentionGrouping(nn.Module):\n\"\"\"Implementation of SlotAttention for perceptual grouping.\"\"\"\ndef __init__(\nself,\nfeature_dim: int,\nobject_dim: int,\nkvq_dim: Optional[int] = None,\nn_heads: int = 1,\niters: int = 3,\neps: float = 1e-8,\nff_mlp: Optional[nn.Module] = None,\npositional_embedding: Optional[nn.Module] = None,\nuse_projection_bias: bool = False,\nuse_implicit_differentiation: bool = False,\nuse_empty_slot_for_masked_slots: bool = False,\n):\n\"\"\"Initialize Slot Attention Grouping.\n        Args:\n            feature_dim: Dimensionality of features to slot attention (after positional encoding).\n            object_dim: Dimensionality of slots.\n            kvq_dim: Dimensionality after projecting to keys, values, and queries. If `None`,\n                `object_dim` is used.\n            n_heads: Number of heads slot attention uses.\n            iters: Number of slot attention iterations.\n            eps: Epsilon in slot attention.\n            ff_mlp: Optional module applied slot-wise after GRU update.\n            positional_embedding: Optional module applied to the features before slot attention,\n                adding positional encoding.\n            use_projection_bias: Whether to use biases in key, value, query projections.\n            use_implicit_differentiation: Whether to use implicit differentiation trick. If true,\n                performs one more iteration of slot attention that is used for the gradient step\n                after `iters` iterations of slot attention without gradients. Faster and more memory\n                efficient than the standard version, but can not backpropagate gradients to the\n                conditioning input.\n            use_empty_slot_for_masked_slots: Replace slots masked with a learnt empty slot vector.\n        \"\"\"\nsuper().__init__()\nself._object_dim = object_dim\nself.slot_attention = SlotAttention(\ndim=object_dim,\nfeature_dim=feature_dim,\nkvq_dim=kvq_dim,\nn_heads=n_heads,\niters=iters,\neps=eps,\nff_mlp=ff_mlp,\nuse_projection_bias=use_projection_bias,\nuse_implicit_differentiation=use_implicit_differentiation,\n)\nself.positional_embedding = positional_embedding\nif use_empty_slot_for_masked_slots:\nself.empty_slot = nn.Parameter(torch.randn(object_dim) * object_dim**-0.5)\nelse:\nself.empty_slot = None\n@property\ndef object_dim(self):\nreturn self._object_dim\ndef forward(\nself,\nfeature: ocl.typing.FeatureExtractorOutput,\nconditioning: ocl.typing.ConditioningOutput,\nslot_mask: Optional[ocl.typing.EmptyIndicator] = None,\n) -&gt; ocl.typing.PerceptualGroupingOutput:\n\"\"\"Apply slot attention based perceptual grouping.\n        Args:\n            feature: Features used for grouping.\n            conditioning: Initial conditioning vectors for slots.\n            slot_mask: Slot mask where true indicates that the slot should be masked.\n        Returns:\n            The grouped features.\n        \"\"\"\nif self.positional_embedding:\nfeature = self.positional_embedding(feature.features, feature.positions)\nelse:\nfeature = feature.features\nslots, attn = self.slot_attention(feature, conditioning, slot_mask)\nif slot_mask is not None and self.empty_slot is not None:\nslots[slot_mask] = self.empty_slot.to(dtype=slots.dtype)\nreturn ocl.typing.PerceptualGroupingOutput(\nslots, feature_attributions=attn, is_empty=slot_mask\n)\n</code></pre>"},{"location":"api/ocl/perceptual_grouping/#ocl.perceptual_grouping.SlotAttentionGrouping.__init__","title":"<code>__init__</code>","text":"<p>Initialize Slot Attention Grouping.</p> <p>Parameters:</p> Name Type Description Default <code>feature_dim</code> <code>int</code> <p>Dimensionality of features to slot attention (after positional encoding).</p> required <code>object_dim</code> <code>int</code> <p>Dimensionality of slots.</p> required <code>kvq_dim</code> <code>Optional[int]</code> <p>Dimensionality after projecting to keys, values, and queries. If <code>None</code>, <code>object_dim</code> is used.</p> <code>None</code> <code>n_heads</code> <code>int</code> <p>Number of heads slot attention uses.</p> <code>1</code> <code>iters</code> <code>int</code> <p>Number of slot attention iterations.</p> <code>3</code> <code>eps</code> <code>float</code> <p>Epsilon in slot attention.</p> <code>1e-08</code> <code>ff_mlp</code> <code>Optional[nn.Module]</code> <p>Optional module applied slot-wise after GRU update.</p> <code>None</code> <code>positional_embedding</code> <code>Optional[nn.Module]</code> <p>Optional module applied to the features before slot attention, adding positional encoding.</p> <code>None</code> <code>use_projection_bias</code> <code>bool</code> <p>Whether to use biases in key, value, query projections.</p> <code>False</code> <code>use_implicit_differentiation</code> <code>bool</code> <p>Whether to use implicit differentiation trick. If true, performs one more iteration of slot attention that is used for the gradient step after <code>iters</code> iterations of slot attention without gradients. Faster and more memory efficient than the standard version, but can not backpropagate gradients to the conditioning input.</p> <code>False</code> <code>use_empty_slot_for_masked_slots</code> <code>bool</code> <p>Replace slots masked with a learnt empty slot vector.</p> <code>False</code> Source code in <code>ocl/perceptual_grouping.py</code> <pre><code>def __init__(\nself,\nfeature_dim: int,\nobject_dim: int,\nkvq_dim: Optional[int] = None,\nn_heads: int = 1,\niters: int = 3,\neps: float = 1e-8,\nff_mlp: Optional[nn.Module] = None,\npositional_embedding: Optional[nn.Module] = None,\nuse_projection_bias: bool = False,\nuse_implicit_differentiation: bool = False,\nuse_empty_slot_for_masked_slots: bool = False,\n):\n\"\"\"Initialize Slot Attention Grouping.\n    Args:\n        feature_dim: Dimensionality of features to slot attention (after positional encoding).\n        object_dim: Dimensionality of slots.\n        kvq_dim: Dimensionality after projecting to keys, values, and queries. If `None`,\n            `object_dim` is used.\n        n_heads: Number of heads slot attention uses.\n        iters: Number of slot attention iterations.\n        eps: Epsilon in slot attention.\n        ff_mlp: Optional module applied slot-wise after GRU update.\n        positional_embedding: Optional module applied to the features before slot attention,\n            adding positional encoding.\n        use_projection_bias: Whether to use biases in key, value, query projections.\n        use_implicit_differentiation: Whether to use implicit differentiation trick. If true,\n            performs one more iteration of slot attention that is used for the gradient step\n            after `iters` iterations of slot attention without gradients. Faster and more memory\n            efficient than the standard version, but can not backpropagate gradients to the\n            conditioning input.\n        use_empty_slot_for_masked_slots: Replace slots masked with a learnt empty slot vector.\n    \"\"\"\nsuper().__init__()\nself._object_dim = object_dim\nself.slot_attention = SlotAttention(\ndim=object_dim,\nfeature_dim=feature_dim,\nkvq_dim=kvq_dim,\nn_heads=n_heads,\niters=iters,\neps=eps,\nff_mlp=ff_mlp,\nuse_projection_bias=use_projection_bias,\nuse_implicit_differentiation=use_implicit_differentiation,\n)\nself.positional_embedding = positional_embedding\nif use_empty_slot_for_masked_slots:\nself.empty_slot = nn.Parameter(torch.randn(object_dim) * object_dim**-0.5)\nelse:\nself.empty_slot = None\n</code></pre>"},{"location":"api/ocl/perceptual_grouping/#ocl.perceptual_grouping.SlotAttentionGrouping.forward","title":"<code>forward</code>","text":"<p>Apply slot attention based perceptual grouping.</p> <p>Parameters:</p> Name Type Description Default <code>feature</code> <code>ocl.typing.FeatureExtractorOutput</code> <p>Features used for grouping.</p> required <code>conditioning</code> <code>ocl.typing.ConditioningOutput</code> <p>Initial conditioning vectors for slots.</p> required <code>slot_mask</code> <code>Optional[ocl.typing.EmptyIndicator]</code> <p>Slot mask where true indicates that the slot should be masked.</p> <code>None</code> <p>Returns:</p> Type Description <code>ocl.typing.PerceptualGroupingOutput</code> <p>The grouped features.</p> Source code in <code>ocl/perceptual_grouping.py</code> <pre><code>def forward(\nself,\nfeature: ocl.typing.FeatureExtractorOutput,\nconditioning: ocl.typing.ConditioningOutput,\nslot_mask: Optional[ocl.typing.EmptyIndicator] = None,\n) -&gt; ocl.typing.PerceptualGroupingOutput:\n\"\"\"Apply slot attention based perceptual grouping.\n    Args:\n        feature: Features used for grouping.\n        conditioning: Initial conditioning vectors for slots.\n        slot_mask: Slot mask where true indicates that the slot should be masked.\n    Returns:\n        The grouped features.\n    \"\"\"\nif self.positional_embedding:\nfeature = self.positional_embedding(feature.features, feature.positions)\nelse:\nfeature = feature.features\nslots, attn = self.slot_attention(feature, conditioning, slot_mask)\nif slot_mask is not None and self.empty_slot is not None:\nslots[slot_mask] = self.empty_slot.to(dtype=slots.dtype)\nreturn ocl.typing.PerceptualGroupingOutput(\nslots, feature_attributions=attn, is_empty=slot_mask\n)\n</code></pre>"},{"location":"api/ocl/perceptual_grouping/#ocl.perceptual_grouping.StickBreakingGrouping","title":"<code>StickBreakingGrouping</code>","text":"<p>         Bases: <code>nn.Module</code></p> <p>Perceptual grouping based on a stick-breaking process.</p> <p>The idea is to pick a random feature from a yet unexplained part of the feature map, then see which parts of the feature map are \"explained\" by this feature using a kernel distance. This process is iterated until some termination criterion is reached. In principle, this process allows to extract a variable number of slots per image.</p> <p>This is based on Engelcke et al, GENESIS-V2: Inferring Unordered Object Representations without Iterative Refinement, http://arxiv.org/abs/2104.09958. Our implementation here differs a bit from the one described there:</p> <ul> <li>It only implements one kernel distance, the Gaussian kernel</li> <li>It does not take features positions into account when computing the kernel distances</li> <li>It L2-normalises the input features to get comparable scales of the kernel distance</li> <li>It has multiple termination criteria, namely termination based on fraction explained, mean   mask value, and min-max mask value. GENESIS-V2 implements termination based on mean mask   value, but does not mention it in the paper. Note that by default, all termination criteria   are disabled.</li> </ul> Source code in <code>ocl/perceptual_grouping.py</code> <pre><code>class StickBreakingGrouping(nn.Module):\n\"\"\"Perceptual grouping based on a stick-breaking process.\n    The idea is to pick a random feature from a yet unexplained part of the feature map, then see\n    which parts of the feature map are \"explained\" by this feature using a kernel distance. This\n    process is iterated until some termination criterion is reached. In principle, this process\n    allows to extract a variable number of slots per image.\n    This is based on Engelcke et al, GENESIS-V2: Inferring Unordered Object Representations without\n    Iterative Refinement, http://arxiv.org/abs/2104.09958. Our implementation here differs a bit from\n    the one described there:\n    - It only implements one kernel distance, the Gaussian kernel\n    - It does not take features positions into account when computing the kernel distances\n    - It L2-normalises the input features to get comparable scales of the kernel distance\n    - It has multiple termination criteria, namely termination based on fraction explained, mean\n      mask value, and min-max mask value. GENESIS-V2 implements termination based on mean mask\n      value, but does not mention it in the paper. Note that by default, all termination criteria\n      are disabled.\n    \"\"\"\ndef __init__(\nself,\nobject_dim: int,\nfeature_dim: int,\nn_slots: int,\nkernel_var: float = 1.0,\nlearn_kernel_var: bool = False,\nmax_unexplained: float = 0.0,\nmin_slot_mask: float = 0.0,\nmin_max_mask_value: float = 0.0,\nearly_termination: bool = False,\nadd_unexplained: bool = False,\neps: float = 1e-8,\ndetach_features: bool = False,\nuse_input_layernorm: bool = False,\n):\n\"\"\"Initialize stick-breaking-based perceptual grouping.\n        Args:\n            object_dim: Dimensionality of extracted slots.\n            feature_dim: Dimensionality of features to operate on.\n            n_slots: Maximum number of slots.\n            kernel_var: Variance in Gaussian kernel.\n            learn_kernel_var: Whether kernel variance should be included as trainable parameter.\n            max_unexplained: If fraction of unexplained features drops under this value,\n                drop the slot.\n            min_slot_mask: If slot mask has lower average value than this value, drop the slot.\n            min_max_mask_value: If slot mask's maximum value is lower than this value,\n                drop the slot.\n            early_termination: If true, all slots after the first dropped slot are also dropped.\n            add_unexplained: If true, add a slot that covers all unexplained parts at the point\n                when the first slot was dropped.\n            eps: Minimum value for masks.\n            detach_features: If true, detach input features such that no gradient flows through\n                this operation.\n            use_input_layernorm: Apply layernorm to features prior to grouping.\n        \"\"\"\nsuper().__init__()\nself.n_slots = n_slots\nself.object_dim = object_dim\nassert kernel_var &gt; 0.0\nif learn_kernel_var:\nself.kernel_logvar = nn.Parameter(torch.tensor(math.log(kernel_var)))\nelse:\nself.register_buffer(\"kernel_logvar\", torch.tensor(math.log(kernel_var)))\nassert 0.0 &lt;= max_unexplained &lt; 1.0\nself.max_unexplained = max_unexplained\nassert 0.0 &lt;= min_slot_mask &lt; 1.0\nself.min_slot_mask = min_slot_mask\nassert 0.0 &lt;= min_max_mask_value &lt; 1.0\nself.min_max_mask_value = min_max_mask_value\nself.early_termination = early_termination\nself.add_unexplained = add_unexplained\nif add_unexplained and not early_termination:\nraise ValueError(\"`add_unexplained=True` only works with `early_termination=True`\")\nself.eps = eps\nself.log_eps = math.log(eps)\nself.detach_features = detach_features\nif use_input_layernorm:\nself.in_proj = nn.Sequential(\nnn.LayerNorm(feature_dim), nn.Linear(feature_dim, feature_dim)\n)\ntorch.nn.init.xavier_uniform_(self.in_proj[-1].weight)\ntorch.nn.init.zeros_(self.in_proj[-1].bias)\nelse:\nself.in_proj = nn.Linear(feature_dim, feature_dim)\ntorch.nn.init.xavier_uniform_(self.in_proj.weight)\ntorch.nn.init.zeros_(self.in_proj.bias)\nself.out_proj = nn.Linear(feature_dim, object_dim)\ntorch.nn.init.xavier_uniform_(self.out_proj.weight)\ntorch.nn.init.zeros_(self.out_proj.bias)\ndef forward(\nself, features: ocl.typing.FeatureExtractorOutput\n) -&gt; ocl.typing.PerceptualGroupingOutput:\n\"\"\"Apply stick-breaking-based perceptual grouping to input features.\n        Args:\n            features: Features that should be grouped.\n        Returns:\n            Grouped features.\n        \"\"\"\nfeatures = features.features\nbs, n_features, feature_dim = features.shape\nif self.detach_features:\nfeatures = features.detach()\nproj_features = torch.nn.functional.normalize(self.in_proj(features), dim=-1)\n# The scope keep tracks of the unexplained parts of the feature map\nlog_scope = torch.zeros_like(features[:, :, 0])\n# Seeds are used for random sampling of features\nlog_seeds = torch.rand_like(log_scope).clamp_min(self.eps).log()\nslot_masks = []\nlog_scopes = []\n# Always iterate for `n_iters` steps for batching reasons. Termination is modeled afterwards.\nn_iters = self.n_slots - 1 if self.add_unexplained else self.n_slots\nfor _ in range(n_iters):\nlog_scopes.append(log_scope)\n# Sample random features from unexplained parts of the feature map\nrand_idxs = torch.argmax(log_scope + log_seeds, dim=1)\ncur_centers = proj_features.gather(\n1, rand_idxs.view(bs, 1, 1).expand(-1, -1, feature_dim)\n)\n# Compute similarity between selected features and other features. alpha can be\n# considered an attention mask.\ndists = torch.sum((cur_centers - proj_features) ** 2, dim=-1)\nlog_alpha = (-dists / self.kernel_logvar.exp()).clamp_min(self.log_eps)\n# To get the slot mask, we subtract already explained parts from alpha using the scope\nmask = (log_scope + log_alpha).exp()\nslot_masks.append(mask)\n# Update scope by masking out parts explained by the current iteration\nlog_1m_alpha = (1 - log_alpha.exp()).clamp_min(self.eps).log()\nlog_scope = log_scope + log_1m_alpha\nif self.add_unexplained:\nslot_masks.append(log_scope.exp())\nlog_scopes.append(log_scope)\nslot_masks = torch.stack(slot_masks, dim=1)\nscopes = torch.stack(log_scopes, dim=1).exp()\n# Compute criteria for ignoring slots\nempty_slots = torch.zeros_like(slot_masks[:, :, 0], dtype=torch.bool)\n# When fraction of unexplained features drops under threshold, ignore slot,\nempty_slots |= scopes.mean(dim=-1) &lt; self.max_unexplained\n# or when slot's mean mask is under threshold, ignore slot,\nempty_slots |= slot_masks.mean(dim=-1) &lt; self.min_slot_mask\n# or when slot's masks maximum value is under threshold, ignore slot.\nempty_slots |= slot_masks.max(dim=-1).values &lt; self.min_max_mask_value\nif self.early_termination:\n# Simulate early termination by marking all slots after the first empty slot as empty\nempty_slots = torch.cummax(empty_slots, dim=1).values\nif self.add_unexplained:\n# After termination, add one more slot using the unexplained parts at that point\nfirst_empty = torch.argmax(empty_slots.to(torch.int32), dim=1).unsqueeze(-1)\nempty_slots.scatter_(1, first_empty, torch.zeros_like(first_empty, dtype=torch.bool))\nidxs = first_empty.view(bs, 1, 1).expand(-1, -1, n_features)\nunexplained = scopes.gather(1, idxs)\nslot_masks.scatter_(1, idxs, unexplained)\n# Create slot representations as weighted average of feature map\nslots = torch.einsum(\"bkp,bpd-&gt;bkd\", slot_masks, features)\nslots = slots / slot_masks.sum(dim=-1, keepdim=True).clamp_min(self.eps)\nslots = self.out_proj(slots)\n# Zero-out masked slots\nslots.masked_fill_(empty_slots.view(bs, slots.shape[1], 1), 0.0)\nreturn ocl.typing.PerceptualGroupingOutput(\nslots, feature_attributions=slot_masks, is_empty=empty_slots\n)\n</code></pre>"},{"location":"api/ocl/perceptual_grouping/#ocl.perceptual_grouping.StickBreakingGrouping.__init__","title":"<code>__init__</code>","text":"<p>Initialize stick-breaking-based perceptual grouping.</p> <p>Parameters:</p> Name Type Description Default <code>object_dim</code> <code>int</code> <p>Dimensionality of extracted slots.</p> required <code>feature_dim</code> <code>int</code> <p>Dimensionality of features to operate on.</p> required <code>n_slots</code> <code>int</code> <p>Maximum number of slots.</p> required <code>kernel_var</code> <code>float</code> <p>Variance in Gaussian kernel.</p> <code>1.0</code> <code>learn_kernel_var</code> <code>bool</code> <p>Whether kernel variance should be included as trainable parameter.</p> <code>False</code> <code>max_unexplained</code> <code>float</code> <p>If fraction of unexplained features drops under this value, drop the slot.</p> <code>0.0</code> <code>min_slot_mask</code> <code>float</code> <p>If slot mask has lower average value than this value, drop the slot.</p> <code>0.0</code> <code>min_max_mask_value</code> <code>float</code> <p>If slot mask's maximum value is lower than this value, drop the slot.</p> <code>0.0</code> <code>early_termination</code> <code>bool</code> <p>If true, all slots after the first dropped slot are also dropped.</p> <code>False</code> <code>add_unexplained</code> <code>bool</code> <p>If true, add a slot that covers all unexplained parts at the point when the first slot was dropped.</p> <code>False</code> <code>eps</code> <code>float</code> <p>Minimum value for masks.</p> <code>1e-08</code> <code>detach_features</code> <code>bool</code> <p>If true, detach input features such that no gradient flows through this operation.</p> <code>False</code> <code>use_input_layernorm</code> <code>bool</code> <p>Apply layernorm to features prior to grouping.</p> <code>False</code> Source code in <code>ocl/perceptual_grouping.py</code> <pre><code>def __init__(\nself,\nobject_dim: int,\nfeature_dim: int,\nn_slots: int,\nkernel_var: float = 1.0,\nlearn_kernel_var: bool = False,\nmax_unexplained: float = 0.0,\nmin_slot_mask: float = 0.0,\nmin_max_mask_value: float = 0.0,\nearly_termination: bool = False,\nadd_unexplained: bool = False,\neps: float = 1e-8,\ndetach_features: bool = False,\nuse_input_layernorm: bool = False,\n):\n\"\"\"Initialize stick-breaking-based perceptual grouping.\n    Args:\n        object_dim: Dimensionality of extracted slots.\n        feature_dim: Dimensionality of features to operate on.\n        n_slots: Maximum number of slots.\n        kernel_var: Variance in Gaussian kernel.\n        learn_kernel_var: Whether kernel variance should be included as trainable parameter.\n        max_unexplained: If fraction of unexplained features drops under this value,\n            drop the slot.\n        min_slot_mask: If slot mask has lower average value than this value, drop the slot.\n        min_max_mask_value: If slot mask's maximum value is lower than this value,\n            drop the slot.\n        early_termination: If true, all slots after the first dropped slot are also dropped.\n        add_unexplained: If true, add a slot that covers all unexplained parts at the point\n            when the first slot was dropped.\n        eps: Minimum value for masks.\n        detach_features: If true, detach input features such that no gradient flows through\n            this operation.\n        use_input_layernorm: Apply layernorm to features prior to grouping.\n    \"\"\"\nsuper().__init__()\nself.n_slots = n_slots\nself.object_dim = object_dim\nassert kernel_var &gt; 0.0\nif learn_kernel_var:\nself.kernel_logvar = nn.Parameter(torch.tensor(math.log(kernel_var)))\nelse:\nself.register_buffer(\"kernel_logvar\", torch.tensor(math.log(kernel_var)))\nassert 0.0 &lt;= max_unexplained &lt; 1.0\nself.max_unexplained = max_unexplained\nassert 0.0 &lt;= min_slot_mask &lt; 1.0\nself.min_slot_mask = min_slot_mask\nassert 0.0 &lt;= min_max_mask_value &lt; 1.0\nself.min_max_mask_value = min_max_mask_value\nself.early_termination = early_termination\nself.add_unexplained = add_unexplained\nif add_unexplained and not early_termination:\nraise ValueError(\"`add_unexplained=True` only works with `early_termination=True`\")\nself.eps = eps\nself.log_eps = math.log(eps)\nself.detach_features = detach_features\nif use_input_layernorm:\nself.in_proj = nn.Sequential(\nnn.LayerNorm(feature_dim), nn.Linear(feature_dim, feature_dim)\n)\ntorch.nn.init.xavier_uniform_(self.in_proj[-1].weight)\ntorch.nn.init.zeros_(self.in_proj[-1].bias)\nelse:\nself.in_proj = nn.Linear(feature_dim, feature_dim)\ntorch.nn.init.xavier_uniform_(self.in_proj.weight)\ntorch.nn.init.zeros_(self.in_proj.bias)\nself.out_proj = nn.Linear(feature_dim, object_dim)\ntorch.nn.init.xavier_uniform_(self.out_proj.weight)\ntorch.nn.init.zeros_(self.out_proj.bias)\n</code></pre>"},{"location":"api/ocl/perceptual_grouping/#ocl.perceptual_grouping.StickBreakingGrouping.forward","title":"<code>forward</code>","text":"<p>Apply stick-breaking-based perceptual grouping to input features.</p> <p>Parameters:</p> Name Type Description Default <code>features</code> <code>ocl.typing.FeatureExtractorOutput</code> <p>Features that should be grouped.</p> required <p>Returns:</p> Type Description <code>ocl.typing.PerceptualGroupingOutput</code> <p>Grouped features.</p> Source code in <code>ocl/perceptual_grouping.py</code> <pre><code>def forward(\nself, features: ocl.typing.FeatureExtractorOutput\n) -&gt; ocl.typing.PerceptualGroupingOutput:\n\"\"\"Apply stick-breaking-based perceptual grouping to input features.\n    Args:\n        features: Features that should be grouped.\n    Returns:\n        Grouped features.\n    \"\"\"\nfeatures = features.features\nbs, n_features, feature_dim = features.shape\nif self.detach_features:\nfeatures = features.detach()\nproj_features = torch.nn.functional.normalize(self.in_proj(features), dim=-1)\n# The scope keep tracks of the unexplained parts of the feature map\nlog_scope = torch.zeros_like(features[:, :, 0])\n# Seeds are used for random sampling of features\nlog_seeds = torch.rand_like(log_scope).clamp_min(self.eps).log()\nslot_masks = []\nlog_scopes = []\n# Always iterate for `n_iters` steps for batching reasons. Termination is modeled afterwards.\nn_iters = self.n_slots - 1 if self.add_unexplained else self.n_slots\nfor _ in range(n_iters):\nlog_scopes.append(log_scope)\n# Sample random features from unexplained parts of the feature map\nrand_idxs = torch.argmax(log_scope + log_seeds, dim=1)\ncur_centers = proj_features.gather(\n1, rand_idxs.view(bs, 1, 1).expand(-1, -1, feature_dim)\n)\n# Compute similarity between selected features and other features. alpha can be\n# considered an attention mask.\ndists = torch.sum((cur_centers - proj_features) ** 2, dim=-1)\nlog_alpha = (-dists / self.kernel_logvar.exp()).clamp_min(self.log_eps)\n# To get the slot mask, we subtract already explained parts from alpha using the scope\nmask = (log_scope + log_alpha).exp()\nslot_masks.append(mask)\n# Update scope by masking out parts explained by the current iteration\nlog_1m_alpha = (1 - log_alpha.exp()).clamp_min(self.eps).log()\nlog_scope = log_scope + log_1m_alpha\nif self.add_unexplained:\nslot_masks.append(log_scope.exp())\nlog_scopes.append(log_scope)\nslot_masks = torch.stack(slot_masks, dim=1)\nscopes = torch.stack(log_scopes, dim=1).exp()\n# Compute criteria for ignoring slots\nempty_slots = torch.zeros_like(slot_masks[:, :, 0], dtype=torch.bool)\n# When fraction of unexplained features drops under threshold, ignore slot,\nempty_slots |= scopes.mean(dim=-1) &lt; self.max_unexplained\n# or when slot's mean mask is under threshold, ignore slot,\nempty_slots |= slot_masks.mean(dim=-1) &lt; self.min_slot_mask\n# or when slot's masks maximum value is under threshold, ignore slot.\nempty_slots |= slot_masks.max(dim=-1).values &lt; self.min_max_mask_value\nif self.early_termination:\n# Simulate early termination by marking all slots after the first empty slot as empty\nempty_slots = torch.cummax(empty_slots, dim=1).values\nif self.add_unexplained:\n# After termination, add one more slot using the unexplained parts at that point\nfirst_empty = torch.argmax(empty_slots.to(torch.int32), dim=1).unsqueeze(-1)\nempty_slots.scatter_(1, first_empty, torch.zeros_like(first_empty, dtype=torch.bool))\nidxs = first_empty.view(bs, 1, 1).expand(-1, -1, n_features)\nunexplained = scopes.gather(1, idxs)\nslot_masks.scatter_(1, idxs, unexplained)\n# Create slot representations as weighted average of feature map\nslots = torch.einsum(\"bkp,bpd-&gt;bkd\", slot_masks, features)\nslots = slots / slot_masks.sum(dim=-1, keepdim=True).clamp_min(self.eps)\nslots = self.out_proj(slots)\n# Zero-out masked slots\nslots.masked_fill_(empty_slots.view(bs, slots.shape[1], 1), 0.0)\nreturn ocl.typing.PerceptualGroupingOutput(\nslots, feature_attributions=slot_masks, is_empty=empty_slots\n)\n</code></pre>"},{"location":"api/ocl/perceptual_grouping/#ocl.perceptual_grouping.KMeansGrouping","title":"<code>KMeansGrouping</code>","text":"<p>         Bases: <code>nn.Module</code></p> <p>Simple K-means clustering based grouping.</p> Source code in <code>ocl/perceptual_grouping.py</code> <pre><code>class KMeansGrouping(nn.Module):\n\"\"\"Simple K-means clustering based grouping.\"\"\"\ndef __init__(\nself,\nn_slots: int,\nuse_l2_normalization: bool = True,\nclustering_kwargs: Optional[Dict[str, Any]] = None,\n):\nsuper().__init__()\nself._object_dim = None\nself.n_slots = n_slots\nself.use_l2_normalization = use_l2_normalization\nkwargs = clustering_kwargs if clustering_kwargs is not None else {}\nself.make_clustering = lambda: cluster.KMeans(n_clusters=n_slots, **kwargs)\n@property\ndef object_dim(self):\nreturn self._object_dim\ndef forward(\nself, feature: ocl.typing.FeatureExtractorOutput\n) -&gt; ocl.typing.PerceptualGroupingOutput:\nfeature = feature.features\nif self._object_dim is None:\nself._object_dim = feature.shape[-1]\nif self.use_l2_normalization:\nfeature = torch.nn.functional.normalize(feature, dim=-1)\nbatch_features = feature.detach().cpu().numpy()\ncluster_ids = []\ncluster_centers = []\nfor feat in batch_features:\nclustering = self.make_clustering()\ncluster_ids.append(clustering.fit_predict(feat).astype(numpy.int64))\ncluster_centers.append(clustering.cluster_centers_)\ncluster_ids = torch.from_numpy(numpy.stack(cluster_ids))\ncluster_centers = torch.from_numpy(numpy.stack(cluster_centers))\nslot_masks = torch.nn.functional.one_hot(cluster_ids, num_classes=self.n_slots)\nslot_masks = slot_masks.transpose(-2, -1).to(torch.float32)\nreturn ocl.typing.PerceptualGroupingOutput(\ncluster_centers.to(feature.device), feature_attributions=slot_masks.to(feature.device)\n)\n</code></pre>"},{"location":"api/ocl/preprocessing/","title":"ocl.preprocessing","text":"<p>Data preprocessing functions.</p>"},{"location":"api/ocl/preprocessing/#ocl.preprocessing.DropEntries","title":"<code>DropEntries</code>","text":"<p>Drop entries from data dictionary.</p> Source code in <code>ocl/preprocessing.py</code> <pre><code>class DropEntries:\n\"\"\"Drop entries from data dictionary.\"\"\"\ndef __init__(self, keys: List[str]):\n\"\"\"Initialize DropEntries.\n        Args:\n            keys: Entries that should be dropped from the input dict.\n        \"\"\"\nself.keys = tuple(keys)\ndef __call__(self, data: Dict[str, Any]):\nreturn {k: v for k, v in data.items() if k not in self.keys}\n</code></pre>"},{"location":"api/ocl/preprocessing/#ocl.preprocessing.DropEntries.__init__","title":"<code>__init__</code>","text":"<p>Initialize DropEntries.</p> <p>Parameters:</p> Name Type Description Default <code>keys</code> <code>List[str]</code> <p>Entries that should be dropped from the input dict.</p> required Source code in <code>ocl/preprocessing.py</code> <pre><code>def __init__(self, keys: List[str]):\n\"\"\"Initialize DropEntries.\n    Args:\n        keys: Entries that should be dropped from the input dict.\n    \"\"\"\nself.keys = tuple(keys)\n</code></pre>"},{"location":"api/ocl/preprocessing/#ocl.preprocessing.CheckFormat","title":"<code>CheckFormat</code>","text":"<p>Check format of data.</p> Source code in <code>ocl/preprocessing.py</code> <pre><code>class CheckFormat:\n\"\"\"Check format of data.\"\"\"\ndef __init__(self, shape: List[int], one_hot: bool = False, class_dim: int = 0):\n\"\"\"Initialize CheckFormat.\n        Args:\n            shape: Shape of input tensor.\n            one_hot: Check if input tensor is one hot.\n            class_dim: Axis along which tensor should be one hot.\n        \"\"\"\nself.shape = tuple(shape)\nself.one_hot = one_hot\nself.class_dim = class_dim\ndef __call__(self, data: torch.Tensor) -&gt; torch.Tensor:\nif data.shape != self.shape:\nraise ValueError(f\"Expected shape to be {self.shape}, but is {data.shape}\")\nif self.one_hot:\nif not torch.all(data.sum(self.class_dim) == 1):\nraise ValueError(\"Data is not one-hot\")\nreturn data\n</code></pre>"},{"location":"api/ocl/preprocessing/#ocl.preprocessing.CheckFormat.__init__","title":"<code>__init__</code>","text":"<p>Initialize CheckFormat.</p> <p>Parameters:</p> Name Type Description Default <code>shape</code> <code>List[int]</code> <p>Shape of input tensor.</p> required <code>one_hot</code> <code>bool</code> <p>Check if input tensor is one hot.</p> <code>False</code> <code>class_dim</code> <code>int</code> <p>Axis along which tensor should be one hot.</p> <code>0</code> Source code in <code>ocl/preprocessing.py</code> <pre><code>def __init__(self, shape: List[int], one_hot: bool = False, class_dim: int = 0):\n\"\"\"Initialize CheckFormat.\n    Args:\n        shape: Shape of input tensor.\n        one_hot: Check if input tensor is one hot.\n        class_dim: Axis along which tensor should be one hot.\n    \"\"\"\nself.shape = tuple(shape)\nself.one_hot = one_hot\nself.class_dim = class_dim\n</code></pre>"},{"location":"api/ocl/preprocessing/#ocl.preprocessing.CompressMask","title":"<code>CompressMask</code>","text":"<p>Compress masks using a binary encoding format.</p> <p>This works for up to 64 objects.</p> Source code in <code>ocl/preprocessing.py</code> <pre><code>class CompressMask:\n\"\"\"Compress masks using a binary encoding format.\n    This works for up to 64 objects.\n    \"\"\"\ndef __call__(self, mask: numpy.ndarray) -&gt; numpy.ndarray:\nnon_empty = numpy.any(mask != 0, axis=(0, 2, 3))\n# Preserve first object beeing empty. This is often considered the\n# foreground mask and sometimes ignored.\nlast_nonempty_index = len(non_empty) - non_empty[::-1].argmax()\ninput_arr = mask[:, :last_nonempty_index]\nn_objects = input_arr.shape[1]\ndtype = numpy.uint8\nif n_objects &gt; 8:\ndtype = numpy.uint16\nif n_objects &gt; 16:\ndtype = numpy.uint32\nif n_objects &gt; 32:\ndtype = numpy.uint64\nif n_objects &gt; 64:\nraise RuntimeError(\"We do not support more than 64 objects at the moment.\")\nobject_flag = (1 &lt;&lt; numpy.arange(n_objects, dtype=dtype))[None, :, None, None]\noutput_arr = numpy.sum(input_arr.astype(dtype) * object_flag, axis=1).astype(dtype)\nreturn output_arr\n</code></pre>"},{"location":"api/ocl/preprocessing/#ocl.preprocessing.CompressedMaskToTensor","title":"<code>CompressedMaskToTensor</code>","text":"<p>Decompress a mask compressed with CompressMask.</p> Source code in <code>ocl/preprocessing.py</code> <pre><code>class CompressedMaskToTensor:\n\"\"\"Decompress a mask compressed with [CompressMask][ocl.preprocessing.CompressMask].\"\"\"\ndef __call__(self, compressed_mask: numpy.ndarray) -&gt; torch.Tensor:\nmaximum_value = numpy.max(compressed_mask)\nn_objects = 0\nwhile maximum_value &gt; 0:\nmaximum_value //= 2\nn_objects += 1\nif n_objects == 0:\n# Cover edge case of no objects.\nn_objects = 1\nsqueeze = False\nif len(compressed_mask.shape) == 2:\ncompressed_mask = compressed_mask[None, ...]\nsqueeze = True\n# Not really sure why we need to invert the order here, but it seems\n# to be necessary for the index to remain consistent between compression\n# and decompression.\nis_bit_active = (1 &lt;&lt; numpy.arange(n_objects, dtype=compressed_mask.dtype))[\nNone, :, None, None\n]\nexpanded_mask = (compressed_mask[:, None, :, :] &amp; is_bit_active) &gt; 0\nif squeeze:\nexpanded_mask = numpy.squeeze(expanded_mask, axis=0)\nreturn torch.from_numpy(expanded_mask).to(torch.float32)\n</code></pre>"},{"location":"api/ocl/preprocessing/#ocl.preprocessing.MaskToTensor","title":"<code>MaskToTensor</code>","text":"<p>Convert a segmentation mask numpy array to a tensor.</p> Source code in <code>ocl/preprocessing.py</code> <pre><code>class MaskToTensor:\n\"\"\"Convert a segmentation mask numpy array to a tensor.\"\"\"\ndef __init__(self, singleton_dim_last: bool = True):\nself.singleton_dim_last = singleton_dim_last\ndef __call__(self, mask: numpy.ndarray) -&gt; torch.Tensor:\n\"\"\"Apply transformation.\n        Args:\n            mask: Mask tensor of shape (..., K, H, W, 1), i.e. one-hot encoded\n                with K classes and any number of leading dimensions.\n        Returns:\n            Tensor of shape (..., K, H, W), containing binary entries.\n        \"\"\"\nmask_binary = mask &gt; 0.0\nif self.singleton_dim_last:\nassert mask_binary.shape[-1] == 1\nreturn torch.from_numpy(mask_binary).squeeze(-1).to(torch.float32)\nelse:\nreturn torch.from_numpy(mask_binary).to(torch.float32)\ndef __repr__(self) -&gt; str:\nreturn f\"{self.__class__.__name__}()\"\n</code></pre>"},{"location":"api/ocl/preprocessing/#ocl.preprocessing.MaskToTensor.__call__","title":"<code>__call__</code>","text":"<p>Apply transformation.</p> <p>Parameters:</p> Name Type Description Default <code>mask</code> <code>numpy.ndarray</code> <p>Mask tensor of shape (..., K, H, W, 1), i.e. one-hot encoded with K classes and any number of leading dimensions.</p> required <p>Returns:</p> Type Description <code>torch.Tensor</code> <p>Tensor of shape (..., K, H, W), containing binary entries.</p> Source code in <code>ocl/preprocessing.py</code> <pre><code>def __call__(self, mask: numpy.ndarray) -&gt; torch.Tensor:\n\"\"\"Apply transformation.\n    Args:\n        mask: Mask tensor of shape (..., K, H, W, 1), i.e. one-hot encoded\n            with K classes and any number of leading dimensions.\n    Returns:\n        Tensor of shape (..., K, H, W), containing binary entries.\n    \"\"\"\nmask_binary = mask &gt; 0.0\nif self.singleton_dim_last:\nassert mask_binary.shape[-1] == 1\nreturn torch.from_numpy(mask_binary).squeeze(-1).to(torch.float32)\nelse:\nreturn torch.from_numpy(mask_binary).to(torch.float32)\n</code></pre>"},{"location":"api/ocl/preprocessing/#ocl.preprocessing.DenseMaskToTensor","title":"<code>DenseMaskToTensor</code>","text":"<p>Convert a dense segmentation mask numpy array to a tensor.</p> <p>Mask is assumed to be of shape (..., K, H, W, 1), i.e. densely encoded with K classes and any number of leading dimensions. Returned tensor is of shape (..., K, H, W).</p> Source code in <code>ocl/preprocessing.py</code> <pre><code>class DenseMaskToTensor:\n\"\"\"Convert a dense segmentation mask numpy array to a tensor.\n    Mask is assumed to be of shape (..., K, H, W, 1), i.e. densely encoded with K classes and any\n    number of leading dimensions. Returned tensor is of shape (..., K, H, W).\n    \"\"\"\ndef __call__(self, mask: numpy.ndarray) -&gt; torch.Tensor:\nassert mask.shape[-1] == 1\nreturn torch.from_numpy(mask).squeeze(-1).to(torch.uint8)\ndef __repr__(self) -&gt; str:\nreturn f\"{self.__class__.__name__}()\"\n</code></pre>"},{"location":"api/ocl/preprocessing/#ocl.preprocessing.MultiMaskToTensor","title":"<code>MultiMaskToTensor</code>","text":"<p>Discretize mask, where multiple objects are partially masked into an exclusive binary mask.</p> Source code in <code>ocl/preprocessing.py</code> <pre><code>class MultiMaskToTensor:\n\"\"\"Discretize mask, where multiple objects are partially masked into an exclusive binary mask.\"\"\"\ndef __init__(self, axis: int = -4):\nself.axis = axis\ndef __call__(self, mask: numpy.ndarray) -&gt; torch.Tensor:\nint_mask = numpy.argmax(mask, axis=self.axis).squeeze(-1)\nout_mask = torch.nn.functional.one_hot(torch.from_numpy(int_mask), mask.shape[self.axis])\n# Ensure the object axis is again at the same location.\n# We operate on the shape prior to squeezing for axis to be consistent.\nlast_index = len(out_mask.shape) - 1\nindices = list(range(len(out_mask.shape) + 1))\nindices.insert(self.axis, last_index)\nindices = indices[:-2]  # Remove last indices as they are squeezed or inserted.\nout_mask = out_mask.permute(*indices).to(torch.float32)\nreturn out_mask\n</code></pre>"},{"location":"api/ocl/preprocessing/#ocl.preprocessing.IntegerToOneHotMask","title":"<code>IntegerToOneHotMask</code>","text":"<p>Convert an integer mask to a one-hot mask.</p> <p>Integer masks are masks where the instance ID is written into the mask. This transform expands them to a one-hot encoding.</p> Source code in <code>ocl/preprocessing.py</code> <pre><code>class IntegerToOneHotMask:\n\"\"\"Convert an integer mask to a one-hot mask.\n    Integer masks are masks where the instance ID is written into the mask.\n    This transform expands them to a one-hot encoding.\n    \"\"\"\ndef __init__(\nself,\nignore_typical_background: bool = True,\noutput_axis: int = -4,\nmax_instances: Optional[int] = None,\n):\n\"\"\"Initialize IntegerToOneHotMask.\n        Args:\n            ignore_typical_background: Ignore pixels where the mask is zero or 255.\n                This often corresponds to the background or to the segmentation boundary.\n            output_axis: Axis along which the output should be one hot.\n            max_instances: The maximum number of instances.\n        \"\"\"\nself.ignore_typical_background = ignore_typical_background\nself.output_axis = output_axis\nself.max_instances = max_instances\ndef __call__(self, array: numpy.ndarray) -&gt; numpy.ndarray:\nmax_value = array.max()\nif self.ignore_typical_background:\nif max_value == 255:\n# Replace 255 with zero, both are ignored.\narray[array == 255] = 0\nmax_value = array.max()\nmax_instances = self.max_instances if self.max_instances else max_value\nto_one_hot = numpy.concatenate(\n[\nnumpy.zeros((1, max_instances), dtype=numpy.uint8),\nnumpy.eye(max_instances, dtype=numpy.uint8),\n],\naxis=0,\n)\nelse:\nmax_instances = self.max_instances if self.max_instances else max_value\nto_one_hot = numpy.eye(max_instances + 1, dtype=numpy.uint8)\nreturn numpy.moveaxis(to_one_hot[array], -1, self.output_axis)\n</code></pre>"},{"location":"api/ocl/preprocessing/#ocl.preprocessing.IntegerToOneHotMask.__init__","title":"<code>__init__</code>","text":"<p>Initialize IntegerToOneHotMask.</p> <p>Parameters:</p> Name Type Description Default <code>ignore_typical_background</code> <code>bool</code> <p>Ignore pixels where the mask is zero or 255. This often corresponds to the background or to the segmentation boundary.</p> <code>True</code> <code>output_axis</code> <code>int</code> <p>Axis along which the output should be one hot.</p> <code>-4</code> <code>max_instances</code> <code>Optional[int]</code> <p>The maximum number of instances.</p> <code>None</code> Source code in <code>ocl/preprocessing.py</code> <pre><code>def __init__(\nself,\nignore_typical_background: bool = True,\noutput_axis: int = -4,\nmax_instances: Optional[int] = None,\n):\n\"\"\"Initialize IntegerToOneHotMask.\n    Args:\n        ignore_typical_background: Ignore pixels where the mask is zero or 255.\n            This often corresponds to the background or to the segmentation boundary.\n        output_axis: Axis along which the output should be one hot.\n        max_instances: The maximum number of instances.\n    \"\"\"\nself.ignore_typical_background = ignore_typical_background\nself.output_axis = output_axis\nself.max_instances = max_instances\n</code></pre>"},{"location":"api/ocl/preprocessing/#ocl.preprocessing.VOCInstanceMasksToDenseMasks","title":"<code>VOCInstanceMasksToDenseMasks</code>","text":"<p>Convert a segmentation mask with integer encoding into a one-hot segmentation mask.</p> <p>We use this transform as Pascal VOC segmentatation and object annotations seems to not be aligned.</p> Source code in <code>ocl/preprocessing.py</code> <pre><code>class VOCInstanceMasksToDenseMasks:\n\"\"\"Convert a segmentation mask with integer encoding into a one-hot segmentation mask.\n    We use this transform as Pascal VOC segmentatation and object annotations seems to not\n    be aligned.\n    \"\"\"\ndef __init__(\nself,\ninstance_mask_key: str = \"segmentation-instance\",\nclass_mask_key: str = \"segmentation-class\",\nclasses_key: str = \"instance_category\",\nignore_mask_key: str = \"ignore_mask\",\ninstance_axis: int = -4,\n):\nself.instance_mask_key = instance_mask_key\nself.class_mask_key = class_mask_key\nself.classes_key = classes_key\nself.ignore_mask_key = ignore_mask_key\nself.instance_axis = instance_axis\ndef __call__(self, data: Dict[str, Any]):\ndata[self.ignore_mask_key] = (data[self.class_mask_key] == 255)[None]  # 1 x H x W x 1\nexpanded_segmentation_mask = data[self.instance_mask_key] * numpy.expand_dims(\ndata[self.class_mask_key], axis=self.instance_axis\n)\nassert expanded_segmentation_mask.max() != 255\ndata[self.instance_mask_key] = expanded_segmentation_mask\nclasses = []\nfor instance_slice in numpy.rollaxis(expanded_segmentation_mask, self.instance_axis):\nunique_values = numpy.unique(instance_slice)\nassert len(unique_values) == 2  # Should contain 0 and class id.\nclasses.append(unique_values[1])\ndata[self.classes_key] = numpy.array(classes)\nreturn data\n</code></pre>"},{"location":"api/ocl/preprocessing/#ocl.preprocessing.AddImageSize","title":"<code>AddImageSize</code>","text":"<p>Add height and width of image as data entry.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>Key of image.</p> <code>'image'</code> <code>target_key</code> <code>str</code> <p>Key under which to store size.</p> <code>'image_size'</code> Source code in <code>ocl/preprocessing.py</code> <pre><code>class AddImageSize:\n\"\"\"Add height and width of image as data entry.\n    Args:\n        key: Key of image.\n        target_key: Key under which to store size.\n    \"\"\"\ndef __init__(self, key: str = \"image\", target_key: str = \"image_size\"):\nself.key = key\nself.target_key = target_key\ndef __call__(self, data: Dict[str, Any]):\nheight, width, _ = data[self.key].shape\ndata[self.target_key] = numpy.array([height, width], dtype=numpy.int64)\nreturn data\n</code></pre>"},{"location":"api/ocl/preprocessing/#ocl.preprocessing.AddEmptyMasks","title":"<code>AddEmptyMasks</code>","text":"<p>Add empty masks to data if the data does not include them already.</p> Source code in <code>ocl/preprocessing.py</code> <pre><code>class AddEmptyMasks:\n\"\"\"Add empty masks to data if the data does not include them already.\"\"\"\ndef __init__(self, mask_keys: Union[str, Sequence[str]], take_size_from: str = \"image\"):\n\"\"\"Initialize AddEmptyMasks.\n        Args:\n            mask_keys: One or several keys of empty masks to be added.\n            take_size_from: Key of element whose height and width is used to create mask. Element is\n                assumed to have shape of (H, W, C).\n        \"\"\"\nif isinstance(mask_keys, str):\nself.mask_keys = (mask_keys,)\nelse:\nself.mask_keys = tuple(mask_keys)\nself.source_key = take_size_from\ndef __call__(self, data: Dict[str, Any]) -&gt; Dict[str, Any]:\nheight, width, _ = data[self.source_key].shape\nfor key in self.mask_keys:\nif key not in data:\ndata[key] = numpy.zeros((1, height, width, 1), dtype=numpy.uint8)\nreturn data\n</code></pre>"},{"location":"api/ocl/preprocessing/#ocl.preprocessing.AddEmptyMasks.__init__","title":"<code>__init__</code>","text":"<p>Initialize AddEmptyMasks.</p> <p>Parameters:</p> Name Type Description Default <code>mask_keys</code> <code>Union[str, Sequence[str]]</code> <p>One or several keys of empty masks to be added.</p> required <code>take_size_from</code> <code>str</code> <p>Key of element whose height and width is used to create mask. Element is assumed to have shape of (H, W, C).</p> <code>'image'</code> Source code in <code>ocl/preprocessing.py</code> <pre><code>def __init__(self, mask_keys: Union[str, Sequence[str]], take_size_from: str = \"image\"):\n\"\"\"Initialize AddEmptyMasks.\n    Args:\n        mask_keys: One or several keys of empty masks to be added.\n        take_size_from: Key of element whose height and width is used to create mask. Element is\n            assumed to have shape of (H, W, C).\n    \"\"\"\nif isinstance(mask_keys, str):\nself.mask_keys = (mask_keys,)\nelse:\nself.mask_keys = tuple(mask_keys)\nself.source_key = take_size_from\n</code></pre>"},{"location":"api/ocl/preprocessing/#ocl.preprocessing.AddEmptyBboxes","title":"<code>AddEmptyBboxes</code>","text":"<p>Add empty bounding boxes to data if the data does not include them already.</p> <p>Parameters:</p> Name Type Description Default <code>keys</code> <code>Union[str, Sequence[str]]</code> <p>One or several keys of empty boxes to be added.</p> <code>'instance_bbox'</code> <code>empty_value</code> <code>float</code> <p>Value of the empty box at all coordinates.</p> <code>-1.0</code> Source code in <code>ocl/preprocessing.py</code> <pre><code>class AddEmptyBboxes:\n\"\"\"Add empty bounding boxes to data if the data does not include them already.\n    Args:\n        keys: One or several keys of empty boxes to be added.\n        empty_value: Value of the empty box at all coordinates.\n    \"\"\"\ndef __init__(self, keys: Union[str, Sequence[str]] = \"instance_bbox\", empty_value: float = -1.0):\nif isinstance(keys, str):\nself.keys = (keys,)\nelse:\nself.keys = tuple(keys)\nself.empty_value = empty_value\ndef __call__(self, data: Dict[str, Any]):\nfor key in self.keys:\nif key not in data:\ndata[key] = numpy.ones((1, 4), dtype=numpy.float32) * self.empty_value\nreturn data\n</code></pre>"},{"location":"api/ocl/preprocessing/#ocl.preprocessing.CanonicalizeBboxes","title":"<code>CanonicalizeBboxes</code>","text":"<p>Convert bounding boxes to canonical (x1, y1, x2, y2) format.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>Key of bounding box, assumed to have shape K x 4.</p> <code>'instance_bbox'</code> <code>format</code> <code>str</code> <p>Format of bounding boxes. Either \"xywh\" or \"yxyx\".</p> <code>'xywh'</code> Source code in <code>ocl/preprocessing.py</code> <pre><code>class CanonicalizeBboxes:\n\"\"\"Convert bounding boxes to canonical (x1, y1, x2, y2) format.\n    Args:\n        key: Key of bounding box, assumed to have shape K x 4.\n        format: Format of bounding boxes. Either \"xywh\" or \"yxyx\".\n    \"\"\"\ndef __init__(self, key: str = \"instance_bbox\", format: str = \"xywh\"):\nself.key = key\nself.format_xywh = False\nself.format_yxyx = False\nif format == \"xywh\":\nself.format_xywh = True\nelif format == \"yxyx\":\nself.format_yxyx = True\nelse:\nraise ValueError(f\"Unknown input format `{format}`\")\ndef __call__(self, data: Dict[str, Any]):\nif self.key not in data:\nreturn data\nbboxes = data[self.key]\nif self.format_xywh:\nx1, y1, w, h = numpy.split(bboxes, 4, axis=1)\nx2 = x1 + w\ny2 = y1 + h\nelif self.format_yxyx:\ny1, x1, y2, x2 = numpy.split(bboxes, 4, axis=1)\ndata[self.key] = numpy.concatenate((x1, y1, x2, y2), axis=1)\nreturn data\n</code></pre>"},{"location":"api/ocl/preprocessing/#ocl.preprocessing.RescaleBboxes","title":"<code>RescaleBboxes</code>","text":"<p>Rescale bounding boxes by size taken from data.</p> <p>Bounding boxes are assumed to have format (x1, y1, x2, y2). The rescaled box is     (x1 * width, y1 * height, x2 * width, y2 * height).</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>Key of bounding box, assumed to have shape K x 4.</p> <code>'instance_bbox'</code> <code>take_size_from</code> <code>str</code> <p>Key of element to take the size for rescaling from, assumed to have shape H x W x C.</p> <code>'image'</code> Source code in <code>ocl/preprocessing.py</code> <pre><code>class RescaleBboxes:\n\"\"\"Rescale bounding boxes by size taken from data.\n    Bounding boxes are assumed to have format (x1, y1, x2, y2). The rescaled box is\n        (x1 * width, y1 * height, x2 * width, y2 * height).\n    Args:\n        key: Key of bounding box, assumed to have shape K x 4.\n        take_size_from: Key of element to take the size for rescaling from, assumed to have shape\n            H x W x C.\n    \"\"\"\ndef __init__(self, key: str = \"instance_bbox\", take_size_from: str = \"image\"):\nself.key = key\nself.take_size_from = take_size_from\ndef __call__(self, data: Dict[str, Any]):\nif self.key not in data:\nreturn data\nheight, width, _ = data[self.take_size_from].shape\nscaling = numpy.array([[width, height, width, height]], dtype=numpy.float32)\ndata[self.key] = data[self.key] * scaling\nreturn data\n</code></pre>"},{"location":"api/ocl/preprocessing/#ocl.preprocessing.AddSegmentationMaskFromInstanceMask","title":"<code>AddSegmentationMaskFromInstanceMask</code>","text":"<p>Convert instance to segmentation masks by joining instances with the same category.</p> <p>Overlaps of instances of different classes are resolved by taking the class with the higher class id.</p> Source code in <code>ocl/preprocessing.py</code> <pre><code>class AddSegmentationMaskFromInstanceMask:\n\"\"\"Convert instance to segmentation masks by joining instances with the same category.\n    Overlaps of instances of different classes are resolved by taking the class with the higher class\n    id.\n    \"\"\"\ndef __init__(\nself,\ninstance_mask_key: str = \"instance_mask\",\ntarget_key: str = \"segmentation_mask\",\n):\nself.instance_mask_key = instance_mask_key\nself.target_key = target_key\n@staticmethod\ndef convert(instance_mask: numpy.ndarray) -&gt; numpy.ndarray:\n\"\"\"Convert instance to segmentation mask.\n        Args:\n            instance_mask: Densely encoded instance masks of shape I x H x W x 1, where I is the\n                number of instances.\n        \"\"\"\n# Reduce instance mask to single dimension\ninstance_mask = instance_mask.max(axis=0, keepdims=True)\nreturn expand_dense_mask(instance_mask)\ndef __call__(self, data: Dict[str, Any]):\nif self.instance_mask_key not in data:\nreturn data\ndata[self.target_key] = self.convert(data[self.instance_mask_key])\nreturn data\n</code></pre>"},{"location":"api/ocl/preprocessing/#ocl.preprocessing.AddSegmentationMaskFromInstanceMask.convert","title":"<code>convert</code>  <code>staticmethod</code>","text":"<p>Convert instance to segmentation mask.</p> <p>Parameters:</p> Name Type Description Default <code>instance_mask</code> <code>numpy.ndarray</code> <p>Densely encoded instance masks of shape I x H x W x 1, where I is the number of instances.</p> required Source code in <code>ocl/preprocessing.py</code> <pre><code>@staticmethod\ndef convert(instance_mask: numpy.ndarray) -&gt; numpy.ndarray:\n\"\"\"Convert instance to segmentation mask.\n    Args:\n        instance_mask: Densely encoded instance masks of shape I x H x W x 1, where I is the\n            number of instances.\n    \"\"\"\n# Reduce instance mask to single dimension\ninstance_mask = instance_mask.max(axis=0, keepdims=True)\nreturn expand_dense_mask(instance_mask)\n</code></pre>"},{"location":"api/ocl/preprocessing/#ocl.preprocessing.AddBBoxFromInstanceMasks","title":"<code>AddBBoxFromInstanceMasks</code>","text":"<p>Convert instance mask to bounding box.</p> Source code in <code>ocl/preprocessing.py</code> <pre><code>class AddBBoxFromInstanceMasks:\n\"\"\"Convert instance mask to bounding box.\"\"\"\ndef __init__(\nself,\ninstance_mask_key: str = \"mask\",\nvideo_id_key: str = \"__key__\",  # not quite sure if this is the best key\ntarget_box_key: str = \"instance_bbox\",\ntarget_cls_key: str = \"instance_cls\",\ntarget_id_key: str = \"instance_id\",\n):\n\"\"\"Initialize AddBBoxFromInstanceMasks.\n        Args:\n            instance_mask_key: mask key name.\n            video_id_key: Field from which to derive the instance key.\n            target_box_key: Output field for bounding box derived from instance mask.\n            target_cls_key: Output field for class derived from instance mask.\n            target_id_key: Output field for instance id.\n        \"\"\"\nself.instance_mask_key = instance_mask_key\nself.video_id_key = video_id_key\nself.target_box_key = target_box_key\nself.target_cls_key = target_cls_key\nself.target_id_key = target_id_key\n@staticmethod\ndef convert(instance_mask: numpy.ndarray, video_id: numpy.ndarray) -&gt; numpy.ndarray:\nnum_frame, num_instance, height, width, _ = instance_mask.shape\n# Convert to binary mask\nbinary_mask = instance_mask &gt; 0\n# Filter background. TODO: now we assume the first mask for each video is background.\n# Might not apply to every dataset\nbinary_mask = binary_mask[:, 1:]\nnum_instance -= 1\nbinary_mask = (\ntorch.tensor(binary_mask).squeeze().view(num_frame * num_instance, height, width)\n)\n# Filter empty masks\nnon_empty_mask_idx = torch.where(binary_mask.sum(-1).sum(-1) &gt; 0)[0]\nempty_mask_idx = torch.where(binary_mask.sum(-1).sum(-1) == 0)[0]\nnon_empty_binary_mask = binary_mask[non_empty_mask_idx]\nnon_empty_bboxes = masks_to_boxes(non_empty_binary_mask)\n# Turn box into cxcyhw\nbboxes = torch.zeros(num_frame * num_instance, 4)\nnon_empty_bboxes = box_xyxy_to_cxcywh(non_empty_bboxes)\nbboxes[non_empty_mask_idx] = non_empty_bboxes\n# normalized to 0,1\n# Make sure width and height are correct\nbboxes[:, 0::2] = bboxes[:, 0::2] / width\nbboxes[:, 1::2] = bboxes[:, 1::2] / height\nbboxes = bboxes.view(num_frame, num_instance, 4).squeeze(-1).to(torch.float32)\n# class\n# -1 is background or no object, 0 is the first object class\ninstance_cls = torch.ones(num_frame * num_instance, 1) * -1\ninstance_cls[non_empty_mask_idx] = 0\ninstance_cls = instance_cls.view(num_frame, num_instance, 1).squeeze(-1).to(torch.long)\n# ID\ninstance_id = torch.range(0, num_instance - 1)[None, :, None].repeat(num_frame, 1, 1)\ninstance_id = instance_id.view(num_frame * num_instance, 1)\ninstance_id[empty_mask_idx] = -1\ninstance_id = instance_id.view(num_frame, num_instance, 1).squeeze(-1).to(torch.long)\nreturn bboxes, instance_cls, instance_id\ndef __call__(self, data: Dict[str, Any]):\nif self.instance_mask_key not in data:\nreturn data\nbboxes, instance_cls, instance_id = self.convert(\ndata[self.instance_mask_key], data[self.video_id_key]\n)\ndata[self.target_box_key] = bboxes\ndata[self.target_cls_key] = instance_cls\ndata[self.target_id_key] = instance_id\nreturn data\n</code></pre>"},{"location":"api/ocl/preprocessing/#ocl.preprocessing.AddBBoxFromInstanceMasks.__init__","title":"<code>__init__</code>","text":"<p>Initialize AddBBoxFromInstanceMasks.</p> <p>Parameters:</p> Name Type Description Default <code>instance_mask_key</code> <code>str</code> <p>mask key name.</p> <code>'mask'</code> <code>video_id_key</code> <code>str</code> <p>Field from which to derive the instance key.</p> <code>'__key__'</code> <code>target_box_key</code> <code>str</code> <p>Output field for bounding box derived from instance mask.</p> <code>'instance_bbox'</code> <code>target_cls_key</code> <code>str</code> <p>Output field for class derived from instance mask.</p> <code>'instance_cls'</code> <code>target_id_key</code> <code>str</code> <p>Output field for instance id.</p> <code>'instance_id'</code> Source code in <code>ocl/preprocessing.py</code> <pre><code>def __init__(\nself,\ninstance_mask_key: str = \"mask\",\nvideo_id_key: str = \"__key__\",  # not quite sure if this is the best key\ntarget_box_key: str = \"instance_bbox\",\ntarget_cls_key: str = \"instance_cls\",\ntarget_id_key: str = \"instance_id\",\n):\n\"\"\"Initialize AddBBoxFromInstanceMasks.\n    Args:\n        instance_mask_key: mask key name.\n        video_id_key: Field from which to derive the instance key.\n        target_box_key: Output field for bounding box derived from instance mask.\n        target_cls_key: Output field for class derived from instance mask.\n        target_id_key: Output field for instance id.\n    \"\"\"\nself.instance_mask_key = instance_mask_key\nself.video_id_key = video_id_key\nself.target_box_key = target_box_key\nself.target_cls_key = target_cls_key\nself.target_id_key = target_id_key\n</code></pre>"},{"location":"api/ocl/preprocessing/#ocl.preprocessing.InstanceMasksToDenseMasks","title":"<code>InstanceMasksToDenseMasks</code>","text":"<p>Convert binary instance masks to dense masks, i.e. where the mask value encodes the class id.</p> <p>Class ids are taken from a list containing a class id per instance.</p> Source code in <code>ocl/preprocessing.py</code> <pre><code>class InstanceMasksToDenseMasks:\n\"\"\"Convert binary instance masks to dense masks, i.e. where the mask value encodes the class id.\n    Class ids are taken from a list containing a class id per instance.\n    \"\"\"\ndef __init__(\nself,\ninstance_mask_key: str = \"instance_mask\",\ncategory_key: str = \"instance_category\",\n):\nself.instance_mask_key = instance_mask_key\nself.category_key = category_key\n@staticmethod\ndef convert(instance_mask: numpy.ndarray, categories: numpy.ndarray) -&gt; numpy.ndarray:\nif numpy.min(categories) &lt;= 0:\nraise ValueError(\"Detected category smaller equal than 0 in instance masks.\")\nif numpy.max(categories) &gt; 255:\nraise ValueError(\n\"Detected category greater than 255 in instance masks. This does not fit in uint8.\"\n)\ncategories = categories[:, None, None, None]\nreturn (instance_mask * categories).astype(numpy.uint8)\ndef __call__(self, data: Dict[str, Any]):\nif self.instance_mask_key not in data:\nreturn data\ndata[self.instance_mask_key] = self.convert(\ndata[self.instance_mask_key], data[self.category_key]\n)\nreturn data\n</code></pre>"},{"location":"api/ocl/preprocessing/#ocl.preprocessing.MergeCocoThingsAndStuff","title":"<code>MergeCocoThingsAndStuff</code>","text":"<p>Merge COCO things and stuff segmentation masks.</p> <p>Parameters:</p> Name Type Description Default <code>things_key</code> <code>str</code> <p>Key to things instance mask. Mask is assumed to be densely encoded, i.e. the mask value encodes the class id, of shape I x H x W x 1, where I is the number of things instances.</p> <code>'instance_mask'</code> <code>stuff_key</code> <code>str</code> <p>Key to stuff segmentation mask. Mask is assumed to be densely encoded, i.e. the mask value encodes the class id, of shape K x H x W x 1, where K is the number stuff classes.</p> <code>'stuff_mask'</code> <code>output_key</code> <code>str</code> <p>Key under which the merged mask is stored. Returns mask of shape L x H x W x 1, where K &lt;= L &lt;= K + I.</p> required <code>include_crowd</code> <code>bool</code> <p>Whether to include pixels marked as crowd with their class, or with class zero.</p> <code>False</code> Source code in <code>ocl/preprocessing.py</code> <pre><code>class MergeCocoThingsAndStuff:\n\"\"\"Merge COCO things and stuff segmentation masks.\n    Args:\n        things_key: Key to things instance mask. Mask is assumed to be densely encoded, i.e.\n            the mask value encodes the class id, of shape I x H x W x 1, where I is the number of\n            things instances.\n        stuff_key: Key to stuff segmentation mask. Mask is assumed to be densely encoded, i.e.\n            the mask value encodes the class id, of shape K x H x W x 1, where K is the number stuff\n            classes.\n        output_key: Key under which the merged mask is stored. Returns mask of shape L x H x W x 1,\n            where K &lt;= L &lt;= K + I.\n        include_crowd: Whether to include pixels marked as crowd with their class, or with class\n            zero.\n    \"\"\"\ndef __init__(\nself,\noutput_key: str,\nthings_key: str = \"instance_mask\",\nstuff_key: str = \"stuff_mask\",\ninclude_crowd: bool = False,\n):\nself.things_key = things_key\nself.stuff_key = stuff_key\nself.output_key = output_key\nself.include_crowd = include_crowd\ndef __call__(self, data: Dict[str, Any]):\nif self.things_key in data:\nthings_instance_mask = data[self.things_key]\nthings_mask = things_instance_mask.max(axis=0, keepdims=True)\nelse:\nthings_mask = None\nstuff_mask = data[self.stuff_key]\nmerged_mask = stuff_mask.max(axis=0, keepdims=True)\n# In stuff annotations, thing pixels are encoded as class 183.\nuse_thing_mask = merged_mask == 183\nif things_mask is not None:\nif self.include_crowd:\n# In the stuff annotations, things marked with the \"crowd\" label are NOT encoded as\n# class 183, but as class 0. We can take the value of the things mask for those\n# pixels.\nuse_thing_mask |= merged_mask == 0\nmerged_mask[use_thing_mask] = things_mask[use_thing_mask]\nelse:\n# No pixel should have value 183 if the things_mask does not exist, but convert it to\n# zero anyways just to be sure.\nmerged_mask[use_thing_mask] = 0\ndata[self.output_key] = expand_dense_mask(merged_mask)\nreturn data\n</code></pre>"},{"location":"api/ocl/preprocessing/#ocl.preprocessing.FlowToTensor","title":"<code>FlowToTensor</code>","text":"<p>Convert an optical flow numpy array to a tensor.</p> <p>Flow is assumed to be of shape (..., H, W, 2), returned tensor is of shape (..., 2, H, W) to match with VideoTensor format.</p> Source code in <code>ocl/preprocessing.py</code> <pre><code>class FlowToTensor:\n\"\"\"Convert an optical flow numpy array to a tensor.\n    Flow is assumed to be of shape (..., H, W, 2), returned tensor is of shape (..., 2, H, W) to\n    match with VideoTensor format.\n    \"\"\"\ndef __call__(self, flow: numpy.ndarray) -&gt; torch.Tensor:\nflow_torch = torch.from_numpy(flow.astype(float)).to(torch.float32)\nreturn torch.moveaxis(flow_torch, -1, -3)\ndef __repr__(self) -&gt; str:\nreturn f\"{self.__class__.__name__}()\"\n</code></pre>"},{"location":"api/ocl/preprocessing/#ocl.preprocessing.ConvertCocoStuff164kMasks","title":"<code>ConvertCocoStuff164kMasks</code>","text":"<p>Convert COCO-Stuff-164k PNG segmentation masks to our format.</p> <p>Parameters:</p> Name Type Description Default <code>output_key</code> <code>str</code> <p>Key under which the output mask is stored. Returns uint8 mask of shape K x H x W x 1, where K is the number of classes in the image. Mask is densely encoded, i.e. the mask values encode the class id.</p> required <code>stuffthings_key</code> <code>str</code> <p>Key to COCO-Stuff-164k PNG mask. Mask has shape H x W x 3.</p> <code>'stuffthings_mask'</code> <code>ignore_key</code> <code>str</code> <p>Key under which the ignore mask is stored. Returns bool mask of shape 1 x H x W x 1. Ignores pixels where PNG mask has value 255 (crowd).</p> <code>'ignore_mask'</code> <code>drop_stuff</code> <code>bool</code> <p>If true, remove all stuff classes (id &gt;= 92), keeping only thing classes.</p> <code>False</code> Source code in <code>ocl/preprocessing.py</code> <pre><code>class ConvertCocoStuff164kMasks:\n\"\"\"Convert COCO-Stuff-164k PNG segmentation masks to our format.\n    Args:\n        output_key: Key under which the output mask is stored. Returns uint8 mask of shape\n            K x H x W x 1, where K is the number of classes in the image. Mask is densely encoded,\n            i.e. the mask values encode the class id.\n        stuffthings_key: Key to COCO-Stuff-164k PNG mask. Mask has shape H x W x 3.\n        ignore_key: Key under which the ignore mask is stored. Returns bool mask of shape\n            1 x H x W x 1. Ignores pixels where PNG mask has value 255 (crowd).\n        drop_stuff: If true, remove all stuff classes (id &gt;= 92), keeping only thing classes.\n    \"\"\"\ndef __init__(\nself,\noutput_key: str,\nstuffthings_key: str = \"stuffthings_mask\",\nignore_key: str = \"ignore_mask\",\ndrop_stuff: bool = False,\n):\nself.stuffthings_key = stuffthings_key\nself.ignore_key = ignore_key\nself.output_key = output_key\nself.drop_stuff = drop_stuff\ndef __call__(self, data: Dict[str, Any]):\nmask = data[self.stuffthings_key]  # H x W x 3, mask is encoded as an image\nassert mask.shape[-1] == 3\nmask = mask[:, :, :1]  # Take first channel, all channels are the same\nignore_mask = mask == 255\n# In PNG annotations, classes occupy indices 0-181, shift by 1\nmask = mask + 1\nmask[ignore_mask] = 0\nif self.drop_stuff:\nmask[mask &gt;= 92] = 0\ndata[self.ignore_key] = ignore_mask[None]  # 1 x H x W x 1\ndata[self.output_key] = expand_dense_mask(mask[None])  # K x H x W x 1\nreturn data\n</code></pre>"},{"location":"api/ocl/preprocessing/#ocl.preprocessing.VideoToTensor","title":"<code>VideoToTensor</code>","text":"<p>Convert a video numpy array of shape (T, H, W, C) to a torch tensor of shape (T, C, H, W).</p> Source code in <code>ocl/preprocessing.py</code> <pre><code>class VideoToTensor:\n\"\"\"Convert a video numpy array of shape (T, H, W, C) to a torch tensor of shape (T, C, H, W).\"\"\"\ndef __call__(self, video):\n\"\"\"Convert a numpy array of a video into a torch tensor.\n        Assumes input is a numpy array of shape T x H x W x C (or T x H x W for monochrome videos)\n        and convert it into torch tensor of shape T x C x H x W in order to allow application of\n        Conv3D operations.\n        \"\"\"\nif isinstance(video, numpy.ndarray):\n# Monochrome video such as mask\nif video.ndim == 3:\nvideo = video[..., None]\nvideo = torch.from_numpy(video.transpose((0, 3, 1, 2))).contiguous()\n# backward compatibility\nif isinstance(video, torch.ByteTensor):\nreturn video.to(dtype=torch.get_default_dtype()).div(255)\nelse:\nreturn video\nelse:\n# Should be torch tensor.\nif video.ndim == 3:\nvideo = video[..., None]\nvideo = video.permute(0, 3, 1, 2).contiguous()\n# backward compatibility\nif isinstance(video, torch.ByteTensor):\nreturn video.to(dtype=torch.get_default_dtype()).div(255)\nelse:\nreturn video\n</code></pre>"},{"location":"api/ocl/preprocessing/#ocl.preprocessing.VideoToTensor.__call__","title":"<code>__call__</code>","text":"<p>Convert a numpy array of a video into a torch tensor.</p> <p>Assumes input is a numpy array of shape T x H x W x C (or T x H x W for monochrome videos) and convert it into torch tensor of shape T x C x H x W in order to allow application of Conv3D operations.</p> Source code in <code>ocl/preprocessing.py</code> <pre><code>def __call__(self, video):\n\"\"\"Convert a numpy array of a video into a torch tensor.\n    Assumes input is a numpy array of shape T x H x W x C (or T x H x W for monochrome videos)\n    and convert it into torch tensor of shape T x C x H x W in order to allow application of\n    Conv3D operations.\n    \"\"\"\nif isinstance(video, numpy.ndarray):\n# Monochrome video such as mask\nif video.ndim == 3:\nvideo = video[..., None]\nvideo = torch.from_numpy(video.transpose((0, 3, 1, 2))).contiguous()\n# backward compatibility\nif isinstance(video, torch.ByteTensor):\nreturn video.to(dtype=torch.get_default_dtype()).div(255)\nelse:\nreturn video\nelse:\n# Should be torch tensor.\nif video.ndim == 3:\nvideo = video[..., None]\nvideo = video.permute(0, 3, 1, 2).contiguous()\n# backward compatibility\nif isinstance(video, torch.ByteTensor):\nreturn video.to(dtype=torch.get_default_dtype()).div(255)\nelse:\nreturn video\n</code></pre>"},{"location":"api/ocl/preprocessing/#ocl.preprocessing.ToSingleFrameVideo","title":"<code>ToSingleFrameVideo</code>","text":"<p>Convert image in tensor format to video format by adding frame dimension with single element.</p> <p>Converts C x H x W tensors into tensors of shape 1 x C x H x W.</p> Source code in <code>ocl/preprocessing.py</code> <pre><code>class ToSingleFrameVideo:\n\"\"\"Convert image in tensor format to video format by adding frame dimension with single element.\n    Converts C x H x W tensors into tensors of shape 1 x C x H x W.\n    \"\"\"\ndef __call__(self, image):\nreturn image.unsqueeze(0)\n</code></pre>"},{"location":"api/ocl/preprocessing/#ocl.preprocessing.NormalizeVideo","title":"<code>NormalizeVideo</code>","text":"<p>Normalize a video tensor of shape (T, C, H, W).</p> Source code in <code>ocl/preprocessing.py</code> <pre><code>class NormalizeVideo:\n\"\"\"Normalize a video tensor of shape (T, C, H, W).\"\"\"\ndef __init__(self, mean, std):\nself.mean = torch.tensor(mean)[None, :, None, None]\nself.std = torch.tensor(std)[None, :, None, None]\ndef __call__(self, video):\nreturn (video - self.mean) / self.std\n</code></pre>"},{"location":"api/ocl/preprocessing/#ocl.preprocessing.Denormalize","title":"<code>Denormalize</code>","text":"<p>         Bases: <code>torch.nn.Module</code></p> <p>Denormalize a tensor of shape (..., C, H, W) with any number of leading dimensions.</p> Source code in <code>ocl/preprocessing.py</code> <pre><code>class Denormalize(torch.nn.Module):\n\"\"\"Denormalize a tensor of shape (..., C, H, W) with any number of leading dimensions.\"\"\"\ndef __init__(self, mean, std):\nsuper().__init__()\nself.register_buffer(\"mean\", torch.tensor(mean)[:, None, None])\nself.register_buffer(\"std\", torch.tensor(std)[:, None, None])\ndef __call__(self, tensor):\nreturn tensor * self.std + self.mean\n</code></pre>"},{"location":"api/ocl/preprocessing/#ocl.preprocessing.ResizeNearestExact","title":"<code>ResizeNearestExact</code>","text":"<p>Resize a tensor using mode nearest-exact.</p> <p>This mode is not available in torchvision.transforms.Resize as of v0.12. This class was adapted from torchvision.transforms.functional_tensor.resize.</p> Source code in <code>ocl/preprocessing.py</code> <pre><code>class ResizeNearestExact:\n\"\"\"Resize a tensor using mode nearest-exact.\n    This mode is not available in torchvision.transforms.Resize as of v0.12. This class was adapted\n    from torchvision.transforms.functional_tensor.resize.\n    \"\"\"\ndef __init__(self, size: Union[int, List[int]], max_size: Optional[int] = None):\nself.size = size\nself.max_size = max_size\n@staticmethod\ndef _cast_squeeze_in(\nimg: torch.Tensor, req_dtypes: List[torch.dtype]\n) -&gt; Tuple[torch.Tensor, bool, bool, torch.dtype]:\nneed_squeeze = False\n# make image NCHW\nif img.ndim &lt; 4:\nimg = img.unsqueeze(dim=0)\nneed_squeeze = True\nout_dtype = img.dtype\nneed_cast = False\nif out_dtype not in req_dtypes:\nneed_cast = True\nreq_dtype = req_dtypes[0]\nimg = img.to(req_dtype)\nreturn img, need_cast, need_squeeze, out_dtype\n@staticmethod\ndef _cast_squeeze_out(\nimg: torch.Tensor, need_cast: bool, need_squeeze: bool, out_dtype: torch.dtype\n) -&gt; torch.Tensor:\nif need_squeeze:\nimg = img.squeeze(dim=0)\nif need_cast:\nif out_dtype in (torch.uint8, torch.int8, torch.int16, torch.int32, torch.int64):\n# it is better to round before cast\nimg = torch.round(img)\nimg = img.to(out_dtype)\nreturn img\n@staticmethod\ndef resize(img: torch.Tensor, size: Union[int, List[int]], max_size: Optional[int] = None):\nh, w = img.shape[-2:]\nif isinstance(size, int) or len(size) == 1:  # specified size only for the smallest edge\nshort, long = (w, h) if w &lt;= h else (h, w)\nrequested_new_short = size if isinstance(size, int) else size[0]\nnew_short, new_long = requested_new_short, int(requested_new_short * long / short)\nif max_size is not None:\nif max_size &lt;= requested_new_short:\nraise ValueError(\nf\"max_size = {max_size} must be strictly greater than the requested \"\nf\"size for the smaller edge size = {size}\"\n)\nif new_long &gt; max_size:\nnew_short, new_long = int(max_size * new_short / new_long), max_size\nnew_w, new_h = (new_short, new_long) if w &lt;= h else (new_long, new_short)\nif (w, h) == (new_w, new_h):\nreturn img\nelse:  # specified both h and w\nnew_w, new_h = size[1], size[0]\nimg, need_cast, need_squeeze, out_dtype = ResizeNearestExact._cast_squeeze_in(\nimg, (torch.float32, torch.float64)\n)\nimg = torch.nn.functional.interpolate(img, size=[new_h, new_w], mode=\"nearest-exact\")\nimg = ResizeNearestExact._cast_squeeze_out(\nimg, need_cast=need_cast, need_squeeze=need_squeeze, out_dtype=out_dtype\n)\nreturn img\ndef __call__(self, img: torch.Tensor) -&gt; torch.Tensor:\nreturn ResizeNearestExact.resize(img, self.size, self.max_size)\n</code></pre>"},{"location":"api/ocl/preprocessing/#ocl.preprocessing.ConvertToCocoSuperclasses","title":"<code>ConvertToCocoSuperclasses</code>","text":"<p>Convert segmentation mask from COCO classes (183) to COCO superclasses (27).</p> Source code in <code>ocl/preprocessing.py</code> <pre><code>class ConvertToCocoSuperclasses:\n\"\"\"Convert segmentation mask from COCO classes (183) to COCO superclasses (27).\"\"\"\nID_TO_SUPERCLASS_AND_NAME = {\n0: (\"unlabeled\", \"unlabeled\"),\n1: (\"person\", \"person\"),\n2: (\"vehicle\", \"bicycle\"),\n3: (\"vehicle\", \"car\"),\n4: (\"vehicle\", \"motorcycle\"),\n5: (\"vehicle\", \"airplane\"),\n6: (\"vehicle\", \"bus\"),\n7: (\"vehicle\", \"train\"),\n8: (\"vehicle\", \"truck\"),\n9: (\"vehicle\", \"boat\"),\n10: (\"outdoor\", \"traffic light\"),\n11: (\"outdoor\", \"fire hydrant\"),\n13: (\"outdoor\", \"stop sign\"),\n14: (\"outdoor\", \"parking meter\"),\n15: (\"outdoor\", \"bench\"),\n16: (\"animal\", \"bird\"),\n17: (\"animal\", \"cat\"),\n18: (\"animal\", \"dog\"),\n19: (\"animal\", \"horse\"),\n20: (\"animal\", \"sheep\"),\n21: (\"animal\", \"cow\"),\n22: (\"animal\", \"elephant\"),\n23: (\"animal\", \"bear\"),\n24: (\"animal\", \"zebra\"),\n25: (\"animal\", \"giraffe\"),\n27: (\"accessory\", \"backpack\"),\n28: (\"accessory\", \"umbrella\"),\n31: (\"accessory\", \"handbag\"),\n32: (\"accessory\", \"tie\"),\n33: (\"accessory\", \"suitcase\"),\n34: (\"sports\", \"frisbee\"),\n35: (\"sports\", \"skis\"),\n36: (\"sports\", \"snowboard\"),\n37: (\"sports\", \"sports ball\"),\n38: (\"sports\", \"kite\"),\n39: (\"sports\", \"baseball bat\"),\n40: (\"sports\", \"baseball glove\"),\n41: (\"sports\", \"skateboard\"),\n42: (\"sports\", \"surfboard\"),\n43: (\"sports\", \"tennis racket\"),\n44: (\"kitchen\", \"bottle\"),\n46: (\"kitchen\", \"wine glass\"),\n47: (\"kitchen\", \"cup\"),\n48: (\"kitchen\", \"fork\"),\n49: (\"kitchen\", \"knife\"),\n50: (\"kitchen\", \"spoon\"),\n51: (\"kitchen\", \"bowl\"),\n52: (\"food\", \"banana\"),\n53: (\"food\", \"apple\"),\n54: (\"food\", \"sandwich\"),\n55: (\"food\", \"orange\"),\n56: (\"food\", \"broccoli\"),\n57: (\"food\", \"carrot\"),\n58: (\"food\", \"hot dog\"),\n59: (\"food\", \"pizza\"),\n60: (\"food\", \"donut\"),\n61: (\"food\", \"cake\"),\n62: (\"furniture\", \"chair\"),\n63: (\"furniture\", \"couch\"),\n64: (\"furniture\", \"potted plant\"),\n65: (\"furniture\", \"bed\"),\n67: (\"furniture\", \"dining table\"),\n70: (\"furniture\", \"toilet\"),\n72: (\"electronic\", \"tv\"),\n73: (\"electronic\", \"laptop\"),\n74: (\"electronic\", \"mouse\"),\n75: (\"electronic\", \"remote\"),\n76: (\"electronic\", \"keyboard\"),\n77: (\"electronic\", \"cell phone\"),\n78: (\"appliance\", \"microwave\"),\n79: (\"appliance\", \"oven\"),\n80: (\"appliance\", \"toaster\"),\n81: (\"appliance\", \"sink\"),\n82: (\"appliance\", \"refrigerator\"),\n84: (\"indoor\", \"book\"),\n85: (\"indoor\", \"clock\"),\n86: (\"indoor\", \"vase\"),\n87: (\"indoor\", \"scissors\"),\n88: (\"indoor\", \"teddy bear\"),\n89: (\"indoor\", \"hair drier\"),\n90: (\"indoor\", \"toothbrush\"),\n92: (\"textile\", \"banner\"),\n93: (\"textile\", \"blanket\"),\n94: (\"plant\", \"branch\"),\n95: (\"building\", \"bridge\"),\n96: (\"building\", \"building-other\"),\n97: (\"plant\", \"bush\"),\n98: (\"furniture-stuff\", \"cabinet\"),\n99: (\"structural\", \"cage\"),\n100: (\"raw-material\", \"cardboard\"),\n101: (\"floor\", \"carpet\"),\n102: (\"ceiling\", \"ceiling-other\"),\n103: (\"ceiling\", \"ceiling-tile\"),\n104: (\"textile\", \"cloth\"),\n105: (\"textile\", \"clothes\"),\n106: (\"sky\", \"clouds\"),\n107: (\"furniture-stuff\", \"counter\"),\n108: (\"furniture-stuff\", \"cupboard\"),\n109: (\"textile\", \"curtain\"),\n110: (\"furniture-stuff\", \"desk-stuff\"),\n111: (\"ground\", \"dirt\"),\n112: (\"furniture-stuff\", \"door-stuff\"),\n113: (\"structural\", \"fence\"),\n114: (\"floor\", \"floor-marble\"),\n115: (\"floor\", \"floor-other\"),\n116: (\"floor\", \"floor-stone\"),\n117: (\"floor\", \"floor-tile\"),\n118: (\"floor\", \"floor-wood\"),\n119: (\"plant\", \"flower\"),\n120: (\"water\", \"fog\"),\n121: (\"food-stuff\", \"food-other\"),\n122: (\"food-stuff\", \"fruit\"),\n123: (\"furniture-stuff\", \"furniture-other\"),\n124: (\"plant\", \"grass\"),\n125: (\"ground\", \"gravel\"),\n126: (\"ground\", \"ground-other\"),\n127: (\"solid\", \"hill\"),\n128: (\"building\", \"house\"),\n129: (\"plant\", \"leaves\"),\n130: (\"furniture-stuff\", \"light\"),\n131: (\"textile\", \"mat\"),\n132: (\"raw-material\", \"metal\"),\n133: (\"furniture-stuff\", \"mirror-stuff\"),\n134: (\"plant\", \"moss\"),\n135: (\"solid\", \"mountain\"),\n136: (\"ground\", \"mud\"),\n137: (\"textile\", \"napkin\"),\n138: (\"structural\", \"net\"),\n139: (\"raw-material\", \"paper\"),\n140: (\"ground\", \"pavement\"),\n141: (\"textile\", \"pillow\"),\n142: (\"plant\", \"plant-other\"),\n143: (\"raw-material\", \"plastic\"),\n144: (\"ground\", \"platform\"),\n145: (\"ground\", \"playingfield\"),\n146: (\"structural\", \"railing\"),\n147: (\"ground\", \"railroad\"),\n148: (\"water\", \"river\"),\n149: (\"ground\", \"road\"),\n150: (\"solid\", \"rock\"),\n151: (\"building\", \"roof\"),\n152: (\"textile\", \"rug\"),\n153: (\"food-stuff\", \"salad\"),\n154: (\"ground\", \"sand\"),\n155: (\"water\", \"sea\"),\n156: (\"furniture-stuff\", \"shelf\"),\n157: (\"sky\", \"sky-other\"),\n158: (\"building\", \"skyscraper\"),\n159: (\"ground\", \"snow\"),\n160: (\"solid\", \"solid-other\"),\n161: (\"furniture-stuff\", \"stairs\"),\n162: (\"solid\", \"stone\"),\n163: (\"plant\", \"straw\"),\n164: (\"structural\", \"structural-other\"),\n165: (\"furniture-stuff\", \"table\"),\n166: (\"building\", \"tent\"),\n167: (\"textile\", \"textile-other\"),\n168: (\"textile\", \"towel\"),\n169: (\"plant\", \"tree\"),\n170: (\"food-stuff\", \"vegetable\"),\n171: (\"wall\", \"wall-brick\"),\n172: (\"wall\", \"wall-concrete\"),\n173: (\"wall\", \"wall-other\"),\n174: (\"wall\", \"wall-panel\"),\n175: (\"wall\", \"wall-stone\"),\n176: (\"wall\", \"wall-tile\"),\n177: (\"wall\", \"wall-wood\"),\n178: (\"water\", \"water-other\"),\n179: (\"water\", \"waterdrops\"),\n180: (\"window\", \"window-blind\"),\n181: (\"window\", \"window-other\"),\n182: (\"solid\", \"wood\"),\n183: (\"other\", \"other\"),\n}\nSUPERCLASS_TO_ID = {\n\"unlabeled\": 0,\n\"person\": 1,\n\"vehicle\": 2,\n\"outdoor\": 3,\n\"animal\": 4,\n\"accessory\": 5,\n\"sports\": 6,\n\"kitchen\": 7,\n\"food\": 8,\n\"furniture\": 9,\n\"electronic\": 10,\n\"appliance\": 11,\n\"indoor\": 12,\n\"textile\": 13,\n\"plant\": 14,\n\"building\": 15,\n\"furniture-stuff\": 16,\n\"structural\": 17,\n\"raw-material\": 18,\n\"floor\": 19,\n\"ceiling\": 20,\n\"sky\": 21,\n\"ground\": 22,\n\"water\": 23,\n\"food-stuff\": 24,\n\"wall\": 25,\n\"window\": 26,\n\"solid\": 27,\n\"other\": 28,\n}\ndef __init__(self):\nmax_class = max(ConvertToCocoSuperclasses.ID_TO_SUPERCLASS_AND_NAME.keys())\nclass_to_superclass = numpy.zeros((max_class + 1,), dtype=numpy.uint8)\nfor class_id, (supclass, _) in ConvertToCocoSuperclasses.ID_TO_SUPERCLASS_AND_NAME.items():\nclass_to_superclass[class_id] = ConvertToCocoSuperclasses.SUPERCLASS_TO_ID[supclass]\nself.class_to_superclass = class_to_superclass\ndef __call__(self, mask: numpy.ndarray) -&gt; numpy.ndarray:\n\"\"\"Convert mask to superclasses.\n        Args:\n            mask: Densely encoded segmentation mask of shape K x H x W x 1.\n        Returns:\n            Segmentation mask of shape C x H x W x 1, where C is the new set of classes.\n        \"\"\"\nclasses = mask.reshape(len(mask), -1).max(axis=-1)\nsuperclasses = self.class_to_superclass[classes]\nmask = (mask &gt; 0) * superclasses[:, None, None, None]\nreturn expand_dense_mask(mask.max(axis=0, keepdims=True))\n</code></pre>"},{"location":"api/ocl/preprocessing/#ocl.preprocessing.ConvertToCocoSuperclasses.__call__","title":"<code>__call__</code>","text":"<p>Convert mask to superclasses.</p> <p>Parameters:</p> Name Type Description Default <code>mask</code> <code>numpy.ndarray</code> <p>Densely encoded segmentation mask of shape K x H x W x 1.</p> required <p>Returns:</p> Type Description <code>numpy.ndarray</code> <p>Segmentation mask of shape C x H x W x 1, where C is the new set of classes.</p> Source code in <code>ocl/preprocessing.py</code> <pre><code>def __call__(self, mask: numpy.ndarray) -&gt; numpy.ndarray:\n\"\"\"Convert mask to superclasses.\n    Args:\n        mask: Densely encoded segmentation mask of shape K x H x W x 1.\n    Returns:\n        Segmentation mask of shape C x H x W x 1, where C is the new set of classes.\n    \"\"\"\nclasses = mask.reshape(len(mask), -1).max(axis=-1)\nsuperclasses = self.class_to_superclass[classes]\nmask = (mask &gt; 0) * superclasses[:, None, None, None]\nreturn expand_dense_mask(mask.max(axis=0, keepdims=True))\n</code></pre>"},{"location":"api/ocl/preprocessing/#ocl.preprocessing.OrigCenterCrop","title":"<code>OrigCenterCrop</code>","text":"<p>Returns center crop at original image resolution.</p> Source code in <code>ocl/preprocessing.py</code> <pre><code>class OrigCenterCrop:\n\"\"\"Returns center crop at original image resolution.\"\"\"\ndef __call__(self, image):\nheight, width = image.shape[-2:]\nreturn transforms.functional.center_crop(image, min(height, width))\n</code></pre>"},{"location":"api/ocl/preprocessing/#ocl.preprocessing.JointRandomResizedCropwithParameters","title":"<code>JointRandomResizedCropwithParameters</code>","text":"<p>         Bases: <code>transforms.RandomResizedCrop</code></p> Source code in <code>ocl/preprocessing.py</code> <pre><code>class JointRandomResizedCropwithParameters(transforms.RandomResizedCrop):\ndef __init__(\nself,\nsize,\nscale=(0.08, 1.0),\nratio=(3.0 / 4.0, 4.0 / 3.0),\ninterpolation=transforms.functional.InterpolationMode.BILINEAR,\n):\nsuper().__init__(size, scale, ratio, interpolation)\nself.mask_to_tensor = DenseMaskToTensor()\nself.mask_resize = ResizeNearestExact((size, size))\ndef forward(self, img: torch.Tensor, masks: Optional[Dict] = None) -&gt; torch.Tensor:\n\"\"\"Returns parameters of the resize in addition to the crop.\n        Args:\n            img (PIL Image or Tensor): Image to be cropped and resized.\n        Returns:\n            PIL Image or Tensor: Randomly cropped and resized image.\n        \"\"\"\nparams = self.get_params(img, self.scale, self.ratio)\nimg = transforms.functional.resized_crop(img, *params, self.size, self.interpolation)\nfor mask_key, mask in masks.items():\nif not isinstance(mask, torch.Tensor):\nmask = self.mask_to_tensor(mask)\nmask = transforms.functional.crop(mask, *params)\nmask = self.mask_resize(mask)\nmasks[mask_key] = mask\nreturn img, masks, params\n</code></pre>"},{"location":"api/ocl/preprocessing/#ocl.preprocessing.JointRandomResizedCropwithParameters.forward","title":"<code>forward</code>","text":"<p>Returns parameters of the resize in addition to the crop.</p> <p>Parameters:</p> Name Type Description Default <code>img</code> <code>PIL Image or Tensor</code> <p>Image to be cropped and resized.</p> required <p>Returns:</p> Type Description <code>torch.Tensor</code> <p>PIL Image or Tensor: Randomly cropped and resized image.</p> Source code in <code>ocl/preprocessing.py</code> <pre><code>def forward(self, img: torch.Tensor, masks: Optional[Dict] = None) -&gt; torch.Tensor:\n\"\"\"Returns parameters of the resize in addition to the crop.\n    Args:\n        img (PIL Image or Tensor): Image to be cropped and resized.\n    Returns:\n        PIL Image or Tensor: Randomly cropped and resized image.\n    \"\"\"\nparams = self.get_params(img, self.scale, self.ratio)\nimg = transforms.functional.resized_crop(img, *params, self.size, self.interpolation)\nfor mask_key, mask in masks.items():\nif not isinstance(mask, torch.Tensor):\nmask = self.mask_to_tensor(mask)\nmask = transforms.functional.crop(mask, *params)\nmask = self.mask_resize(mask)\nmasks[mask_key] = mask\nreturn img, masks, params\n</code></pre>"},{"location":"api/ocl/preprocessing/#ocl.preprocessing.RandomSample","title":"<code>RandomSample</code>","text":"<p>Draw a random sample from the first axis of a list or array.</p> Source code in <code>ocl/preprocessing.py</code> <pre><code>class RandomSample:\n\"\"\"Draw a random sample from the first axis of a list or array.\"\"\"\ndef __call__(self, tokens):\nreturn random.choice(tokens)\n</code></pre>"},{"location":"api/ocl/preprocessing/#ocl.preprocessing.SampleFramesUsingIndices","title":"<code>SampleFramesUsingIndices</code>","text":"<p>Sample frames form a tensor dependent on indices provided in the instance.</p> Source code in <code>ocl/preprocessing.py</code> <pre><code>class SampleFramesUsingIndices:\n\"\"\"Sample frames form a tensor dependent on indices provided in the instance.\"\"\"\ndef __init__(self, frame_fields: List[str], index_field: str):\nself.frame_fields = frame_fields\nself.index_field = index_field\ndef __call__(self, inputs: dict):\nindices = inputs[self.index_field]\nfor frame_field in self.frame_fields:\ninputs[frame_field] = inputs[frame_field][indices]\nreturn inputs\n</code></pre>"},{"location":"api/ocl/preprocessing/#ocl.preprocessing.MaskInstances","title":"<code>MaskInstances</code>","text":"<p>Filter instances by masking non matching with NaN.</p> Source code in <code>ocl/preprocessing.py</code> <pre><code>class MaskInstances:\n\"\"\"Filter instances by masking non matching with NaN.\"\"\"\ndef __init__(\nself,\nfields: List[str],\nkeys_to_keep: List[str],\nmask_video: bool = False,\n):\nself.fields = fields\nself.keys_to_keep = set(keys_to_keep)\nself.mask_video = mask_video\nif self.mask_video:\nself.video_key_to_frame_mapping = defaultdict(set)\nfor key in self.keys_to_keep:\nvideo_key, frame = key.split(\"_\")\nself.video_key_to_frame_mapping[video_key].add(int(frame))\ndef mask_instance(self, instance):\nkey = instance[\"__key__\"]\nif key not in self.keys_to_keep:\nfor field in self.fields:\ndata = instance[field]\nif isinstance(data, numpy.ndarray):\ninstance[field] = numpy.full_like(data, numpy.NaN)\nelif isinstance(data, torch.Tensor):\ninstance[field] = torch.full_like(data, numpy.NaN)\nelse:\nraise RuntimeError(f\"Field {field} is of unexpected type {type(data)}.\")\nreturn instance\ndef mask_instance_video(self, instance):\nkey = instance[\"__key__\"]\noutput = instance.copy()\nfor field in self.fields:\ndata = instance[field]\nif isinstance(data, numpy.ndarray):\noutput[field] = numpy.full_like(data, numpy.NaN)\nelif isinstance(data, torch.Tensor):\noutput[field] = torch.full_like(data, numpy.NaN)\nelse:\nraise RuntimeError(f\"Field {field} is of unexpected type {type(data)}.\")\n# We need to do some special handling here due to the strided decoding.\n# This is not really nice, but fixing it nicely would require significantly\n# more work for which we do not have the time at the moment.\nif \"decoded_indices\" in instance.keys():\n# Input comes from strided decoding, we thus need to adapt\n# key and frames.\nkey, _ = key.split(\"_\")  # Get video key.\nkey = str(int(key))\nif key in self.video_key_to_frame_mapping.keys():\nframes_to_keep = self.video_key_to_frame_mapping[key]\ndecoded_indices = instance[\"decoded_indices\"]\nframes_to_keep = [index for index in decoded_indices if index in frames_to_keep]\nfor field in self.fields:\ndata = instance[field]\noutput[field][frames_to_keep] = data[frames_to_keep]\nelse:\nif key in self.video_key_to_frame_mapping.keys():\nframes_to_keep = self.video_key_to_frame_mapping[key]\nfor field in self.fields:\ndata = instance[field]\noutput[field][frames_to_keep] = data[frames_to_keep]\nreturn output\ndef __call__(self, input_dict: Dict[str, Any]) -&gt; Dict[str, Any]:\nif self.mask_video:\nreturn self.mask_instance_video(input_dict)\nreturn self.mask_instance(input_dict)\n</code></pre>"},{"location":"api/ocl/preprocessing/#ocl.preprocessing.expand_dense_mask","title":"<code>expand_dense_mask</code>","text":"<p>Convert dense segmentation mask to one where each class occupies one dimension.</p> <p>Parameters:</p> Name Type Description Default <code>mask</code> <code>numpy.ndarray</code> <p>Densely encoded segmentation mask of shape 1 x H x W x 1.</p> required <p>Densely encoded segmentation mask of shape K x H x W x 1, where K is the</p> Type Description <code>numpy.ndarray</code> <p>number of classes in the mask. Zero is taken to indicate an unoccupied pixel.</p> Source code in <code>ocl/preprocessing.py</code> <pre><code>def expand_dense_mask(mask: numpy.ndarray) -&gt; numpy.ndarray:\n\"\"\"Convert dense segmentation mask to one where each class occupies one dimension.\n    Args:\n        mask: Densely encoded segmentation mask of shape 1 x H x W x 1.\n    Returns: Densely encoded segmentation mask of shape K x H x W x 1, where K is the\n        number of classes in the mask. Zero is taken to indicate an unoccupied pixel.\n    \"\"\"\nclasses = numpy.unique(mask)[:, None, None, None]\nmask = (classes == mask) * classes\n# Strip empty class, but only if there is something else in the mask\nif classes[0].squeeze() == 0 and len(classes) != 1:\nmask = mask[1:]\nreturn mask\n</code></pre>"},{"location":"api/ocl/scheduling/","title":"ocl.scheduling","text":"<p>Scheduling of learning rate and hyperparameters.</p>"},{"location":"api/ocl/scheduling/#ocl.scheduling.HPScheduler","title":"<code>HPScheduler</code>","text":"<p>         Bases: <code>torch.nn.Module</code></p> <p>Base class for scheduling of scalar hyperparameters based on the number of training steps.</p> <p>A separate callback ocl.callbacks.UpdateHyperparameterScheduling calls <code>update_global_step</code> to update the state of the hyperparameter according to the scheduling.</p> <p>This class can be used in computations similar to a regular float if operations are applied from the left otherwise it needs to be converted using <code>float(instance)</code> which will return the currently scheduled value of the hyperparameter.</p> Source code in <code>ocl/scheduling.py</code> <pre><code>class HPScheduler(torch.nn.Module, metaclass=abc.ABCMeta):\n\"\"\"Base class for scheduling of scalar hyperparameters based on the number of training steps.\n    A separate callback [ocl.callbacks.UpdateHyperparameterScheduling][] calls\n    `update_global_step` to update the state of the hyperparameter according\n    to the scheduling.\n    This class can be used in computations similar to a regular float if operations\n    are applied from the left otherwise it needs to be converted using\n    `float(instance)` which will return the currently scheduled value of the\n    hyperparameter.\n    \"\"\"\ndef __init__(self):\nsuper().__init__()\nself.last_global_step: Optional[int] = None\ndef update_global_step(self, global_step: int):\n\"\"\"Update global step used in `compute_scheduled_value`.\n        This should be called by the\n        [ocl.callbacks.UpdateHyperparameterScheduling][] callback.\n        Args:\n            global_step: The current global step.\n        \"\"\"\nself.last_global_step = global_step\n@abc.abstractmethod\ndef compute_scheduled_value(self) -&gt; float:\n\"\"\"Return current value of hyperparameter based on global step.\n        Returns:\n            The scheduled hyperparameter value.\n        \"\"\"\npass\ndef __float__(self):\nif self.last_global_step is None:\nraise RuntimeError(\n\"HPScheduler was not provided with last_global_step. \"\n\"Make sure UpdateHyperparameterScheduling callback is called.\"\n)\nreturn self.compute_scheduled_value()\ndef __add__(self, other):\nreturn float(self) + other\ndef __sub__(self, other):\nreturn float(self) - other\ndef __mul__(self, other):\nreturn float(self) * other\ndef __div__(self, other):\nreturn float(self) / other\n</code></pre>"},{"location":"api/ocl/scheduling/#ocl.scheduling.HPScheduler.update_global_step","title":"<code>update_global_step</code>","text":"<p>Update global step used in <code>compute_scheduled_value</code>.</p> <p>This should be called by the ocl.callbacks.UpdateHyperparameterScheduling callback.</p> <p>Parameters:</p> Name Type Description Default <code>global_step</code> <code>int</code> <p>The current global step.</p> required Source code in <code>ocl/scheduling.py</code> <pre><code>def update_global_step(self, global_step: int):\n\"\"\"Update global step used in `compute_scheduled_value`.\n    This should be called by the\n    [ocl.callbacks.UpdateHyperparameterScheduling][] callback.\n    Args:\n        global_step: The current global step.\n    \"\"\"\nself.last_global_step = global_step\n</code></pre>"},{"location":"api/ocl/scheduling/#ocl.scheduling.HPScheduler.compute_scheduled_value","title":"<code>compute_scheduled_value</code>  <code>abstractmethod</code>","text":"<p>Return current value of hyperparameter based on global step.</p> <p>Returns:</p> Type Description <code>float</code> <p>The scheduled hyperparameter value.</p> Source code in <code>ocl/scheduling.py</code> <pre><code>@abc.abstractmethod\ndef compute_scheduled_value(self) -&gt; float:\n\"\"\"Return current value of hyperparameter based on global step.\n    Returns:\n        The scheduled hyperparameter value.\n    \"\"\"\npass\n</code></pre>"},{"location":"api/ocl/scheduling/#ocl.scheduling.LinearHPScheduler","title":"<code>LinearHPScheduler</code>","text":"<p>         Bases: <code>HPScheduler</code></p> <p>Linearly increase value of a hyperparameter.</p> Source code in <code>ocl/scheduling.py</code> <pre><code>class LinearHPScheduler(HPScheduler):\n\"\"\"Linearly increase value of a hyperparameter.\"\"\"\ndef __init__(\nself, end_value: float, end_step: int, start_value: float = 0.0, start_step: int = 0\n):\n\"\"\"Initialize LinearHPScheduler.\n        Args:\n            end_value: Value after scheduling.\n            end_step: `global_step` at which `end_value` should be reached.\n            start_value: Value to be used prior to `start_step`\n            start_step: Value at which linear scheduling schould start.\n        \"\"\"\nsuper().__init__()\nif start_step &gt; end_step:\nraise ValueError(\"`start_step` needs to be smaller equal to `end_step`.\")\nself.start_value = start_value\nself.end_value = end_value\nself.start_step = start_step\nself.end_step = end_step\ndef compute_scheduled_value(self) -&gt; float:\nstep: int = self.last_global_step\nif step &lt; self.start_step:\nreturn self.start_value\nelif step &gt; self.end_step:\nreturn self.end_value\nelse:\nt = step - self.start_step\nT = self.end_step - self.start_step\nreturn self.start_value + t * (self.end_value - self.start_value) / T\n</code></pre>"},{"location":"api/ocl/scheduling/#ocl.scheduling.LinearHPScheduler.__init__","title":"<code>__init__</code>","text":"<p>Initialize LinearHPScheduler.</p> <p>Parameters:</p> Name Type Description Default <code>end_value</code> <code>float</code> <p>Value after scheduling.</p> required <code>end_step</code> <code>int</code> <p><code>global_step</code> at which <code>end_value</code> should be reached.</p> required <code>start_value</code> <code>float</code> <p>Value to be used prior to <code>start_step</code></p> <code>0.0</code> <code>start_step</code> <code>int</code> <p>Value at which linear scheduling schould start.</p> <code>0</code> Source code in <code>ocl/scheduling.py</code> <pre><code>def __init__(\nself, end_value: float, end_step: int, start_value: float = 0.0, start_step: int = 0\n):\n\"\"\"Initialize LinearHPScheduler.\n    Args:\n        end_value: Value after scheduling.\n        end_step: `global_step` at which `end_value` should be reached.\n        start_value: Value to be used prior to `start_step`\n        start_step: Value at which linear scheduling schould start.\n    \"\"\"\nsuper().__init__()\nif start_step &gt; end_step:\nraise ValueError(\"`start_step` needs to be smaller equal to `end_step`.\")\nself.start_value = start_value\nself.end_value = end_value\nself.start_step = start_step\nself.end_step = end_step\n</code></pre>"},{"location":"api/ocl/scheduling/#ocl.scheduling.StepHPScheduler","title":"<code>StepHPScheduler</code>","text":"<p>         Bases: <code>HPScheduler</code></p> <p>Schedule hyperparameter using discrete step.</p> Source code in <code>ocl/scheduling.py</code> <pre><code>class StepHPScheduler(HPScheduler):\n\"\"\"Schedule hyperparameter using discrete step.\"\"\"\ndef __init__(self, end_value: float, switch_step: int, start_value: float = 0.0):\n\"\"\"Initialize StepHPScheduler.\n        Args:\n            end_value: Value after `switch_step`.\n            switch_step: `global_step` at which to switch from `start_value` to `end_value`\n            start_value: Value to be used prior to `switch_step`\n        \"\"\"\nsuper().__init__()\nself.start_value = start_value\nself.end_value = end_value\nself.switch_step = switch_step\ndef compute_scheduled_value(self) -&gt; float:\nif self.last_global_step &lt; self.switch_step:\nreturn self.start_value\nelse:\nreturn self.end_value\n</code></pre>"},{"location":"api/ocl/scheduling/#ocl.scheduling.StepHPScheduler.__init__","title":"<code>__init__</code>","text":"<p>Initialize StepHPScheduler.</p> <p>Parameters:</p> Name Type Description Default <code>end_value</code> <code>float</code> <p>Value after <code>switch_step</code>.</p> required <code>switch_step</code> <code>int</code> <p><code>global_step</code> at which to switch from <code>start_value</code> to <code>end_value</code></p> required <code>start_value</code> <code>float</code> <p>Value to be used prior to <code>switch_step</code></p> <code>0.0</code> Source code in <code>ocl/scheduling.py</code> <pre><code>def __init__(self, end_value: float, switch_step: int, start_value: float = 0.0):\n\"\"\"Initialize StepHPScheduler.\n    Args:\n        end_value: Value after `switch_step`.\n        switch_step: `global_step` at which to switch from `start_value` to `end_value`\n        start_value: Value to be used prior to `switch_step`\n    \"\"\"\nsuper().__init__()\nself.start_value = start_value\nself.end_value = end_value\nself.switch_step = switch_step\n</code></pre>"},{"location":"api/ocl/scheduling/#ocl.scheduling.CosineAnnealingHPScheduler","title":"<code>CosineAnnealingHPScheduler</code>","text":"<p>         Bases: <code>HPScheduler</code></p> <p>Cosine annealing of hyperparameter.</p> Source code in <code>ocl/scheduling.py</code> <pre><code>class CosineAnnealingHPScheduler(HPScheduler):\n\"\"\"Cosine annealing of hyperparameter.\"\"\"\ndef __init__(self, start_value: float, end_value: float, start_step: int, end_step: int):\n\"\"\"Initialize CosineAnnealingHPScheduler.\n        Args:\n            end_value: Value after scheduling.\n            end_step: `global_step` at which `end_value` should be reached.\n            start_value: Value to be used prior to `start_step`\n            start_step: Value at which cosine scheduling schould start.\n        \"\"\"\nsuper().__init__()\nassert start_value &gt;= end_value\nassert start_step &lt;= end_step\nself.start_value = start_value\nself.end_value = end_value\nself.start_step = start_step\nself.end_step = end_step\ndef compute_scheduled_value(self) -&gt; float:\nstep: int = self.last_global_step\nif step &lt; self.start_step:\nvalue = self.start_value\nelif step &gt;= self.end_step:\nvalue = self.end_value\nelse:\na = 0.5 * (self.start_value - self.end_value)\nb = 0.5 * (self.start_value + self.end_value)\nprogress = (step - self.start_step) / (self.end_step - self.start_step)\nvalue = a * math.cos(math.pi * progress) + b\nreturn value\n</code></pre>"},{"location":"api/ocl/scheduling/#ocl.scheduling.CosineAnnealingHPScheduler.__init__","title":"<code>__init__</code>","text":"<p>Initialize CosineAnnealingHPScheduler.</p> <p>Parameters:</p> Name Type Description Default <code>end_value</code> <code>float</code> <p>Value after scheduling.</p> required <code>end_step</code> <code>int</code> <p><code>global_step</code> at which <code>end_value</code> should be reached.</p> required <code>start_value</code> <code>float</code> <p>Value to be used prior to <code>start_step</code></p> required <code>start_step</code> <code>int</code> <p>Value at which cosine scheduling schould start.</p> required Source code in <code>ocl/scheduling.py</code> <pre><code>def __init__(self, start_value: float, end_value: float, start_step: int, end_step: int):\n\"\"\"Initialize CosineAnnealingHPScheduler.\n    Args:\n        end_value: Value after scheduling.\n        end_step: `global_step` at which `end_value` should be reached.\n        start_value: Value to be used prior to `start_step`\n        start_step: Value at which cosine scheduling schould start.\n    \"\"\"\nsuper().__init__()\nassert start_value &gt;= end_value\nassert start_step &lt;= end_step\nself.start_value = start_value\nself.end_value = end_value\nself.start_step = start_step\nself.end_step = end_step\n</code></pre>"},{"location":"api/ocl/scheduling/#ocl.scheduling.exponential_decay_with_optional_warmup","title":"<code>exponential_decay_with_optional_warmup</code>","text":"<p>Return pytorch lighting optimizer configuration for exponential decay with optional warmup.</p> <p>Exponential decay is applied at each optimization step.  Exponential decay starts while warmup is still taking place.  This is in line with the typical scheduling used to train Transformer models.</p> <p>Parameters:</p> Name Type Description Default <code>optimizer</code> <code>Optimizer</code> <p>Pytorch lighting optimizer of which the learning rate should be scheduled.</p> required <code>decay_rate</code> <code>float</code> <p>Decay rate of exponential decay.</p> <code>1.0</code> <code>decay_steps</code> <code>int</code> <p>Number of optimization steps after which learning rate should be decayed by decay factor.</p> <code>10000</code> <code>warmup_steps</code> <code>int</code> <p>Number of warmup steps.</p> <code>0</code> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dict with structure compatible with ptl.  See pytorch lightning documentation</p> Source code in <code>ocl/scheduling.py</code> <pre><code>def exponential_decay_with_optional_warmup(\noptimizer: Optimizer, decay_rate: float = 1.0, decay_steps: int = 10000, warmup_steps: int = 0\n) -&gt; Dict[str, Any]:\n\"\"\"Return pytorch lighting optimizer configuration for exponential decay with optional warmup.\n    Exponential decay is applied at each optimization step.  Exponential decay starts\n    **while** warmup is still taking place.  This is in line with the typical scheduling\n    used to train Transformer models.\n    Args:\n        optimizer: Pytorch lighting optimizer of which the learning rate should be scheduled.\n        decay_rate: Decay rate of exponential decay.\n        decay_steps: Number of optimization steps after which learning rate should be decayed\n            by decay factor.\n        warmup_steps: Number of warmup steps.\n    Returns:\n        Dict with structure compatible with ptl.  See\n            [pytorch lightning documentation](\n                https://lightning.ai/docs/pytorch/stable/common/lightning_module.html#configure-optimizers)\n    \"\"\"\ndecay_fn = functools.partial(\n_exp_decay_with_warmup_fn,\ndecay_rate=decay_rate,\ndecay_steps=decay_steps,\nwarmup_steps=warmup_steps,\n)\nreturn {\"lr_scheduler\": {\"scheduler\": LambdaLR(optimizer, decay_fn), \"interval\": \"step\"}}\n</code></pre>"},{"location":"api/ocl/scheduling/#ocl.scheduling.exponential_decay_after_optional_warmup","title":"<code>exponential_decay_after_optional_warmup</code>","text":"<p>Return pytorch lighting optimizer configuration for exponential decay with optional warmup.</p> <p>Exponential decay is applied at each optimization step.  Exponential decay starts after warmup is took place.</p> <p>Parameters:</p> Name Type Description Default <code>optimizer</code> <code>Optimizer</code> <p>Pytorch lighting optimizer of which the learning rate should be scheduled.</p> required <code>decay_rate</code> <code>float</code> <p>Decay rate of exponential decay.</p> <code>1.0</code> <code>decay_steps</code> <code>int</code> <p>Number of optimization steps after which learning rate should be decayed by decay factor.</p> <code>10000</code> <code>warmup_steps</code> <code>int</code> <p>Number of warmup steps.</p> <code>0</code> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dict with structure compatible with ptl.  See pytorch lightning documentation</p> Source code in <code>ocl/scheduling.py</code> <pre><code>def exponential_decay_after_optional_warmup(\noptimizer: Optimizer, decay_rate: float = 1.0, decay_steps: int = 10000, warmup_steps: int = 0\n) -&gt; Dict[str, Any]:\n\"\"\"Return pytorch lighting optimizer configuration for exponential decay with optional warmup.\n    Exponential decay is applied at each optimization step.  Exponential decay starts\n    **after** warmup is took place.\n    Args:\n        optimizer: Pytorch lighting optimizer of which the learning rate should be scheduled.\n        decay_rate: Decay rate of exponential decay.\n        decay_steps: Number of optimization steps after which learning rate should be decayed\n            by decay factor.\n        warmup_steps: Number of warmup steps.\n    Returns:\n        Dict with structure compatible with ptl.  See\n            [pytorch lightning documentation](\n                https://lightning.ai/docs/pytorch/stable/common/lightning_module.html#configure-optimizers)\n    \"\"\"\ndecay_fn = functools.partial(\n_exp_decay_after_warmup_fn,\ndecay_rate=decay_rate,\ndecay_steps=decay_steps,\nwarmup_steps=warmup_steps,\n)\nreturn {\"lr_scheduler\": {\"scheduler\": LambdaLR(optimizer, decay_fn), \"interval\": \"step\"}}\n</code></pre>"},{"location":"api/ocl/scheduling/#ocl.scheduling.plateau_decay","title":"<code>plateau_decay</code>","text":"<p>Return pytorch lighting optimizer configuration for plato decay.</p> <p>Parameters:</p> Name Type Description Default <code>optimizer</code> <code>Optimizer</code> <p>Pytorch lighting optimizer of which the learning rate should be scheduled.</p> required <code>decay_rate</code> <code>float</code> <p>Factor by which learning rate should be decayed when plateau is reached.</p> <code>1.0</code> <code>patience</code> <code>int</code> <p>Number of epochs to wait for improvement.</p> <code>10</code> <code>mode</code> <code>str</code> <p><code>min</code> or <code>max</code>.</p> <code>'min'</code> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dict with structure compatible with ptl.  See pytorch lightning documentation</p> Source code in <code>ocl/scheduling.py</code> <pre><code>def plateau_decay(\noptimizer: Optimizer,\ndecay_rate: float = 1.0,\npatience: int = 10,\nmonitor_metric: str = \"val/lotal_loss\",\nmode: str = \"min\",\n) -&gt; Dict[str, Any]:\n\"\"\"Return pytorch lighting optimizer configuration for plato decay.\n    Args:\n        optimizer: Pytorch lighting optimizer of which the learning rate should be scheduled.\n        decay_rate: Factor by which learning rate should be decayed when plateau is reached.\n        patience: Number of epochs to wait for improvement.\n        mode: `min` or `max`.\n    Returns:\n        Dict with structure compatible with ptl.  See\n            [pytorch lightning documentation](\n                https://lightning.ai/docs/pytorch/stable/common/lightning_module.html#configure-optimizers)\n    \"\"\"\nplateau_scheduler = ReduceLROnPlateau(\noptimizer=optimizer, mode=mode, factor=decay_rate, patience=patience\n)\nreturn {\n\"lr_scheduler\": {\n\"scheduler\": plateau_scheduler,\n\"interval\": \"epoch\",\n\"monitor\": monitor_metric,\n}\n}\n</code></pre>"},{"location":"api/ocl/scheduling/#ocl.scheduling.cosine_annealing_with_optional_warmup","title":"<code>cosine_annealing_with_optional_warmup</code>","text":"<p>Return pytorch lighting optimizer configuration for cosine annealing with warmup.</p> <p>Parameters:</p> Name Type Description Default <code>optimizer</code> <code>Optimizer</code> <p>Pytorch lighting optimizer of which the learning rate should be scheduled.</p> required <code>T_max</code> <code>int</code> <p>The length of the scheduling in steps.</p> <code>100000</code> <code>eta_min</code> <code>float</code> <p>Minimal fraction of initial learning rate that should be reached when scheduling cycle is complete.</p> <code>0.0</code> <code>warmup_steps</code> <code>int</code> <p>Number of warmup steps.</p> <code>0</code> <code>error_on_exceeding_steps</code> <code>bool</code> <p>Raise error if more than <code>T_max</code> steps are trained.</p> <code>False</code> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dict with structure compatible with ptl.  See pytorch lightning documentation</p> Source code in <code>ocl/scheduling.py</code> <pre><code>def cosine_annealing_with_optional_warmup(\noptimizer: Optimizer,\nT_max: int = 100000,\neta_min: float = 0.0,\nwarmup_steps: int = 0,\nerror_on_exceeding_steps: bool = False,\n) -&gt; Dict[str, Any]:\n\"\"\"Return pytorch lighting optimizer configuration for cosine annealing with warmup.\n    Args:\n        optimizer: Pytorch lighting optimizer of which the learning rate should be scheduled.\n        T_max: The length of the scheduling in steps.\n        eta_min: Minimal fraction of initial learning rate that should be reached when\n            scheduling cycle is complete.\n        warmup_steps: Number of warmup steps.\n        error_on_exceeding_steps: Raise error if more than `T_max` steps are trained.\n    Returns:\n        Dict with structure compatible with ptl.  See\n            [pytorch lightning documentation](\n                https://lightning.ai/docs/pytorch/stable/common/lightning_module.html#configure-optimizers)\n    \"\"\"\nreturn {\n\"lr_scheduler\": {\n\"scheduler\": _CosineAnnealingWithWarmup(\noptimizer,\nT_max,\neta_min=eta_min,\nwarmup_steps=warmup_steps,\nerror_on_exceeding_steps=error_on_exceeding_steps,\n),\n\"interval\": \"step\",\n}\n}\n</code></pre>"},{"location":"api/ocl/transforms/","title":"ocl.transforms","text":"<p>Module with data pipe transforms.</p> <p>Transforms are callables which transform a input torchdata datapipe into a new datapipe. For further information see ocl.transforms.Transform.</p>"},{"location":"api/ocl/transforms/#ocl.transforms.Transform","title":"<code>Transform</code>","text":"<p>         Bases: <code>ABC</code></p> <p>Abstract Base Class representing a transformation of the input data pipe.</p> <p>A transform is a callable which when called with a torchdata.datapipes.iter.IterDataPipe applies a transformation and returns a new torchdata.datapipes.iter.IterDataPipe.</p> <p>Attributes:</p> Name Type Description <code>is_batch_transform</code> <code>bool</code> <p>True if the transform should be applied to a batch of examples instead of individual examples. False otherwise.</p> <code>fields</code> <code>Tuple[str]</code> <p>Tuple of strings, that indicate which elements of the input are needed for this transform to be applied.  This allows to avoid decoding parts of the input which are not needed for training/evaluating a particular model.</p> Source code in <code>ocl/transforms.py</code> <pre><code>class Transform(ABC):\n\"\"\"Abstract Base Class representing a transformation of the input data pipe.\n    A transform is a callable which when called with a [torchdata.datapipes.iter.IterDataPipe][]\n    applies a transformation and returns a new [torchdata.datapipes.iter.IterDataPipe][].\n    Attributes:\n        is_batch_transform: True if the transform should be applied to a batch of\n            examples instead of individual examples. False otherwise.\n        fields: Tuple of strings, that indicate which elements of the input are needed\n            for this transform to be applied.  This allows to avoid decoding parts of the\n            input which are not needed for training/evaluating a particular model.\n    \"\"\"\nis_batch_transform: bool\n@property\n@abstractmethod\ndef fields(self) -&gt; Tuple[str]:\n\"\"\"Fields that will be transformed with this transform.\"\"\"\n@abstractmethod\ndef __call__(self, input_pipe: IterDataPipe) -&gt; IterDataPipe:\n\"\"\"Application of transform to input pipe.\n        Args:\n            input_pipe: Input data pipe\n        Returns:\n            Transformed data pipe.\n        \"\"\"\n</code></pre>"},{"location":"api/ocl/transforms/#ocl.transforms.Transform.fields","title":"<code>fields: Tuple[str]</code>  <code>property</code> <code>abstractmethod</code>","text":"<p>Fields that will be transformed with this transform.</p>"},{"location":"api/ocl/transforms/#ocl.transforms.Transform.__call__","title":"<code>__call__</code>  <code>abstractmethod</code>","text":"<p>Application of transform to input pipe.</p> <p>Parameters:</p> Name Type Description Default <code>input_pipe</code> <code>IterDataPipe</code> <p>Input data pipe</p> required <p>Returns:</p> Type Description <code>IterDataPipe</code> <p>Transformed data pipe.</p> Source code in <code>ocl/transforms.py</code> <pre><code>@abstractmethod\ndef __call__(self, input_pipe: IterDataPipe) -&gt; IterDataPipe:\n\"\"\"Application of transform to input pipe.\n    Args:\n        input_pipe: Input data pipe\n    Returns:\n        Transformed data pipe.\n    \"\"\"\n</code></pre>"},{"location":"api/ocl/transforms/#ocl.transforms.SimpleTransform","title":"<code>SimpleTransform</code>","text":"<p>         Bases: <code>Transform</code></p> <p>Transform of individual key in input dict using different callables.</p> Example <pre><code>from torchdata.datapipes.iter import IterableWrapper\nfrom ocl.transforms import SimpleTransform\ninput_dicts = [{\"object_a\": 1, \"object_b\": 2}]\ntransform = SimpleTransform(\ntransforms={\n\"object_a\": lambda a: a*2,\n\"object_b\": lambda b: b*3\n}\n)\ninput_pipe = IterableWrapper(input_dicts)\ntransformed_pipe = transform(input_pipe)\nfor transformed_dict in transformed_pipe:\nassert transformed_dict[\"object_a\"] == 1 * 2\nassert transformed_dict[\"object_b\"] == 2 * 3\n</code></pre> Source code in <code>ocl/transforms.py</code> <pre><code>class SimpleTransform(Transform):\n\"\"\"Transform of individual key in input dict using different callables.\n    Example:\n        ```python\n        from torchdata.datapipes.iter import IterableWrapper\n        from ocl.transforms import SimpleTransform\n        input_dicts = [{\"object_a\": 1, \"object_b\": 2}]\n        transform = SimpleTransform(\n            transforms={\n                \"object_a\": lambda a: a*2,\n                \"object_b\": lambda b: b*3\n            }\n        )\n        input_pipe = IterableWrapper(input_dicts)\n        transformed_pipe = transform(input_pipe)\n        for transformed_dict in transformed_pipe:\n            assert transformed_dict[\"object_a\"] == 1 * 2\n            assert transformed_dict[\"object_b\"] == 2 * 3\n        ```\n    \"\"\"\ndef __init__(self, transforms: Dict[str, Callable], batch_transform: bool):\n\"\"\"Initialize SimpleTransform.\n        Args:\n            transforms: Mapping of dict keys to callables that should be used to transform them.\n            batch_transform: Set to true if you want your transform to be applied after the\n                data has been batched.\n        \"\"\"\nself.transforms = transforms\nself.is_batch_transform = batch_transform\n@property\ndef fields(self) -&gt; Tuple[str]:\nreturn tuple(self.transforms.keys())\ndef __call__(self, input_pipe: IterDataPipe) -&gt; IterDataPipe:\n\"\"\"Transform input data pipe using transforms.\n        Args:\n            input_pipe: Input data pipe\n        Returns:\n            Transformed data pipe.\n        \"\"\"\nreturn input_pipe.map_dict(self.transforms)\n</code></pre>"},{"location":"api/ocl/transforms/#ocl.transforms.SimpleTransform.__init__","title":"<code>__init__</code>","text":"<p>Initialize SimpleTransform.</p> <p>Parameters:</p> Name Type Description Default <code>transforms</code> <code>Dict[str, Callable]</code> <p>Mapping of dict keys to callables that should be used to transform them.</p> required <code>batch_transform</code> <code>bool</code> <p>Set to true if you want your transform to be applied after the data has been batched.</p> required Source code in <code>ocl/transforms.py</code> <pre><code>def __init__(self, transforms: Dict[str, Callable], batch_transform: bool):\n\"\"\"Initialize SimpleTransform.\n    Args:\n        transforms: Mapping of dict keys to callables that should be used to transform them.\n        batch_transform: Set to true if you want your transform to be applied after the\n            data has been batched.\n    \"\"\"\nself.transforms = transforms\nself.is_batch_transform = batch_transform\n</code></pre>"},{"location":"api/ocl/transforms/#ocl.transforms.SimpleTransform.__call__","title":"<code>__call__</code>","text":"<p>Transform input data pipe using transforms.</p> <p>Parameters:</p> Name Type Description Default <code>input_pipe</code> <code>IterDataPipe</code> <p>Input data pipe</p> required <p>Returns:</p> Type Description <code>IterDataPipe</code> <p>Transformed data pipe.</p> Source code in <code>ocl/transforms.py</code> <pre><code>def __call__(self, input_pipe: IterDataPipe) -&gt; IterDataPipe:\n\"\"\"Transform input data pipe using transforms.\n    Args:\n        input_pipe: Input data pipe\n    Returns:\n        Transformed data pipe.\n    \"\"\"\nreturn input_pipe.map_dict(self.transforms)\n</code></pre>"},{"location":"api/ocl/transforms/#ocl.transforms.DuplicateFields","title":"<code>DuplicateFields</code>","text":"<p>         Bases: <code>Transform</code></p> <p>Transform to duplicate a key of a dictionary.</p> <p>This is useful if your pipeline requires the same input to be transformed in different ways.</p> Example <pre><code>from torchdata.datapipes.iter import IterableWrapper\nfrom ocl.transforms import DuplicateFields\ninput_dicts = [{\"object_a\": 1, \"object_b\": 2}]\ntransform = DuplicateFields(\nmapping={\n\"object_a\": \"copy_of_object_a\",\n\"object_b\": \"copy_of_object_b\"\n}\n)\ninput_pipe = IterableWrapper(input_dicts)\ntransformed_pipe = transform(input_pipe)\nfor transformed_dict in transformed_pipe:\nassert transformed_dict[\"object_a\"] == 1\nassert transformed_dict[\"copy_of_object_a\"] == 1\nassert transformed_dict[\"object_b\"] == 2\nassert transformed_dict[\"copy_of_object_b\"] == 2\n</code></pre> Source code in <code>ocl/transforms.py</code> <pre><code>class DuplicateFields(Transform):\n\"\"\"Transform to duplicate a key of a dictionary.\n    This is useful if your pipeline requires the same input to be transformed in different ways.\n    Example:\n        ```python\n        from torchdata.datapipes.iter import IterableWrapper\n        from ocl.transforms import DuplicateFields\n        input_dicts = [{\"object_a\": 1, \"object_b\": 2}]\n        transform = DuplicateFields(\n            mapping={\n                \"object_a\": \"copy_of_object_a\",\n                \"object_b\": \"copy_of_object_b\"\n            }\n        )\n        input_pipe = IterableWrapper(input_dicts)\n        transformed_pipe = transform(input_pipe)\n        for transformed_dict in transformed_pipe:\n            assert transformed_dict[\"object_a\"] == 1\n            assert transformed_dict[\"copy_of_object_a\"] == 1\n            assert transformed_dict[\"object_b\"] == 2\n            assert transformed_dict[\"copy_of_object_b\"] == 2\n        ```\n    \"\"\"\ndef __init__(self, mapping: Dict[str, str], batch_transform: bool):\n\"\"\"Initialize DuplicateFields.\n        Args:\n            mapping: Source to target mapping for dupplicated fields. Keys are sources,\n                values are the key for duplicated field.\n            batch_transform: Apply to batched input.\n        \"\"\"\nself.mapping = mapping\nself.is_batch_transform = batch_transform\ndef duplicate_keys(self, input_dict: Dict[str, Any]):\nfor key, value in self.mapping.items():\ninput_dict[value] = input_dict[key]\nreturn input_dict\n@property\ndef fields(self) -&gt; Tuple[str]:\nreturn tuple(self.mapping.keys())\ndef __call__(self, input_pipe: IterDataPipe) -&gt; IterDataPipe:\nreturn input_pipe.map(self.duplicate_keys)\n</code></pre>"},{"location":"api/ocl/transforms/#ocl.transforms.DuplicateFields.__init__","title":"<code>__init__</code>","text":"<p>Initialize DuplicateFields.</p> <p>Parameters:</p> Name Type Description Default <code>mapping</code> <code>Dict[str, str]</code> <p>Source to target mapping for dupplicated fields. Keys are sources, values are the key for duplicated field.</p> required <code>batch_transform</code> <code>bool</code> <p>Apply to batched input.</p> required Source code in <code>ocl/transforms.py</code> <pre><code>def __init__(self, mapping: Dict[str, str], batch_transform: bool):\n\"\"\"Initialize DuplicateFields.\n    Args:\n        mapping: Source to target mapping for dupplicated fields. Keys are sources,\n            values are the key for duplicated field.\n        batch_transform: Apply to batched input.\n    \"\"\"\nself.mapping = mapping\nself.is_batch_transform = batch_transform\n</code></pre>"},{"location":"api/ocl/transforms/#ocl.transforms.Map","title":"<code>Map</code>","text":"<p>         Bases: <code>Transform</code></p> <p>Apply a function to the whole input dict to create a new output dict.</p> <p>This transform requires explicitly defining the input fields as this cannot be determined from the provided callable alone.</p> Example <pre><code>from torchdata.datapipes.iter import IterableWrapper\nfrom ocl.transforms import Map\ninput_dicts = [{\"object_a\": 1, \"object_b\": 2}]\ndef combine_a_and_b(input_dict):\noutput_dict = input_dict.copy()\noutput_dict[\"combined\"] = input_dict[\"object_a\"] + input_dict[\"object_b\"]\nreturn output_dict\ntransform = Map(\ntransform=combine_a_and_b,\nfields=(\"object_a\", \"object_b\")\n)\ninput_pipe = IterableWrapper(input_dicts)\ntransformed_pipe = transform(input_pipe)\nfor transformed_dict in transformed_pipe:\na = transformed_dict[\"object_a\"]\nb = transformed_dict[\"object_b\"]\nassert transformed_dict[\"combined\"] == a + b\n</code></pre> Source code in <code>ocl/transforms.py</code> <pre><code>class Map(Transform):\n\"\"\"Apply a function to the whole input dict to create a new output dict.\n    This transform requires explicitly defining the input fields as this\n    cannot be determined from the provided callable alone.\n    Example:\n        ```python\n        from torchdata.datapipes.iter import IterableWrapper\n        from ocl.transforms import Map\n        input_dicts = [{\"object_a\": 1, \"object_b\": 2}]\n        def combine_a_and_b(input_dict):\n            output_dict = input_dict.copy()\n            output_dict[\"combined\"] = input_dict[\"object_a\"] + input_dict[\"object_b\"]\n            return output_dict\n        transform = Map(\n            transform=combine_a_and_b,\n            fields=(\"object_a\", \"object_b\")\n        )\n        input_pipe = IterableWrapper(input_dicts)\n        transformed_pipe = transform(input_pipe)\n        for transformed_dict in transformed_pipe:\n            a = transformed_dict[\"object_a\"]\n            b = transformed_dict[\"object_b\"]\n            assert transformed_dict[\"combined\"] == a + b\n        ```\n    \"\"\"\ndef __init__(\nself,\ntransform: Callable[[Dict[str, Any]], Dict[str, Any]],\nfields: Tuple[str],\nbatch_transform: bool,\n):\n\"\"\"Initialize Map transform.\n        Args:\n            transform: Callable which is applied to the individual input dictionaries.\n            fields: The fields the transform requires to operate.\n            batch_transform: Apply to batched input.\n        \"\"\"\nself.transfrom = transform\nself._fields = fields\nself.is_batch_transform = batch_transform\n@property\ndef fields(self) -&gt; Tuple[str]:\nreturn self._fields\ndef __call__(self, input_pipe: IterDataPipe) -&gt; IterDataPipe:\nreturn input_pipe.map(self.transfrom)\n</code></pre>"},{"location":"api/ocl/transforms/#ocl.transforms.Map.__init__","title":"<code>__init__</code>","text":"<p>Initialize Map transform.</p> <p>Parameters:</p> Name Type Description Default <code>transform</code> <code>Callable[[Dict[str, Any]], Dict[str, Any]]</code> <p>Callable which is applied to the individual input dictionaries.</p> required <code>fields</code> <code>Tuple[str]</code> <p>The fields the transform requires to operate.</p> required <code>batch_transform</code> <code>bool</code> <p>Apply to batched input.</p> required Source code in <code>ocl/transforms.py</code> <pre><code>def __init__(\nself,\ntransform: Callable[[Dict[str, Any]], Dict[str, Any]],\nfields: Tuple[str],\nbatch_transform: bool,\n):\n\"\"\"Initialize Map transform.\n    Args:\n        transform: Callable which is applied to the individual input dictionaries.\n        fields: The fields the transform requires to operate.\n        batch_transform: Apply to batched input.\n    \"\"\"\nself.transfrom = transform\nself._fields = fields\nself.is_batch_transform = batch_transform\n</code></pre>"},{"location":"api/ocl/transforms/#ocl.transforms.Filter","title":"<code>Filter</code>","text":"<p>         Bases: <code>Transform</code></p> <p>Filter samples according to predicate.</p> <p>Remove samples from input data pipe by evaluating a predicate.</p> Example <pre><code>from torchdata.datapipes.iter import IterableWrapper\nfrom ocl.transforms import Filter\ninput_dicts = [{\"myvalue\": 5}, {\"myvalue\": 10}]\ntransform = Filter(\npredicate=lambda a: a &gt; 5,\nfields=(\"myvalue\",)\n)\ninput_pipe = IterableWrapper(input_dicts)\ntransformed_pipe = transform(input_pipe)\nfor transformed_dict in transformed_pipe:\nassert transformed_dict[\"myvalue\"] &gt; 5\n</code></pre> Source code in <code>ocl/transforms.py</code> <pre><code>class Filter(Transform):\n\"\"\"Filter samples according to predicate.\n    Remove samples from input data pipe by evaluating a predicate.\n    Example:\n        ```python\n        from torchdata.datapipes.iter import IterableWrapper\n        from ocl.transforms import Filter\n        input_dicts = [{\"myvalue\": 5}, {\"myvalue\": 10}]\n        transform = Filter(\n            predicate=lambda a: a &gt; 5,\n            fields=(\"myvalue\",)\n        )\n        input_pipe = IterableWrapper(input_dicts)\n        transformed_pipe = transform(input_pipe)\n        for transformed_dict in transformed_pipe:\n            assert transformed_dict[\"myvalue\"] &gt; 5\n        ```\n    \"\"\"\ndef __init__(self, predicate: Callable[..., bool], fields: Sequence[str]):\n\"\"\"Transform to create a subset of a dataset by discarding samples.\n        Args:\n            predicate: Function which determines if elements should be kept (return value is True)\n                or discarded (return value is False). The function is only provided with the fields\n                specified in the `fields` parameter.\n            fields (Sequence[str]): The fields from the input which should be passed on to the\n                predicate for evaluation.\n        \"\"\"\nself.predicate = predicate\nself._fields = tuple(fields)\nself.is_batch_transform = False\n@property\ndef fields(self):\nreturn self._fields\ndef _filter_using_predicate(self, d: Dict[str, Any]):\nreturn self.predicate(*(d[field] for field in self._fields))\ndef __call__(self, input_pipe: IterDataPipe):\nreturn input_pipe.filter(self._filter_using_predicate)\n</code></pre>"},{"location":"api/ocl/transforms/#ocl.transforms.Filter.__init__","title":"<code>__init__</code>","text":"<p>Transform to create a subset of a dataset by discarding samples.</p> <p>Parameters:</p> Name Type Description Default <code>predicate</code> <code>Callable[..., bool]</code> <p>Function which determines if elements should be kept (return value is True) or discarded (return value is False). The function is only provided with the fields specified in the <code>fields</code> parameter.</p> required <code>fields</code> <code>Sequence[str]</code> <p>The fields from the input which should be passed on to the predicate for evaluation.</p> required Source code in <code>ocl/transforms.py</code> <pre><code>def __init__(self, predicate: Callable[..., bool], fields: Sequence[str]):\n\"\"\"Transform to create a subset of a dataset by discarding samples.\n    Args:\n        predicate: Function which determines if elements should be kept (return value is True)\n            or discarded (return value is False). The function is only provided with the fields\n            specified in the `fields` parameter.\n        fields (Sequence[str]): The fields from the input which should be passed on to the\n            predicate for evaluation.\n    \"\"\"\nself.predicate = predicate\nself._fields = tuple(fields)\nself.is_batch_transform = False\n</code></pre>"},{"location":"api/ocl/transforms/#ocl.transforms.SampleSlices","title":"<code>SampleSlices</code>","text":"<p>         Bases: <code>Transform</code></p> <p>Transform to sample slices from input tensors / numpy arrays.</p> <p>If multiple fields are provided the input tensors are assumed to be of same length along slicing axes and the same slices will be returned.</p> Example <pre><code>import numpy as np\nfrom torchdata.datapipes.iter import IterableWrapper\nfrom ocl.transforms import SampleSlices\nmy_array = np.random.randn(100, 10)\ninput_dicts = [{\"array1\": my_array, \"array2\": my_array.copy()}]\ntransform = SampleSlices(\nn_slices_per_input=5,\nfields=(\"array1\", \"array2\"),\ndim=0,\nshuffle_buffer_size=1\n)\ninput_pipe = IterableWrapper(input_dicts)\ntransformed_pipe = transform(input_pipe)\nfor transformed_dict in transformed_pipe:\nassert transformed_dict[\"array1\"].shape == (5, 10)\nassert transformed_dict[\"array2\"].shape == (5, 10)\nassert np.allclose(transformed_dict[\"array1\"], transformed_dict[\"array2\"])\n</code></pre> Source code in <code>ocl/transforms.py</code> <pre><code>class SampleSlices(Transform):\n\"\"\"Transform to sample slices from input tensors / numpy arrays.\n    If multiple fields are provided the input tensors are assumed to be of\n    same length along slicing axes and the same slices will be returned.\n    Example:\n        ```python\n        import numpy as np\n        from torchdata.datapipes.iter import IterableWrapper\n        from ocl.transforms import SampleSlices\n        my_array = np.random.randn(100, 10)\n        input_dicts = [{\"array1\": my_array, \"array2\": my_array.copy()}]\n        transform = SampleSlices(\n            n_slices_per_input=5,\n            fields=(\"array1\", \"array2\"),\n            dim=0,\n            shuffle_buffer_size=1\n        )\n        input_pipe = IterableWrapper(input_dicts)\n        transformed_pipe = transform(input_pipe)\n        for transformed_dict in transformed_pipe:\n            assert transformed_dict[\"array1\"].shape == (5, 10)\n            assert transformed_dict[\"array2\"].shape == (5, 10)\n            assert np.allclose(transformed_dict[\"array1\"], transformed_dict[\"array2\"])\n        ```\n    \"\"\"\ndef __init__(\nself,\nn_slices_per_input: int,\nfields: Sequence[str],\ndim: int = 0,\nseed: int = 39480234,\nper_epoch: bool = False,\nshuffle_buffer_size: int = 1000,\n):\n\"\"\"Initialize SampleSlices.\n        Args:\n            n_slices_per_input: Number of slices per input to sample. -1 indicates that all possible\n                slices should be sampled.\n            fields: The fields that should be considered video data and thus sliced\n                according to the frame sampling during training.\n            dim: The dimension along which to slice the tensors.\n            seed: Random number generator seed to deterministic sampling during evaluation.\n            per_epoch: Sampling of frames over epochs, this ensures that after\n                n_frames / n_frames_per_video epochs all frames have been seen at least once.\n                In the case of uneven division, some frames will be seen more than once.\n            shuffle_buffer_size: Size of shuffle buffer used during training. An additional\n                shuffling step ensures each batch contains a diverse set of images and not only\n                images from the same video.\n        \"\"\"\nself.n_slices_per_input = n_slices_per_input\nself._fields = tuple(fields)\nself.dim = dim\nself.seed = seed\nself.per_epoch = per_epoch\nself.shuffle_buffer_size = shuffle_buffer_size\nself.is_batch_transform = False\ndef slice_data(self, data, index: int):\n\"\"\"Small utility method to slice a numpy array along a specified axis.\"\"\"\nn_dims_before = self.dim\nn_dims_after = data.ndim - 1 - self.dim\nslices = (slice(None),) * n_dims_before + (index,) + (slice(None),) * n_dims_after\nreturn data[slices]\ndef sample_frames_using_key(self, data):\n\"\"\"Sample frames deterministically from generator of videos using the __key__ field.\"\"\"\nfor sample in data:\n# Initialize random number generator dependent on instance key. This should make the\n# sampling process deterministic, which is useful when sampling frames for the\n# validation/test data.\nkey = sample[\"__key__\"]\n# TODO (hornmax): We assume all fields to have the same size. I do not want to check\n# this here as it seems a bit verbose.\nn_frames = sample[self._fields[0]].shape[self.dim]\nframes_per_video = self.n_slices_per_input if self.n_slices_per_input != -1 else n_frames\nif self.per_epoch and self.n_slices_per_input != -1:\nn_different_epochs_per_seed = int(math.ceil(n_frames / frames_per_video))\ntry:\nepoch = int(os.environ[\"EPOCH\"])\nexcept KeyError:\nraise RuntimeError(\n\"Using SampleSlices with per_epoch=True \"\n\"requires environment variable `EPOCH` to be set. \"\n\"You might need to add the SetEpochEnvironmentVariable callback.\"\n)\n# Only update the seed after n_frames / n_frames_per_video epochs.\n# This ensures that we get the same random order of frames until\n# we have sampled all of them.\nrand = np.random.RandomState(\nint(key) + self.seed + (epoch // n_different_epochs_per_seed)\n)\nindices = rand.permutation(n_frames)\nselected_frames = indices[\nepoch * self.n_slices_per_input : (epoch + 1) * self.n_slices_per_input\n].tolist()\nif len(selected_frames) &lt; self.n_slices_per_input:\n# Input cannot be evenly split, take some frames from the first batch of frames.\nn_missing = self.n_slices_per_input - len(selected_frames)\nselected_frames.extend(indices[0:n_missing].tolist())\nelse:\nrand = random.Random(int(key) + self.seed)\nselected_frames = rand.sample(range(n_frames), k=frames_per_video)\nfor frame in selected_frames:\n# Slice the fields according to the frame.\nsliced_fields = {\nfield: self.slice_data(sample[field], frame) for field in self._fields\n}\n# Leave all fields besides the sliced ones as before, augment the __key__ field to\n# include the frame number.\nsliced_fields[\"__key__\"] = f\"{key}_{frame}\"\nyield {**sample, **sliced_fields}\n# Delete fields to be sure we remove all references.\nfor field in self.fields:\ndel sample[field]\n@property\ndef fields(self):\nreturn self._fields\ndef __call__(self, input_pipe: IterDataPipe) -&gt; IterDataPipe:\noutput_pipe = input_pipe.then(self.sample_frames_using_key)\nif self.shuffle_buffer_size &gt; 1:\noutput_pipe = output_pipe.shuffle(buffer_size=self.shuffle_buffer_size)\nreturn output_pipe\n</code></pre>"},{"location":"api/ocl/transforms/#ocl.transforms.SampleSlices.__init__","title":"<code>__init__</code>","text":"<p>Initialize SampleSlices.</p> <p>Parameters:</p> Name Type Description Default <code>n_slices_per_input</code> <code>int</code> <p>Number of slices per input to sample. -1 indicates that all possible slices should be sampled.</p> required <code>fields</code> <code>Sequence[str]</code> <p>The fields that should be considered video data and thus sliced according to the frame sampling during training.</p> required <code>dim</code> <code>int</code> <p>The dimension along which to slice the tensors.</p> <code>0</code> <code>seed</code> <code>int</code> <p>Random number generator seed to deterministic sampling during evaluation.</p> <code>39480234</code> <code>per_epoch</code> <code>bool</code> <p>Sampling of frames over epochs, this ensures that after n_frames / n_frames_per_video epochs all frames have been seen at least once. In the case of uneven division, some frames will be seen more than once.</p> <code>False</code> <code>shuffle_buffer_size</code> <code>int</code> <p>Size of shuffle buffer used during training. An additional shuffling step ensures each batch contains a diverse set of images and not only images from the same video.</p> <code>1000</code> Source code in <code>ocl/transforms.py</code> <pre><code>def __init__(\nself,\nn_slices_per_input: int,\nfields: Sequence[str],\ndim: int = 0,\nseed: int = 39480234,\nper_epoch: bool = False,\nshuffle_buffer_size: int = 1000,\n):\n\"\"\"Initialize SampleSlices.\n    Args:\n        n_slices_per_input: Number of slices per input to sample. -1 indicates that all possible\n            slices should be sampled.\n        fields: The fields that should be considered video data and thus sliced\n            according to the frame sampling during training.\n        dim: The dimension along which to slice the tensors.\n        seed: Random number generator seed to deterministic sampling during evaluation.\n        per_epoch: Sampling of frames over epochs, this ensures that after\n            n_frames / n_frames_per_video epochs all frames have been seen at least once.\n            In the case of uneven division, some frames will be seen more than once.\n        shuffle_buffer_size: Size of shuffle buffer used during training. An additional\n            shuffling step ensures each batch contains a diverse set of images and not only\n            images from the same video.\n    \"\"\"\nself.n_slices_per_input = n_slices_per_input\nself._fields = tuple(fields)\nself.dim = dim\nself.seed = seed\nself.per_epoch = per_epoch\nself.shuffle_buffer_size = shuffle_buffer_size\nself.is_batch_transform = False\n</code></pre>"},{"location":"api/ocl/transforms/#ocl.transforms.SampleSlices.slice_data","title":"<code>slice_data</code>","text":"<p>Small utility method to slice a numpy array along a specified axis.</p> Source code in <code>ocl/transforms.py</code> <pre><code>def slice_data(self, data, index: int):\n\"\"\"Small utility method to slice a numpy array along a specified axis.\"\"\"\nn_dims_before = self.dim\nn_dims_after = data.ndim - 1 - self.dim\nslices = (slice(None),) * n_dims_before + (index,) + (slice(None),) * n_dims_after\nreturn data[slices]\n</code></pre>"},{"location":"api/ocl/transforms/#ocl.transforms.SampleSlices.sample_frames_using_key","title":"<code>sample_frames_using_key</code>","text":"<p>Sample frames deterministically from generator of videos using the key field.</p> Source code in <code>ocl/transforms.py</code> <pre><code>def sample_frames_using_key(self, data):\n\"\"\"Sample frames deterministically from generator of videos using the __key__ field.\"\"\"\nfor sample in data:\n# Initialize random number generator dependent on instance key. This should make the\n# sampling process deterministic, which is useful when sampling frames for the\n# validation/test data.\nkey = sample[\"__key__\"]\n# TODO (hornmax): We assume all fields to have the same size. I do not want to check\n# this here as it seems a bit verbose.\nn_frames = sample[self._fields[0]].shape[self.dim]\nframes_per_video = self.n_slices_per_input if self.n_slices_per_input != -1 else n_frames\nif self.per_epoch and self.n_slices_per_input != -1:\nn_different_epochs_per_seed = int(math.ceil(n_frames / frames_per_video))\ntry:\nepoch = int(os.environ[\"EPOCH\"])\nexcept KeyError:\nraise RuntimeError(\n\"Using SampleSlices with per_epoch=True \"\n\"requires environment variable `EPOCH` to be set. \"\n\"You might need to add the SetEpochEnvironmentVariable callback.\"\n)\n# Only update the seed after n_frames / n_frames_per_video epochs.\n# This ensures that we get the same random order of frames until\n# we have sampled all of them.\nrand = np.random.RandomState(\nint(key) + self.seed + (epoch // n_different_epochs_per_seed)\n)\nindices = rand.permutation(n_frames)\nselected_frames = indices[\nepoch * self.n_slices_per_input : (epoch + 1) * self.n_slices_per_input\n].tolist()\nif len(selected_frames) &lt; self.n_slices_per_input:\n# Input cannot be evenly split, take some frames from the first batch of frames.\nn_missing = self.n_slices_per_input - len(selected_frames)\nselected_frames.extend(indices[0:n_missing].tolist())\nelse:\nrand = random.Random(int(key) + self.seed)\nselected_frames = rand.sample(range(n_frames), k=frames_per_video)\nfor frame in selected_frames:\n# Slice the fields according to the frame.\nsliced_fields = {\nfield: self.slice_data(sample[field], frame) for field in self._fields\n}\n# Leave all fields besides the sliced ones as before, augment the __key__ field to\n# include the frame number.\nsliced_fields[\"__key__\"] = f\"{key}_{frame}\"\nyield {**sample, **sliced_fields}\n# Delete fields to be sure we remove all references.\nfor field in self.fields:\ndel sample[field]\n</code></pre>"},{"location":"api/ocl/transforms/#ocl.transforms.SplitConsecutive","title":"<code>SplitConsecutive</code>","text":"<p>         Bases: <code>Transform</code></p> Source code in <code>ocl/transforms.py</code> <pre><code>class SplitConsecutive(Transform):\ndef __init__(\nself,\nn_consecutive: int,\nfields: Sequence[str],\ndim: int = 0,\nshuffle_buffer_size: int = 1000,\ndrop_last: bool = True,\n):\nself.n_consecutive = n_consecutive\nself._fields = tuple(fields)\nself.dim = dim\nself.shuffle_buffer_size = shuffle_buffer_size\nself.drop_last = drop_last\nself.is_batch_transform = False\n@property\ndef fields(self):\nreturn self._fields\ndef split_to_consecutive_frames(self, data):\n\"\"\"Sample frames deterministically from generator of videos using the __key__ field.\"\"\"\nfor sample in data:\nkey = sample[\"__key__\"]\nn_frames = sample[self._fields[0]].shape[self.dim]\nsplitted_fields = [\nnp.array_split(\nsample[field],\nrange(self.n_consecutive, n_frames, self.n_consecutive),\naxis=self.dim,\n)\nfor field in self._fields\n]\nfor i, slices in enumerate(zip(*splitted_fields)):\nif self.drop_last and slices[0].shape[self.dim] &lt; self.n_consecutive:\n# Last slice of not equally divisible input, discard.\ncontinue\nsliced_fields = dict(zip(self._fields, slices))\nsliced_fields[\"__key__\"] = f\"{key}_{i}\"\nyield {**sample, **sliced_fields}\ndef __call__(self, input_pipe: IterDataPipe) -&gt; IterDataPipe:\noutput_pipe = input_pipe.then(self.split_to_consecutive_frames)\nif self.shuffle_buffer_size &gt; 1:\noutput_pipe = output_pipe.shuffle(buffer_size=self.shuffle_buffer_size)\nreturn output_pipe\n</code></pre>"},{"location":"api/ocl/transforms/#ocl.transforms.SplitConsecutive.split_to_consecutive_frames","title":"<code>split_to_consecutive_frames</code>","text":"<p>Sample frames deterministically from generator of videos using the key field.</p> Source code in <code>ocl/transforms.py</code> <pre><code>def split_to_consecutive_frames(self, data):\n\"\"\"Sample frames deterministically from generator of videos using the __key__ field.\"\"\"\nfor sample in data:\nkey = sample[\"__key__\"]\nn_frames = sample[self._fields[0]].shape[self.dim]\nsplitted_fields = [\nnp.array_split(\nsample[field],\nrange(self.n_consecutive, n_frames, self.n_consecutive),\naxis=self.dim,\n)\nfor field in self._fields\n]\nfor i, slices in enumerate(zip(*splitted_fields)):\nif self.drop_last and slices[0].shape[self.dim] &lt; self.n_consecutive:\n# Last slice of not equally divisible input, discard.\ncontinue\nsliced_fields = dict(zip(self._fields, slices))\nsliced_fields[\"__key__\"] = f\"{key}_{i}\"\nyield {**sample, **sliced_fields}\n</code></pre>"},{"location":"api/ocl/transforms/#ocl.transforms.SampleConsecutive","title":"<code>SampleConsecutive</code>","text":"<p>         Bases: <code>Transform</code></p> <p>Select a random consecutive subsequence of frames in a strided manner.</p> <p>Given a sequence of [1, 2, 3, 4, 5, 6, 7, 8, 9] this will return one of [1, 2, 3] [4, 5, 6] [7, 8, 9].</p> Source code in <code>ocl/transforms.py</code> <pre><code>class SampleConsecutive(Transform):\n\"\"\"Select a random consecutive subsequence of frames in a strided manner.\n    Given a sequence of [1, 2, 3, 4, 5, 6, 7, 8, 9] this will return one of\n    [1, 2, 3] [4, 5, 6] [7, 8, 9].\n    \"\"\"\ndef __init__(\nself,\nn_consecutive: int,\nfields: Sequence[str],\ndim: int = 0,\n):\nself.n_consecutive = n_consecutive\nself._fields = tuple(fields)\nself.dim = dim\nself._random = None\nself.is_batch_transform = False\n@property\ndef fields(self):\nreturn self._fields\n@property\ndef random(self):\nif not self._random:\nworker_info = torch.utils.data.get_worker_info()\nif worker_info:\nself._random = random.Random(worker_info.seed)\nelse:\nself._random = random.Random(torch.initial_seed())\nreturn self._random\ndef split_to_consecutive_frames(self, sample):\n\"\"\"Sample frames deterministically from generator of videos using the __key__ field.\"\"\"\nkey = sample[\"__key__\"]\nn_frames = sample[self._fields[0]].shape[self.dim]\nsplitted_fields = [\nnp.array_split(\nsample[field],\nrange(self.n_consecutive, n_frames, self.n_consecutive),\naxis=self.dim,\n)\nfor field in self._fields\n]\nn_fragments = len(splitted_fields[0])\nif len(splitted_fields[0][-1] &lt; self.n_consecutive):\n# Discard last fragment if too short.\nn_fragments -= 1\nfragment_id = self.random.randint(0, n_fragments - 1)\nsliced_fields: Dict[str, Any] = {\nfield_name: splitted_field[fragment_id]\nfor field_name, splitted_field in zip(self._fields, splitted_fields)\n}\nsliced_fields[\"__key__\"] = f\"{key}_{fragment_id}\"\nreturn {**sample, **sliced_fields}\ndef __call__(self, input_pipe: IterDataPipe) -&gt; IterDataPipe:\nreturn input_pipe.map(self.split_to_consecutive_frames)\n</code></pre>"},{"location":"api/ocl/transforms/#ocl.transforms.SampleConsecutive.split_to_consecutive_frames","title":"<code>split_to_consecutive_frames</code>","text":"<p>Sample frames deterministically from generator of videos using the key field.</p> Source code in <code>ocl/transforms.py</code> <pre><code>def split_to_consecutive_frames(self, sample):\n\"\"\"Sample frames deterministically from generator of videos using the __key__ field.\"\"\"\nkey = sample[\"__key__\"]\nn_frames = sample[self._fields[0]].shape[self.dim]\nsplitted_fields = [\nnp.array_split(\nsample[field],\nrange(self.n_consecutive, n_frames, self.n_consecutive),\naxis=self.dim,\n)\nfor field in self._fields\n]\nn_fragments = len(splitted_fields[0])\nif len(splitted_fields[0][-1] &lt; self.n_consecutive):\n# Discard last fragment if too short.\nn_fragments -= 1\nfragment_id = self.random.randint(0, n_fragments - 1)\nsliced_fields: Dict[str, Any] = {\nfield_name: splitted_field[fragment_id]\nfor field_name, splitted_field in zip(self._fields, splitted_fields)\n}\nsliced_fields[\"__key__\"] = f\"{key}_{fragment_id}\"\nreturn {**sample, **sliced_fields}\n</code></pre>"},{"location":"api/ocl/transforms/#ocl.transforms.VideoDecoder","title":"<code>VideoDecoder</code>","text":"<p>         Bases: <code>Transform</code></p> <p>Video decoder based on Decord.</p> <p>Video decoding is implemented as a preprocessing transform instead of a part of the decoding mechanics as this allows sparse decoding if we only require parts of the input video.</p> Source code in <code>ocl/transforms.py</code> <pre><code>class VideoDecoder(Transform):\n\"\"\"Video decoder based on Decord.\n    Video decoding is implemented as a preprocessing transform instead of a\n    part of the decoding mechanics as this allows sparse decoding if we\n    only require parts of the input video.\n    \"\"\"\ndef __init__(\nself,\nfields: Sequence[str],\nstride: int = 1,\nsplit_extension: bool = True,\nvideo_reader_kwargs: Optional[Dict[str, Any]] = None,\n):\n\"\"\"Video decoder based on decord.\n        It will decode the whole video into a single tensor and can be used with other downstream\n        processing plugins.\n        Args:\n            fields (Sequence[str]): The field of the input dictionary containing the video bytes.\n            stride (int): Downsample frames by using striding. Default: 1\n            split_extension (bool): Split the extension off the field name.\n            video_reader_kwargs (Dict[str, Any]): Arguments to decord.VideoReader.\n        \"\"\"\nself._fields = tuple(fields)\nself.stride = stride\nself.split_extension = split_extension\nself.video_reader_kwargs = video_reader_kwargs if video_reader_kwargs else {}\nself.is_batch_transform = False\n@property\ndef fields(self):\nreturn self._fields\ndef _chunk_iterator(\nself, vrs: Dict[str, decord.VideoReader], key: str, inputs: Dict[str, Any]\n) -&gt; Iterable[Tuple[str, Dict]]:\n\"\"\"Iterate over chunks of the video.\n        For the video decoder we simply return a single chunk containing the whole video, subclasses\n        might override this method though.\n        Returns:\n            str: Derived key which combines chunk and video key.\n            torch.Tensor: Chunk of video data.\n            Dict: Additional information, for example which frames where selected.  This might be of\n                relevance when different modalities need to be sliced in a similar fashion as the\n                video input.\n        \"\"\"\n# Get whole video.\nindices = list(range(0, len(next(iter(vrs.values()))), self.stride))\nvideos = {output_name: vr.get_batch(indices) for output_name, vr in vrs.items()}\nyield (key, {**videos, \"decoded_indices\": indices})\ndef video_decoding(self, input_generator):\nfor input_data in input_generator:\nkey = input_data[\"__key__\"]\nvrs = {}\nfor field in self.fields:\nvideo_file: StreamWrapper = input_data[field]\n# Remove the input field\ndel input_data[field]\nif self.split_extension:\noutput_field, _ = os.path.splitext(field)\nelse:\noutput_field = field\nvr = decord.VideoReader(video_file.file_obj, **self.video_reader_kwargs)\nvideo_file.autoclose()\nvrs[output_field] = vr\nfor derived_key, videos_and_additional_info in self._chunk_iterator(\nvrs, key, input_data\n):\nyield {\n**input_data,\n\"__key__\": derived_key,\n**videos_and_additional_info,\n}\ndef __call__(self, input_pipe: IterDataPipe) -&gt; IterDataPipe:\nreturn input_pipe.then(self.video_decoding)\n</code></pre>"},{"location":"api/ocl/transforms/#ocl.transforms.VideoDecoder.__init__","title":"<code>__init__</code>","text":"<p>Video decoder based on decord.</p> <p>It will decode the whole video into a single tensor and can be used with other downstream processing plugins.</p> <p>Parameters:</p> Name Type Description Default <code>fields</code> <code>Sequence[str]</code> <p>The field of the input dictionary containing the video bytes.</p> required <code>stride</code> <code>int</code> <p>Downsample frames by using striding. Default: 1</p> <code>1</code> <code>split_extension</code> <code>bool</code> <p>Split the extension off the field name.</p> <code>True</code> <code>video_reader_kwargs</code> <code>Dict[str, Any]</code> <p>Arguments to decord.VideoReader.</p> <code>None</code> Source code in <code>ocl/transforms.py</code> <pre><code>def __init__(\nself,\nfields: Sequence[str],\nstride: int = 1,\nsplit_extension: bool = True,\nvideo_reader_kwargs: Optional[Dict[str, Any]] = None,\n):\n\"\"\"Video decoder based on decord.\n    It will decode the whole video into a single tensor and can be used with other downstream\n    processing plugins.\n    Args:\n        fields (Sequence[str]): The field of the input dictionary containing the video bytes.\n        stride (int): Downsample frames by using striding. Default: 1\n        split_extension (bool): Split the extension off the field name.\n        video_reader_kwargs (Dict[str, Any]): Arguments to decord.VideoReader.\n    \"\"\"\nself._fields = tuple(fields)\nself.stride = stride\nself.split_extension = split_extension\nself.video_reader_kwargs = video_reader_kwargs if video_reader_kwargs else {}\nself.is_batch_transform = False\n</code></pre>"},{"location":"api/ocl/transforms/#ocl.transforms.DecodeRandomWindow","title":"<code>DecodeRandomWindow</code>","text":"<p>         Bases: <code>VideoDecoder</code></p> <p>Decode a random window of the video.</p> Source code in <code>ocl/transforms.py</code> <pre><code>class DecodeRandomWindow(VideoDecoder):\n\"\"\"Decode a random window of the video.\"\"\"\ndef __init__(self, n_consecutive_frames: int, **video_decoder_args):\nself.n_consecutive_frames = n_consecutive_frames\nself._random = None\nsuper().__init__(**video_decoder_args)\n@property\ndef random(self):\nif not self._random:\nworker_info = torch.utils.data.get_worker_info()\nif worker_info:\nself._random = random.Random(worker_info.seed)\nelse:\nself._random = random.Random(torch.initial_seed())\nreturn self._random\ndef _chunk_iterator(\nself, vrs: Mapping[str, decord.VideoReader], key: str, inputs: Dict[str, Any]\n) -&gt; Iterable[Tuple[str, Dict]]:\n\"\"\"Iterate over chunks of the video.\n        Returns:\n            str: Derived key which combines chunk and video key.\n            torch.Tensor: Chunk of video data.\n            Dict: Additional information, for example which frames where selected.  This might be of\n                relevance when different modalities need to be sliced in a similar fashion as the\n                video input.\n        \"\"\"\nn_frames = len(next(iter(vrs.values())))\nassert self.n_consecutive_frames * self.stride &lt; n_frames\nstarting_index = self.random.randint(0, n_frames - self.n_consecutive_frames * self.stride)\nindices = list(\nrange(\nstarting_index, starting_index + self.n_consecutive_frames * self.stride, self.stride\n)\n)\nvideos = {output_field: vr.get_batch(indices) for output_field, vr in vrs.items()}\nyield f\"{key}_{starting_index}\", {**videos, \"decoded_indices\": indices}\n</code></pre>"},{"location":"api/ocl/transforms/#ocl.transforms.DecodeRandomStridedWindow","title":"<code>DecodeRandomStridedWindow</code>","text":"<p>         Bases: <code>DecodeRandomWindow</code></p> <p>Decode random strided segment of input video.</p> Source code in <code>ocl/transforms.py</code> <pre><code>class DecodeRandomStridedWindow(DecodeRandomWindow):\n\"\"\"Decode random strided segment of input video.\"\"\"\ndef _chunk_iterator(\nself, vrs: Mapping[str, decord.VideoReader], key: str, inputs: Dict[str, Any]\n) -&gt; Iterable[Tuple[str, Dict]]:\n\"\"\"Iterate over chunks of the video.\n        For the video decoder we simply return a single chunk containing the whole video, subclasses\n        might override this method though.\n        Returns:\n            str: Derived key which combines chunk and video key.\n            torch.Tensor: Chunk of video data.\n            Dict: Additional information, for example which frames where selected.  This might be of\n                relevance when different modalities need to be sliced in a similar fashion as the\n                video input.\n        \"\"\"\nn_frames = len(next(iter(vrs.values())))\nsegment_indices = list(range(0, n_frames + 1, self.n_consecutive_frames * self.stride))\nsegment_index = self.random.randint(0, len(segment_indices) - 2)\nindices = list(\nrange(segment_indices[segment_index], segment_indices[segment_index + 1], self.stride)\n)\nvideos = {output_field: vr.get_batch(indices) for output_field, vr in vrs.items()}\nyield f\"{key}_{segment_index}\", {**videos, \"decoded_indices\": indices}\n</code></pre>"},{"location":"api/ocl/transforms/#ocl.transforms.SpatialSlidingWindow","title":"<code>SpatialSlidingWindow</code>","text":"<p>         Bases: <code>Transform</code></p> <p>Split image data spatially by sliding a window across.</p> Source code in <code>ocl/transforms.py</code> <pre><code>class SpatialSlidingWindow(Transform):\n\"\"\"Split image data spatially by sliding a window across.\"\"\"\ndef __init__(\nself,\nwindow_size: Tuple[int, int],\nstride: Tuple[int, int],\npadding: Tuple[int, int, int, int],\nfields: Sequence[str],\nexpected_n_windows: Optional[int] = None,\n):\nself.window_size = window_size\nself.stride = stride\nself.padding = padding\nself.expected_n_windows = expected_n_windows\nself._fields = tuple(fields)\nself.is_batch_transform = False\n@property\ndef fields(self):\nreturn self._fields\n@staticmethod\ndef pad(elem, padding):\nif elem.shape[-1] != 1 and elem.shape[-1] != 3:\nelem = elem[..., None]\norig_height = elem.shape[-3]\norig_width = elem.shape[-2]\np_left, p_top, p_right, p_bottom = padding\nheight = orig_height + p_top + p_bottom\nwidth = orig_width + p_left + p_right\npadded_shape = list(elem.shape[:-3]) + [height, width, elem.shape[-1]]\nelem_padded = np.zeros_like(elem, shape=padded_shape)\nelem_padded[..., p_top : p_top + orig_height, p_left : p_left + orig_width, :] = elem\nreturn elem_padded\ndef sliding_window(self, data):\nfor sample in data:\nkey = sample[\"__key__\"]\nwindow_x, window_y = self.window_size\nstride_x, stride_y = self.stride\npadded_elems = {key: self.pad(sample[key], self.padding) for key in self._fields}\nn_windows = 0\nx = 0\ny = 0\nwhile True:\nshape = None\nwindowed_fields = {}\nfor key in self._fields:\nelem_padded = padded_elems[key]\nif shape is None:\nshape = elem_padded.shape\nelse:\nif shape[-3:-1] != elem_padded.shape[-3:-1]:\nraise ValueError(\"Element height, width after padding do not match\")\nwindowed_fields[key] = elem_padded[..., y : y + window_y, x : x + window_x, :]\nwindow_height, window_width = windowed_fields[key].shape[-3:-1]\nassert (\nwindow_y == window_height and window_x == window_width\n), f\"Expected {window_y}, {window_x}, received {window_height}, {window_width}\"\nwindowed_fields[\"__key__\"] = f\"{key}_{x - self.padding[0]}_{y - self.padding[1]}\"\nyield {**sample, **windowed_fields}\nn_windows += 1\nx += stride_x\nif x &gt;= shape[-2]:\ny += stride_y\nx = 0\nif y &gt;= shape[-3]:\nbreak\nif self.expected_n_windows is not None and self.expected_n_windows != n_windows:\nraise ValueError(f\"Expected {self.expected_n_windows} windows, but got {n_windows}\")\ndef __call__(self, input_pipe: IterDataPipe):\nreturn input_pipe.then(self.sliding_window)\n</code></pre>"},{"location":"api/ocl/typing/","title":"ocl.typing","text":"<p>Types used in object centric learning framework.</p>"},{"location":"api/ocl/typing/#ocl.typing.ImageData","title":"<code>ImageData = TensorType['batch size', 'channels', 'height', 'width']</code>  <code>module-attribute</code>","text":""},{"location":"api/ocl/typing/#ocl.typing.VideoData","title":"<code>VideoData = TensorType['batch size', 'frames', 'channels', 'height', 'width']</code>  <code>module-attribute</code>","text":""},{"location":"api/ocl/typing/#ocl.typing.ImageOrVideoData","title":"<code>ImageOrVideoData = Union[VideoData, ImageData]</code>  <code>module-attribute</code>","text":""},{"location":"api/ocl/typing/#ocl.typing.TextData","title":"<code>TextData = TensorType['batch_size', 'max_tokens']</code>  <code>module-attribute</code>","text":""},{"location":"api/ocl/typing/#ocl.typing.CNNImageFeatures","title":"<code>CNNImageFeatures = ImageData</code>  <code>module-attribute</code>","text":""},{"location":"api/ocl/typing/#ocl.typing.TransformerImageFeatures","title":"<code>TransformerImageFeatures = TensorType['batch_size', 'n_spatial_features', 'feature_dim']</code>  <code>module-attribute</code>","text":""},{"location":"api/ocl/typing/#ocl.typing.ImageFeatures","title":"<code>ImageFeatures = TransformerImageFeatures</code>  <code>module-attribute</code>","text":""},{"location":"api/ocl/typing/#ocl.typing.VideoFeatures","title":"<code>VideoFeatures = TensorType['batch_size', 'frames', 'n_spatial_features', 'feature_dim']</code>  <code>module-attribute</code>","text":""},{"location":"api/ocl/typing/#ocl.typing.ImageOrVideoFeatures","title":"<code>ImageOrVideoFeatures = Union[ImageFeatures, VideoFeatures]</code>  <code>module-attribute</code>","text":""},{"location":"api/ocl/typing/#ocl.typing.Positions","title":"<code>Positions = TensorType['n_spatial_features', 'spatial_dims']</code>  <code>module-attribute</code>","text":""},{"location":"api/ocl/typing/#ocl.typing.PooledFeatures","title":"<code>PooledFeatures = TensorType['batch_size', 'feature_dim']</code>  <code>module-attribute</code>","text":""},{"location":"api/ocl/typing/#ocl.typing.ObjectFeatures","title":"<code>ObjectFeatures = TensorType['batch_size', 'n_objects', 'object_dim']</code>  <code>module-attribute</code>","text":""},{"location":"api/ocl/typing/#ocl.typing.EmptyIndicator","title":"<code>EmptyIndicator = TensorType['batch_size', 'n_objects']</code>  <code>module-attribute</code>","text":""},{"location":"api/ocl/typing/#ocl.typing.ObjectFeatureAttributions","title":"<code>ObjectFeatureAttributions = TensorType['batch_size', 'n_objects', 'n_spatial_features']</code>  <code>module-attribute</code>","text":""},{"location":"api/ocl/typing/#ocl.typing.ConditioningOutput","title":"<code>ConditioningOutput = TensorType['batch_size', 'n_objects', 'object_dim']</code>  <code>module-attribute</code>","text":"<p>Output of conditioning modules.</p>"},{"location":"api/ocl/typing/#ocl.typing.FrameFeatures","title":"<code>FrameFeatures</code>  <code>dataclass</code>","text":"<p>Features associated with a single frame.</p> Source code in <code>ocl/typing.py</code> <pre><code>@dataclasses.dataclass\nclass FrameFeatures:\n\"\"\"Features associated with a single frame.\"\"\"\nfeatures: ImageFeatures\npositions: Positions\n</code></pre>"},{"location":"api/ocl/typing/#ocl.typing.FrameFeatures.features","title":"<code>features: ImageFeatures</code>  <code>class-attribute</code>","text":""},{"location":"api/ocl/typing/#ocl.typing.FrameFeatures.positions","title":"<code>positions: Positions</code>  <code>class-attribute</code>","text":""},{"location":"api/ocl/typing/#ocl.typing.FeatureExtractorOutput","title":"<code>FeatureExtractorOutput</code>  <code>dataclass</code>","text":"<p>Output of feature extractor.</p> Source code in <code>ocl/typing.py</code> <pre><code>@dataclasses.dataclass\nclass FeatureExtractorOutput:\n\"\"\"Output of feature extractor.\"\"\"\nfeatures: ImageOrVideoFeatures\npositions: Positions\naux_features: Optional[Dict[str, torch.Tensor]] = None\ndef __iter__(self) -&gt; Iterable[ImageFeatures]:\n\"\"\"Iterate over features and positions per frame.\"\"\"\nif self.features.ndim == 4:\nyield FrameFeatures(self.features, self.positions)\nelse:\nfor frame_features in torch.split(self.features, 1, dim=1):\nyield FrameFeatures(frame_features.squeeze(1), self.positions)\n</code></pre>"},{"location":"api/ocl/typing/#ocl.typing.FeatureExtractorOutput.features","title":"<code>features: ImageOrVideoFeatures</code>  <code>class-attribute</code>","text":""},{"location":"api/ocl/typing/#ocl.typing.FeatureExtractorOutput.positions","title":"<code>positions: Positions</code>  <code>class-attribute</code>","text":""},{"location":"api/ocl/typing/#ocl.typing.FeatureExtractorOutput.aux_features","title":"<code>aux_features: Optional[Dict[str, torch.Tensor]] = None</code>  <code>class-attribute</code>","text":""},{"location":"api/ocl/typing/#ocl.typing.FeatureExtractorOutput.__iter__","title":"<code>__iter__</code>","text":"<p>Iterate over features and positions per frame.</p> Source code in <code>ocl/typing.py</code> <pre><code>def __iter__(self) -&gt; Iterable[ImageFeatures]:\n\"\"\"Iterate over features and positions per frame.\"\"\"\nif self.features.ndim == 4:\nyield FrameFeatures(self.features, self.positions)\nelse:\nfor frame_features in torch.split(self.features, 1, dim=1):\nyield FrameFeatures(frame_features.squeeze(1), self.positions)\n</code></pre>"},{"location":"api/ocl/typing/#ocl.typing.PerceptualGroupingOutput","title":"<code>PerceptualGroupingOutput</code>  <code>dataclass</code>","text":"<p>Output of a perceptual grouping algorithm.</p> Source code in <code>ocl/typing.py</code> <pre><code>@dataclasses.dataclass\nclass PerceptualGroupingOutput:\n\"\"\"Output of a perceptual grouping algorithm.\"\"\"\nobjects: ObjectFeatures\nis_empty: Optional[EmptyIndicator] = None  # noqa: F821\nfeature_attributions: Optional[ObjectFeatureAttributions] = None  # noqa: F821\n</code></pre>"},{"location":"api/ocl/typing/#ocl.typing.PerceptualGroupingOutput.objects","title":"<code>objects: ObjectFeatures</code>  <code>class-attribute</code>","text":""},{"location":"api/ocl/typing/#ocl.typing.PerceptualGroupingOutput.is_empty","title":"<code>is_empty: Optional[EmptyIndicator] = None</code>  <code>class-attribute</code>","text":""},{"location":"api/ocl/typing/#ocl.typing.PerceptualGroupingOutput.feature_attributions","title":"<code>feature_attributions: Optional[ObjectFeatureAttributions] = None</code>  <code>class-attribute</code>","text":""},{"location":"api/ocl/visualization_types/","title":"ocl.visualization_types","text":"<p>Classes for handling different types of visualizations.</p>"},{"location":"api/ocl/visualization_types/#ocl.visualization_types.SummaryWriter","title":"<code>SummaryWriter</code>","text":"<p>Placeholder class for SummaryWriter.</p> <p>Emulates interface of <code>torch.utils.tensorboard.SummaryWriter</code>.</p> Source code in <code>ocl/visualization_types.py</code> <pre><code>class SummaryWriter:\n\"\"\"Placeholder class for SummaryWriter.\n    Emulates interface of `torch.utils.tensorboard.SummaryWriter`.\n    \"\"\"\ndef add_figure(self, *args, **kwargs):\npass\ndef add_image(self, *args, **kwargs):\npass\ndef add_images(self, *args, **kwargs):\npass\ndef add_video(self, *args, **kwargs):\npass\ndef add_embedding(self, *args, **kwargs):\npass\n</code></pre>"},{"location":"api/ocl/visualization_types/#ocl.visualization_types.Figure","title":"<code>Figure</code>  <code>dataclass</code>","text":"<p>         Bases: <code>Visualization</code></p> <p>Matplotlib figure.</p> Source code in <code>ocl/visualization_types.py</code> <pre><code>@dataclasses.dataclass\nclass Figure(Visualization):\n\"\"\"Matplotlib figure.\"\"\"\nfigure: matplotlib.pyplot.figure\nclose: bool = True\ndef add_to_experiment(self, experiment: SummaryWriter, tag: str, global_step: int):\nexperiment.add_figure(**dataclass_to_dict(self), tag=tag, global_step=global_step)\n</code></pre>"},{"location":"api/ocl/visualization_types/#ocl.visualization_types.Image","title":"<code>Image</code>  <code>dataclass</code>","text":"<p>         Bases: <code>Visualization</code></p> <p>Single image.</p> Source code in <code>ocl/visualization_types.py</code> <pre><code>@dataclasses.dataclass\nclass Image(Visualization):\n\"\"\"Single image.\"\"\"\nimg_tensor: torch.Tensor\ndataformats: str = \"CHW\"\ndef add_to_experiment(self, experiment: SummaryWriter, tag: str, global_step: int):\nexperiment.add_image(**dataclass_to_dict(self), tag=tag, global_step=global_step)\n</code></pre>"},{"location":"api/ocl/visualization_types/#ocl.visualization_types.Images","title":"<code>Images</code>  <code>dataclass</code>","text":"<p>         Bases: <code>Visualization</code></p> <p>Batch of images.</p> Source code in <code>ocl/visualization_types.py</code> <pre><code>@dataclasses.dataclass\nclass Images(Visualization):\n\"\"\"Batch of images.\"\"\"\nimg_tensor: torch.Tensor\ndataformats: str = \"NCHW\"\ndef add_to_experiment(self, experiment: SummaryWriter, tag: str, global_step: int):\nexperiment.add_images(**dataclass_to_dict(self), tag=tag, global_step=global_step)\n</code></pre>"},{"location":"api/ocl/visualization_types/#ocl.visualization_types.Video","title":"<code>Video</code>  <code>dataclass</code>","text":"<p>         Bases: <code>Visualization</code></p> <p>Batch of videos.</p> Source code in <code>ocl/visualization_types.py</code> <pre><code>@dataclasses.dataclass\nclass Video(Visualization):\n\"\"\"Batch of videos.\"\"\"\nvid_tensor: TensorType[\"batch_size\", \"frames\", \"channels\", \"height\", \"width\"]  # noqa: F821\nfps: Union[int, float] = 4\ndef add_to_experiment(self, experiment: SummaryWriter, tag: str, global_step: int):\nexperiment.add_video(**dataclass_to_dict(self), tag=tag, global_step=global_step)\n</code></pre>"},{"location":"api/ocl/visualization_types/#ocl.visualization_types.Embedding","title":"<code>Embedding</code>","text":"<p>         Bases: <code>Visualization</code></p> <p>Batch of embeddings.</p> Source code in <code>ocl/visualization_types.py</code> <pre><code>class Embedding(Visualization):\n\"\"\"Batch of embeddings.\"\"\"\nmat: TensorType[\"batch_size\", \"feature_dim\"]  # noqa: F821\nmetadata: Optional[List[Any]] = None\nlabel_img: Optional[TensorType[\"batch_size\", \"channels\", \"height\", \"width\"]] = None  # noqa: F821\nmetadata_header: Optional[List[str]] = None\ndef add_to_experiment(self, experiment: SummaryWriter, tag: str, global_step: int):\nexperiment.add_embedding(**dataclass_to_dict(self), tag=tag, global_step=global_step)\n</code></pre>"},{"location":"api/ocl/visualizations/","title":"ocl.visualizations","text":""},{"location":"api/ocl/visualizations/#ocl.visualizations.VisualizationMethod","title":"<code>VisualizationMethod</code>","text":"<p>         Bases: <code>ABC</code></p> <p>Abstract base class of a visualization method.</p> Source code in <code>ocl/visualizations.py</code> <pre><code>class VisualizationMethod(ABC):\n\"\"\"Abstract base class of a visualization method.\"\"\"\n@abstractmethod\ndef __call__(self, *args, **kwargs) -&gt; visualization_types.Visualization:\n\"\"\"Comput visualization output.\n        A visualization method takes some inputs and returns a Visualization.\n        \"\"\"\npass\n</code></pre>"},{"location":"api/ocl/visualizations/#ocl.visualizations.VisualizationMethod.__call__","title":"<code>__call__</code>  <code>abstractmethod</code>","text":"<p>Comput visualization output.</p> <p>A visualization method takes some inputs and returns a Visualization.</p> Source code in <code>ocl/visualizations.py</code> <pre><code>@abstractmethod\ndef __call__(self, *args, **kwargs) -&gt; visualization_types.Visualization:\n\"\"\"Comput visualization output.\n    A visualization method takes some inputs and returns a Visualization.\n    \"\"\"\npass\n</code></pre>"},{"location":"api/ocl/visualizations/#ocl.visualizations.Image","title":"<code>Image</code>","text":"<p>         Bases: <code>VisualizationMethod</code></p> <p>Visualize an image.</p> Source code in <code>ocl/visualizations.py</code> <pre><code>class Image(VisualizationMethod):\n\"\"\"Visualize an image.\"\"\"\ndef __init__(\nself,\nn_instances: int = 8,\nn_row: int = 8,\ndenormalization: Optional[Callable[[torch.Tensor], torch.Tensor]] = None,\nas_grid: bool = True,\n):\n\"\"\"Initialize image visualization.\n        Args:\n            n_instances: Number of instances to visualize\n            n_row: Number of rows when `as_grid=True`\n            denormalization: Function to map from normalized inputs to unnormalized values\n            as_grid: Output a grid of images\n        \"\"\"\nself.n_instances = n_instances\nself.n_row = n_row\nself.denormalization = denormalization if denormalization else _nop\nself.as_grid = as_grid\ndef __call__(\nself, image: torch.Tensor\n) -&gt; Union[visualization_types.Image, visualization_types.Images]:\n\"\"\"Visualize image.\n        Args:\n            image: Tensor to visualize as image\n        Returns:\n            Visualized image or images.\n        \"\"\"\nimage = self.denormalization(image[: self.n_instances].cpu())\nif self.as_grid:\nreturn visualization_types.Image(make_grid(image, nrow=self.n_row))\nelse:\nreturn visualization_types.Images(image)\n</code></pre>"},{"location":"api/ocl/visualizations/#ocl.visualizations.Image.__init__","title":"<code>__init__</code>","text":"<p>Initialize image visualization.</p> <p>Parameters:</p> Name Type Description Default <code>n_instances</code> <code>int</code> <p>Number of instances to visualize</p> <code>8</code> <code>n_row</code> <code>int</code> <p>Number of rows when <code>as_grid=True</code></p> <code>8</code> <code>denormalization</code> <code>Optional[Callable[[torch.Tensor], torch.Tensor]]</code> <p>Function to map from normalized inputs to unnormalized values</p> <code>None</code> <code>as_grid</code> <code>bool</code> <p>Output a grid of images</p> <code>True</code> Source code in <code>ocl/visualizations.py</code> <pre><code>def __init__(\nself,\nn_instances: int = 8,\nn_row: int = 8,\ndenormalization: Optional[Callable[[torch.Tensor], torch.Tensor]] = None,\nas_grid: bool = True,\n):\n\"\"\"Initialize image visualization.\n    Args:\n        n_instances: Number of instances to visualize\n        n_row: Number of rows when `as_grid=True`\n        denormalization: Function to map from normalized inputs to unnormalized values\n        as_grid: Output a grid of images\n    \"\"\"\nself.n_instances = n_instances\nself.n_row = n_row\nself.denormalization = denormalization if denormalization else _nop\nself.as_grid = as_grid\n</code></pre>"},{"location":"api/ocl/visualizations/#ocl.visualizations.Image.__call__","title":"<code>__call__</code>","text":"<p>Visualize image.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>torch.Tensor</code> <p>Tensor to visualize as image</p> required <p>Returns:</p> Type Description <code>Union[visualization_types.Image, visualization_types.Images]</code> <p>Visualized image or images.</p> Source code in <code>ocl/visualizations.py</code> <pre><code>def __call__(\nself, image: torch.Tensor\n) -&gt; Union[visualization_types.Image, visualization_types.Images]:\n\"\"\"Visualize image.\n    Args:\n        image: Tensor to visualize as image\n    Returns:\n        Visualized image or images.\n    \"\"\"\nimage = self.denormalization(image[: self.n_instances].cpu())\nif self.as_grid:\nreturn visualization_types.Image(make_grid(image, nrow=self.n_row))\nelse:\nreturn visualization_types.Images(image)\n</code></pre>"},{"location":"api/ocl/visualizations/#ocl.visualizations.Video","title":"<code>Video</code>","text":"<p>         Bases: <code>VisualizationMethod</code></p> Source code in <code>ocl/visualizations.py</code> <pre><code>class Video(VisualizationMethod):\ndef __init__(\nself,\nn_instances: int = 8,\nn_row: int = 8,\ndenormalization: Optional[Callable[[torch.Tensor], torch.Tensor]] = None,\nas_grid: bool = True,\nfps: int = 10,\n):\n\"\"\"Initialize video visualization.\n        Args:\n            n_instances: Number of instances to visualize\n            n_row: Number of rows when `as_grid=True`\n            denormalization: Function to map from normalized inputs to unnormalized values\n            as_grid: Output a grid of images\n            fps: Frames per second\n        \"\"\"\nself.n_instances = n_instances\nself.n_row = n_row\nself.denormalization = denormalization if denormalization else _nop\nself.as_grid = as_grid\nself.fps = fps\ndef __call__(self, video: torch.Tensor) -&gt; visualization_types.Video:\n\"\"\"Visualize video.\n        Args:\n            video: Tensor to visualize as video\n        Returns:\n            Visualized video.\n        \"\"\"\nvideo = video[: self.n_instances].cpu()\nif self.as_grid:\nvideo = torch.stack(\n[\nmake_grid(self.denormalization(frame.unsqueeze(1)).squeeze(1), nrow=self.n_row)\nfor frame in torch.unbind(video, 1)\n],\ndim=0,\n).unsqueeze(0)\nreturn visualization_types.Video(video, fps=self.fps)\n</code></pre>"},{"location":"api/ocl/visualizations/#ocl.visualizations.Video.__init__","title":"<code>__init__</code>","text":"<p>Initialize video visualization.</p> <p>Parameters:</p> Name Type Description Default <code>n_instances</code> <code>int</code> <p>Number of instances to visualize</p> <code>8</code> <code>n_row</code> <code>int</code> <p>Number of rows when <code>as_grid=True</code></p> <code>8</code> <code>denormalization</code> <code>Optional[Callable[[torch.Tensor], torch.Tensor]]</code> <p>Function to map from normalized inputs to unnormalized values</p> <code>None</code> <code>as_grid</code> <code>bool</code> <p>Output a grid of images</p> <code>True</code> <code>fps</code> <code>int</code> <p>Frames per second</p> <code>10</code> Source code in <code>ocl/visualizations.py</code> <pre><code>def __init__(\nself,\nn_instances: int = 8,\nn_row: int = 8,\ndenormalization: Optional[Callable[[torch.Tensor], torch.Tensor]] = None,\nas_grid: bool = True,\nfps: int = 10,\n):\n\"\"\"Initialize video visualization.\n    Args:\n        n_instances: Number of instances to visualize\n        n_row: Number of rows when `as_grid=True`\n        denormalization: Function to map from normalized inputs to unnormalized values\n        as_grid: Output a grid of images\n        fps: Frames per second\n    \"\"\"\nself.n_instances = n_instances\nself.n_row = n_row\nself.denormalization = denormalization if denormalization else _nop\nself.as_grid = as_grid\nself.fps = fps\n</code></pre>"},{"location":"api/ocl/visualizations/#ocl.visualizations.Video.__call__","title":"<code>__call__</code>","text":"<p>Visualize video.</p> <p>Parameters:</p> Name Type Description Default <code>video</code> <code>torch.Tensor</code> <p>Tensor to visualize as video</p> required <p>Returns:</p> Type Description <code>visualization_types.Video</code> <p>Visualized video.</p> Source code in <code>ocl/visualizations.py</code> <pre><code>def __call__(self, video: torch.Tensor) -&gt; visualization_types.Video:\n\"\"\"Visualize video.\n    Args:\n        video: Tensor to visualize as video\n    Returns:\n        Visualized video.\n    \"\"\"\nvideo = video[: self.n_instances].cpu()\nif self.as_grid:\nvideo = torch.stack(\n[\nmake_grid(self.denormalization(frame.unsqueeze(1)).squeeze(1), nrow=self.n_row)\nfor frame in torch.unbind(video, 1)\n],\ndim=0,\n).unsqueeze(0)\nreturn visualization_types.Video(video, fps=self.fps)\n</code></pre>"},{"location":"api/ocl/visualizations/#ocl.visualizations.Mask","title":"<code>Mask</code>","text":"<p>         Bases: <code>VisualizationMethod</code></p> Source code in <code>ocl/visualizations.py</code> <pre><code>class Mask(VisualizationMethod):\ndef __init__(\nself,\nn_instances: int = 8,\nfps: int = 10,\n):\n\"\"\"Initialize mask visualization.\n        Args:\n            n_instances: Number of masks to visualize\n            fps: Frames per second in the case of video input.\n        \"\"\"\nself.n_instances = n_instances\nself.fps = fps\ndef __call__(\nself, mask: torch.Tensor\n) -&gt; Union[visualization_types.Image, visualization_types.Video]:\n\"\"\"Visualize mask.\n        Args:\n            mask: Tensor to visualize as mask\n        Returns:\n            Visualized mask.\n        \"\"\"\nmasks = mask[: self.n_instances].cpu().contiguous()\nimage_shape = masks.shape[-2:]\nn_objects = masks.shape[-3]\nif masks.dim() == 5:\n# Handling video data.\n# bs x frames x objects x H x W\nmask_vis = masks.transpose(1, 2).contiguous()\nflattened_masks = mask_vis.flatten(0, 1).unsqueeze(2)\n# Draw masks inverted as they are easier to print.\nmask_vis = torch.stack(\n[\nmake_grid(1.0 - masks, nrow=n_objects)\nfor masks in torch.unbind(flattened_masks, 1)\n],\ndim=0,\n)\nmask_vis = mask_vis.unsqueeze(0)\nreturn visualization_types.Video(mask_vis, fps=self.fps)\nelif masks.dim() == 4:\n# Handling image data.\n# bs x objects x H x W\n# Monochrome image with single channel.\nmasks = masks.view(-1, 1, *image_shape)\n# Draw masks inverted as they are easier to print.\nreturn visualization_types.Image(make_grid(1.0 - masks, nrow=n_objects))\nelse:\nraise RuntimeError(\"Unsupported tensor dimensions.\")\n</code></pre>"},{"location":"api/ocl/visualizations/#ocl.visualizations.Mask.__init__","title":"<code>__init__</code>","text":"<p>Initialize mask visualization.</p> <p>Parameters:</p> Name Type Description Default <code>n_instances</code> <code>int</code> <p>Number of masks to visualize</p> <code>8</code> <code>fps</code> <code>int</code> <p>Frames per second in the case of video input.</p> <code>10</code> Source code in <code>ocl/visualizations.py</code> <pre><code>def __init__(\nself,\nn_instances: int = 8,\nfps: int = 10,\n):\n\"\"\"Initialize mask visualization.\n    Args:\n        n_instances: Number of masks to visualize\n        fps: Frames per second in the case of video input.\n    \"\"\"\nself.n_instances = n_instances\nself.fps = fps\n</code></pre>"},{"location":"api/ocl/visualizations/#ocl.visualizations.Mask.__call__","title":"<code>__call__</code>","text":"<p>Visualize mask.</p> <p>Parameters:</p> Name Type Description Default <code>mask</code> <code>torch.Tensor</code> <p>Tensor to visualize as mask</p> required <p>Returns:</p> Type Description <code>Union[visualization_types.Image, visualization_types.Video]</code> <p>Visualized mask.</p> Source code in <code>ocl/visualizations.py</code> <pre><code>def __call__(\nself, mask: torch.Tensor\n) -&gt; Union[visualization_types.Image, visualization_types.Video]:\n\"\"\"Visualize mask.\n    Args:\n        mask: Tensor to visualize as mask\n    Returns:\n        Visualized mask.\n    \"\"\"\nmasks = mask[: self.n_instances].cpu().contiguous()\nimage_shape = masks.shape[-2:]\nn_objects = masks.shape[-3]\nif masks.dim() == 5:\n# Handling video data.\n# bs x frames x objects x H x W\nmask_vis = masks.transpose(1, 2).contiguous()\nflattened_masks = mask_vis.flatten(0, 1).unsqueeze(2)\n# Draw masks inverted as they are easier to print.\nmask_vis = torch.stack(\n[\nmake_grid(1.0 - masks, nrow=n_objects)\nfor masks in torch.unbind(flattened_masks, 1)\n],\ndim=0,\n)\nmask_vis = mask_vis.unsqueeze(0)\nreturn visualization_types.Video(mask_vis, fps=self.fps)\nelif masks.dim() == 4:\n# Handling image data.\n# bs x objects x H x W\n# Monochrome image with single channel.\nmasks = masks.view(-1, 1, *image_shape)\n# Draw masks inverted as they are easier to print.\nreturn visualization_types.Image(make_grid(1.0 - masks, nrow=n_objects))\nelse:\nraise RuntimeError(\"Unsupported tensor dimensions.\")\n</code></pre>"},{"location":"api/ocl/visualizations/#ocl.visualizations.VisualObject","title":"<code>VisualObject</code>","text":"<p>         Bases: <code>VisualizationMethod</code></p> Source code in <code>ocl/visualizations.py</code> <pre><code>class VisualObject(VisualizationMethod):\ndef __init__(\nself,\nn_instances: int = 8,\ndenormalization: Optional[Callable[[torch.Tensor], torch.Tensor]] = None,\nfps: int = 10,\n):\n\"\"\"Initialize VisualObject visualization.\n        Args:\n            n_instances: Number of masks to visualize\n            denormalization: Function to map from normalized inputs to unnormalized values\n            fps: Frames per second in the case of video input.\n        \"\"\"\nself.n_instances = n_instances\nself.denormalization = denormalization if denormalization else _nop\nself.fps = fps\ndef __call__(\nself, object: torch.Tensor, mask: torch.Tensor\n) -&gt; Union[Dict[str, visualization_types.Image], Dict[str, visualization_types.Video]]:\n\"\"\"Visualize a visual object.\n        Args:\n            object: Tensor of objects to visualize\n            mask: Tensor of object masks\n        Returns:\n            Visualized objects as masked images and masks in the keys `reconstruction` and `mask`.\n        \"\"\"\nobjects = object[: self.n_instances].cpu()\nmasks = mask[: self.n_instances].cpu().contiguous()\nimage_shape = objects.shape[-3:]\nn_objects = objects.shape[-4]\nif objects.dim() == 6:\n# Handling video data.\n# bs x frames x objects x C x H x W\n# We need to denormalize prior to constructing the grid, yet the denormalization\n# method assumes video input. We thus convert a frame into a single frame video and\n# remove the additional dimension prior to make_grid.\n# Switch object and frame dimension.\nobject_vis = objects.transpose(1, 2).contiguous()\nmask_vis = masks.transpose(1, 2).contiguous()\nflattened_masks = mask_vis.flatten(0, 1).unsqueeze(2)\nobject_vis = self.denormalization(object_vis.flatten(0, 1))\n# Keep object pixels and apply white background to non-objects parts.\nobject_vis = object_vis * flattened_masks + (1.0 - flattened_masks)\nobject_vis = torch.stack(\n[\nmake_grid(\nobject_vis_frame,\nnrow=n_objects,\n)\nfor object_vis_frame in torch.unbind(object_vis, 1)\n],\ndim=0,\n)\n# Add batch dimension as this is required for video input.\nobject_vis = object_vis.unsqueeze(0)\n# Draw masks inverted as they are easier to print.\nmask_vis = torch.stack(\n[\nmake_grid(1.0 - masks, nrow=n_objects)\nfor masks in torch.unbind(flattened_masks, 1)\n],\ndim=0,\n)\nmask_vis = mask_vis.unsqueeze(0)\nreturn {\n\"reconstruction\": visualization_types.Video(object_vis, fps=self.fps),\n\"mask\": visualization_types.Video(mask_vis, fps=self.fps),\n}\nelif objects.dim() == 5:\n# Handling image data.\n# bs x objects x C x H x W\nobject_reconstructions = self.denormalization(objects.view(-1, *image_shape))\n# Monochrome image with single channel.\nmasks = masks.view(-1, 1, *image_shape[1:])\n# Save object reconstructions as RGBA image. make_grid does not support RGBA input, thus\n# we combine the channels later.  For the masks we need to pad with 1 as we want the\n# borders between images to remain visible (i.e. alpha value of 1.)\nmasks_grid = make_grid(masks, nrow=n_objects, pad_value=1.0)\nobject_grid = make_grid(object_reconstructions, nrow=n_objects)\n# masks_grid expands the image to three channels, which we don't need. Only keep one, and\n# use it as the alpha channel. After make_grid the tensor has the shape C X W x H.\nobject_grid = torch.cat((object_grid, masks_grid[:1]), dim=0)\nreturn {\n\"reconstruction\": visualization_types.Image(object_grid),\n# Draw masks inverted as they are easier to print.\n\"mask\": visualization_types.Image(make_grid(1.0 - masks, nrow=n_objects)),\n}\nelse:\nraise RuntimeError(\"Unsupported tensor dimensions.\")\n</code></pre>"},{"location":"api/ocl/visualizations/#ocl.visualizations.VisualObject.__init__","title":"<code>__init__</code>","text":"<p>Initialize VisualObject visualization.</p> <p>Parameters:</p> Name Type Description Default <code>n_instances</code> <code>int</code> <p>Number of masks to visualize</p> <code>8</code> <code>denormalization</code> <code>Optional[Callable[[torch.Tensor], torch.Tensor]]</code> <p>Function to map from normalized inputs to unnormalized values</p> <code>None</code> <code>fps</code> <code>int</code> <p>Frames per second in the case of video input.</p> <code>10</code> Source code in <code>ocl/visualizations.py</code> <pre><code>def __init__(\nself,\nn_instances: int = 8,\ndenormalization: Optional[Callable[[torch.Tensor], torch.Tensor]] = None,\nfps: int = 10,\n):\n\"\"\"Initialize VisualObject visualization.\n    Args:\n        n_instances: Number of masks to visualize\n        denormalization: Function to map from normalized inputs to unnormalized values\n        fps: Frames per second in the case of video input.\n    \"\"\"\nself.n_instances = n_instances\nself.denormalization = denormalization if denormalization else _nop\nself.fps = fps\n</code></pre>"},{"location":"api/ocl/visualizations/#ocl.visualizations.VisualObject.__call__","title":"<code>__call__</code>","text":"<p>Visualize a visual object.</p> <p>Parameters:</p> Name Type Description Default <code>object</code> <code>torch.Tensor</code> <p>Tensor of objects to visualize</p> required <code>mask</code> <code>torch.Tensor</code> <p>Tensor of object masks</p> required <p>Returns:</p> Type Description <code>Union[Dict[str, visualization_types.Image], Dict[str, visualization_types.Video]]</code> <p>Visualized objects as masked images and masks in the keys <code>reconstruction</code> and <code>mask</code>.</p> Source code in <code>ocl/visualizations.py</code> <pre><code>def __call__(\nself, object: torch.Tensor, mask: torch.Tensor\n) -&gt; Union[Dict[str, visualization_types.Image], Dict[str, visualization_types.Video]]:\n\"\"\"Visualize a visual object.\n    Args:\n        object: Tensor of objects to visualize\n        mask: Tensor of object masks\n    Returns:\n        Visualized objects as masked images and masks in the keys `reconstruction` and `mask`.\n    \"\"\"\nobjects = object[: self.n_instances].cpu()\nmasks = mask[: self.n_instances].cpu().contiguous()\nimage_shape = objects.shape[-3:]\nn_objects = objects.shape[-4]\nif objects.dim() == 6:\n# Handling video data.\n# bs x frames x objects x C x H x W\n# We need to denormalize prior to constructing the grid, yet the denormalization\n# method assumes video input. We thus convert a frame into a single frame video and\n# remove the additional dimension prior to make_grid.\n# Switch object and frame dimension.\nobject_vis = objects.transpose(1, 2).contiguous()\nmask_vis = masks.transpose(1, 2).contiguous()\nflattened_masks = mask_vis.flatten(0, 1).unsqueeze(2)\nobject_vis = self.denormalization(object_vis.flatten(0, 1))\n# Keep object pixels and apply white background to non-objects parts.\nobject_vis = object_vis * flattened_masks + (1.0 - flattened_masks)\nobject_vis = torch.stack(\n[\nmake_grid(\nobject_vis_frame,\nnrow=n_objects,\n)\nfor object_vis_frame in torch.unbind(object_vis, 1)\n],\ndim=0,\n)\n# Add batch dimension as this is required for video input.\nobject_vis = object_vis.unsqueeze(0)\n# Draw masks inverted as they are easier to print.\nmask_vis = torch.stack(\n[\nmake_grid(1.0 - masks, nrow=n_objects)\nfor masks in torch.unbind(flattened_masks, 1)\n],\ndim=0,\n)\nmask_vis = mask_vis.unsqueeze(0)\nreturn {\n\"reconstruction\": visualization_types.Video(object_vis, fps=self.fps),\n\"mask\": visualization_types.Video(mask_vis, fps=self.fps),\n}\nelif objects.dim() == 5:\n# Handling image data.\n# bs x objects x C x H x W\nobject_reconstructions = self.denormalization(objects.view(-1, *image_shape))\n# Monochrome image with single channel.\nmasks = masks.view(-1, 1, *image_shape[1:])\n# Save object reconstructions as RGBA image. make_grid does not support RGBA input, thus\n# we combine the channels later.  For the masks we need to pad with 1 as we want the\n# borders between images to remain visible (i.e. alpha value of 1.)\nmasks_grid = make_grid(masks, nrow=n_objects, pad_value=1.0)\nobject_grid = make_grid(object_reconstructions, nrow=n_objects)\n# masks_grid expands the image to three channels, which we don't need. Only keep one, and\n# use it as the alpha channel. After make_grid the tensor has the shape C X W x H.\nobject_grid = torch.cat((object_grid, masks_grid[:1]), dim=0)\nreturn {\n\"reconstruction\": visualization_types.Image(object_grid),\n# Draw masks inverted as they are easier to print.\n\"mask\": visualization_types.Image(make_grid(1.0 - masks, nrow=n_objects)),\n}\nelse:\nraise RuntimeError(\"Unsupported tensor dimensions.\")\n</code></pre>"},{"location":"api/ocl/visualizations/#ocl.visualizations.Segmentation","title":"<code>Segmentation</code>","text":"<p>         Bases: <code>VisualizationMethod</code></p> <p>Segmentaiton visualization.</p> Source code in <code>ocl/visualizations.py</code> <pre><code>class Segmentation(VisualizationMethod):\n\"\"\"Segmentaiton visualization.\"\"\"\ndef __init__(\nself,\nn_instances: int = 8,\ndenormalization: Optional[Callable[[torch.Tensor], torch.Tensor]] = None,\n):\n\"\"\"Initialize segmentation visualization.\n        Args:\n            n_instances: Number of masks to visualize\n            denormalization: Function to map from normalized inputs to unnormalized values\n        \"\"\"\nself.n_instances = n_instances\nself.denormalization = denormalization if denormalization else _nop\nself._cmap_cache: Dict[int, List[Tuple[int, int, int]]] = {}\ndef _get_cmap(self, num_classes: int) -&gt; List[Tuple[int, int, int]]:\nif num_classes in self._cmap_cache:\nreturn self._cmap_cache[num_classes]\nfrom matplotlib import cm\nif num_classes &lt;= 20:\nmpl_cmap = cm.get_cmap(\"tab20\", num_classes)(range(num_classes))\nelse:\nmpl_cmap = cm.get_cmap(\"turbo\", num_classes)(range(num_classes))\ncmap = [tuple((255 * cl[:3]).astype(int)) for cl in mpl_cmap]\nself._cmap_cache[num_classes] = cmap\nreturn cmap\ndef __call__(\nself, image: torch.Tensor, mask: torch.Tensor\n) -&gt; Optional[visualization_types.Image]:\n\"\"\"Visualize segmentation overlaying original image.\n        Args:\n            image: Image to overlay\n            mask: Masks of individual objects\n        \"\"\"\nimage = image[: self.n_instances].cpu()\nmask = mask[: self.n_instances].cpu().contiguous()\nif image.dim() == 4:  # Only support image data at the moment.\ninput_image = self.denormalization(image)\nn_objects = mask.shape[1]\nmasks_argmax = mask.argmax(dim=1)[:, None]\nclasses = torch.arange(n_objects)[None, :, None, None].to(masks_argmax)\nmasks_one_hot = masks_argmax == classes\ncmap = self._get_cmap(n_objects)\nmasks_on_image = torch.stack(\n[\ndraw_segmentation_masks(\n(255 * img).to(torch.uint8), mask, alpha=0.75, colors=cmap\n)\nfor img, mask in zip(input_image.to(\"cpu\"), masks_one_hot.to(\"cpu\"))\n]\n)\nreturn visualization_types.Image(make_grid(masks_on_image, nrow=8))\nreturn None\n</code></pre>"},{"location":"api/ocl/visualizations/#ocl.visualizations.Segmentation.__init__","title":"<code>__init__</code>","text":"<p>Initialize segmentation visualization.</p> <p>Parameters:</p> Name Type Description Default <code>n_instances</code> <code>int</code> <p>Number of masks to visualize</p> <code>8</code> <code>denormalization</code> <code>Optional[Callable[[torch.Tensor], torch.Tensor]]</code> <p>Function to map from normalized inputs to unnormalized values</p> <code>None</code> Source code in <code>ocl/visualizations.py</code> <pre><code>def __init__(\nself,\nn_instances: int = 8,\ndenormalization: Optional[Callable[[torch.Tensor], torch.Tensor]] = None,\n):\n\"\"\"Initialize segmentation visualization.\n    Args:\n        n_instances: Number of masks to visualize\n        denormalization: Function to map from normalized inputs to unnormalized values\n    \"\"\"\nself.n_instances = n_instances\nself.denormalization = denormalization if denormalization else _nop\nself._cmap_cache: Dict[int, List[Tuple[int, int, int]]] = {}\n</code></pre>"},{"location":"api/ocl/visualizations/#ocl.visualizations.Segmentation.__call__","title":"<code>__call__</code>","text":"<p>Visualize segmentation overlaying original image.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>torch.Tensor</code> <p>Image to overlay</p> required <code>mask</code> <code>torch.Tensor</code> <p>Masks of individual objects</p> required Source code in <code>ocl/visualizations.py</code> <pre><code>def __call__(\nself, image: torch.Tensor, mask: torch.Tensor\n) -&gt; Optional[visualization_types.Image]:\n\"\"\"Visualize segmentation overlaying original image.\n    Args:\n        image: Image to overlay\n        mask: Masks of individual objects\n    \"\"\"\nimage = image[: self.n_instances].cpu()\nmask = mask[: self.n_instances].cpu().contiguous()\nif image.dim() == 4:  # Only support image data at the moment.\ninput_image = self.denormalization(image)\nn_objects = mask.shape[1]\nmasks_argmax = mask.argmax(dim=1)[:, None]\nclasses = torch.arange(n_objects)[None, :, None, None].to(masks_argmax)\nmasks_one_hot = masks_argmax == classes\ncmap = self._get_cmap(n_objects)\nmasks_on_image = torch.stack(\n[\ndraw_segmentation_masks(\n(255 * img).to(torch.uint8), mask, alpha=0.75, colors=cmap\n)\nfor img, mask in zip(input_image.to(\"cpu\"), masks_one_hot.to(\"cpu\"))\n]\n)\nreturn visualization_types.Image(make_grid(masks_on_image, nrow=8))\nreturn None\n</code></pre>"},{"location":"api/ocl/visualizations/#ocl.visualizations.masks_to_bboxes_xyxy","title":"<code>masks_to_bboxes_xyxy</code>","text":"<p>Compute bounding boxes around the provided masks.</p> <p>Adapted from DETR: https://github.com/facebookresearch/detr/blob/main/util/box_ops.py</p> <p>Parameters:</p> Name Type Description Default <code>masks</code> <code>torch.Tensor</code> <p>Tensor of shape (N, H, W), where N is the number of masks, H and W are the spatial dimensions.</p> required <code>empty_value</code> <code>float</code> <p>Value bounding boxes should contain for empty masks.</p> <code>-1.0</code> <p>Returns:</p> Type Description <code>torch.Tensor</code> <p>Tensor of shape (N, 4), containing bounding boxes in (x1, y1, x2, y2) format, where (x1, y1)</p> <code>torch.Tensor</code> <p>is the coordinate of top-left corner and (x2, y2) is the coordinate of the bottom-right</p> <code>torch.Tensor</code> <p>corner (inclusive) in pixel coordinates. If mask is empty, all coordinates contain</p> <code>torch.Tensor</code> <p><code>empty_value</code> instead.</p> Source code in <code>ocl/visualizations.py</code> <pre><code>def masks_to_bboxes_xyxy(masks: torch.Tensor, empty_value: float = -1.0) -&gt; torch.Tensor:\n\"\"\"Compute bounding boxes around the provided masks.\n    Adapted from DETR: https://github.com/facebookresearch/detr/blob/main/util/box_ops.py\n    Args:\n        masks: Tensor of shape (N, H, W), where N is the number of masks, H and W are the spatial\n            dimensions.\n        empty_value: Value bounding boxes should contain for empty masks.\n    Returns:\n        Tensor of shape (N, 4), containing bounding boxes in (x1, y1, x2, y2) format, where (x1, y1)\n        is the coordinate of top-left corner and (x2, y2) is the coordinate of the bottom-right\n        corner (inclusive) in pixel coordinates. If mask is empty, all coordinates contain\n        `empty_value` instead.\n    \"\"\"\nmasks = masks.bool()\nif masks.numel() == 0:\nreturn torch.zeros((0, 4), device=masks.device)\nlarge_value = 1e8\ninv_mask = ~masks\nh, w = masks.shape[-2:]\ny = torch.arange(0, h, dtype=torch.float, device=masks.device)\nx = torch.arange(0, w, dtype=torch.float, device=masks.device)\ny, x = torch.meshgrid(y, x, indexing=\"ij\")\nx_mask = masks * x.unsqueeze(0)\nx_max = x_mask.flatten(1).max(-1)[0]\nx_min = x_mask.masked_fill(inv_mask, large_value).flatten(1).min(-1)[0]\ny_mask = masks * y.unsqueeze(0)\ny_max = y_mask.flatten(1).max(-1)[0]\ny_min = y_mask.masked_fill(inv_mask, large_value).flatten(1).min(-1)[0]\nbboxes = torch.stack((x_min, y_min, x_max, y_max), dim=1)  # x1y1x2y2\nbboxes[x_min == large_value] = empty_value\nreturn bboxes\n</code></pre>"},{"location":"api/ocl/cli/_config/","title":"ocl.cli._config","text":""},{"location":"api/ocl/cli/_config/#ocl.cli._config.ConfigDefinedLambda","title":"<code>ConfigDefinedLambda</code>","text":"<p>Lambda function defined in the config.</p> <p>This allows lambda functions defined in the config to be pickled.</p> Source code in <code>ocl/cli/_config.py</code> <pre><code>class ConfigDefinedLambda:\n\"\"\"Lambda function defined in the config.\n    This allows lambda functions defined in the config to be pickled.\n    \"\"\"\ndef __init__(self, function_string: str):\nself.__setstate__(function_string)\ndef __getstate__(self) -&gt; str:\nreturn self.function_string\ndef __setstate__(self, function_string: str):\nself.function_string = function_string\nself._fn = lambda_string_to_function(function_string)\ndef __call__(self, *args, **kwargs):\nreturn self._fn(*args, **kwargs)\n</code></pre>"},{"location":"api/ocl/cli/_config/#ocl.cli._config.lambda_string_to_function","title":"<code>lambda_string_to_function</code>","text":"<p>Convert string of the form \"lambda x: x\" into a callable Python function.</p> Source code in <code>ocl/cli/_config.py</code> <pre><code>def lambda_string_to_function(function_string: str) -&gt; Callable[..., Any]:\n\"\"\"Convert string of the form \"lambda x: x\" into a callable Python function.\"\"\"\n# This is a bit hacky but ensures that the syntax of the input is correct and contains\n# a valid lambda function definition without requiring to run `eval`.\nparsed = ast.parse(function_string)\nis_lambda = isinstance(parsed.body[0], ast.Expr) and isinstance(parsed.body[0].value, ast.Lambda)\nif not is_lambda:\nraise ValueError(f\"'{function_string}' is not a valid lambda definition.\")\nreturn eval(function_string)\n</code></pre>"},{"location":"api/ocl/cli/_config/#ocl.cli._config.slice_string","title":"<code>slice_string</code>","text":"<p>Split a string according to a split_char and slice.</p> <p>If the output contains more than one element, join these using the split char again.</p> Source code in <code>ocl/cli/_config.py</code> <pre><code>def slice_string(string: str, split_char: str, slice_str: str) -&gt; str:\n\"\"\"Split a string according to a split_char and slice.\n    If the output contains more than one element, join these using the split char again.\n    \"\"\"\nsl = make_slice(slice_str)\nres = string.split(split_char)[sl]\nif isinstance(res, list):\nres = split_char.join(res)\nreturn res\n</code></pre>"},{"location":"api/ocl/cli/cli_utils/","title":"ocl.cli.cli_utils","text":""},{"location":"api/ocl/cli/cli_utils/#ocl.cli.cli_utils.get_commandline_config_path","title":"<code>get_commandline_config_path</code>","text":"<p>Get the path of a config path specified on the command line.</p> Source code in <code>ocl/cli/cli_utils.py</code> <pre><code>def get_commandline_config_path():\n\"\"\"Get the path of a config path specified on the command line.\"\"\"\nhydra_cfg = HydraConfig.get()\nconfig_sources = hydra_cfg.runtime.config_sources\nconfig_path = None\nfor source in config_sources:\nif source.schema == \"file\" and source.provider == \"command-line\":\nconfig_path = source.path\nbreak\nreturn config_path\n</code></pre>"},{"location":"api/ocl/cli/cli_utils/#ocl.cli.cli_utils.find_checkpoint","title":"<code>find_checkpoint</code>","text":"<p>Find checkpoint in output path of previous run.</p> Source code in <code>ocl/cli/cli_utils.py</code> <pre><code>def find_checkpoint(path):\n\"\"\"Find checkpoint in output path of previous run.\"\"\"\ncheckpoints = glob.glob(\nos.path.join(path, \"lightning_logs\", \"version_*\", \"checkpoints\", \"*.ckpt\")\n)\ncheckpoints.sort()\n# Return the last checkpoint.\n# TODO (hornmax): If more than one checkpoint is stored this might not lead to the most recent\n# checkpoint being loaded. Generally, I think this is ok as we still allow people to set the\n# checkpoint manually.\nreturn checkpoints[-1]\n</code></pre>"},{"location":"api/ocl/cli/compute_dataset_size/","title":"ocl.cli.compute_dataset_size","text":"<p>Script to compute the size of a dataset.</p> <p>This is useful when subsampling data using transformations in order to determine the final dataset size.  The size of the dataset is typically need when running distributed training in order to ensure that all nodes and gpu training processes are presented with the same number of batches.</p>"},{"location":"api/ocl/cli/compute_dataset_size/#ocl.cli.compute_dataset_size.ComputeSizeConfig","title":"<code>ComputeSizeConfig</code>  <code>dataclass</code>","text":"<p>Configuration of a training run.</p> Source code in <code>ocl/cli/compute_dataset_size.py</code> <pre><code>@dataclasses.dataclass\nclass ComputeSizeConfig:\n\"\"\"Configuration of a training run.\"\"\"\ndataset: Any\nplugins: Dict[str, Dict] = dataclasses.field(default_factory=dict)\n</code></pre>"},{"location":"api/ocl/cli/dump_movi_dataset/","title":"ocl.cli.dump_movi_dataset","text":"<p>Script to dump MOVi datasets.</p>"},{"location":"api/ocl/cli/dump_movi_dataset/#ocl.cli.dump_movi_dataset.ComputeSizeConfig","title":"<code>ComputeSizeConfig</code>  <code>dataclass</code>","text":"<p>Configuration of a training run.</p> Source code in <code>ocl/cli/dump_movi_dataset.py</code> <pre><code>@dataclasses.dataclass\nclass ComputeSizeConfig:\n\"\"\"Configuration of a training run.\"\"\"\ndataset: Any\nplugins: Dict[str, Any] = dataclasses.field(default_factory=dict)\noutput_dir: str = \"./data\"\n</code></pre>"},{"location":"api/ocl/cli/eval/","title":"ocl.cli.eval","text":"<p>Evaluate a trained slot attention type model.</p>"},{"location":"api/ocl/cli/eval/#ocl.cli.eval.EvaluationConfig","title":"<code>EvaluationConfig</code>  <code>dataclass</code>","text":"<p>Configuration for evaluation.</p> Source code in <code>ocl/cli/eval.py</code> <pre><code>@dataclasses.dataclass\nclass EvaluationConfig:\n\"\"\"Configuration for evaluation.\"\"\"\n# Path to training configuration file or configuration dir. If dir, train_config_name\n# needs to be set as well.\ntrain_config_path: str\ntrain_config_overrides: Optional[List[str]] = None\ntrain_config_name: Optional[str] = None\ncheckpoint_path: Optional[str] = None\noutput_dir: Optional[str] = None\nreport_filename: str = \"metrics.json\"\n# Setting this allows to add modules to the model that are executed during evaluation\nmodules: Optional[Dict[str, Any]] = None\n# Setting this allows to evaluate on a different dataset than the model was trained on\ndataset: Optional[Any] = None\n# Setting this allows to evaluate on different metrics than the model was trained on\nevaluation_metrics: Optional[Dict[str, Any]] = None\nsave_outputs: bool = False\nskip_metrics: bool = False\noutputs_dirname: str = \"outputs\"\noutputs_to_store: Optional[List[str]] = None\nn_samples_to_store: Optional[int] = None\neval_train: bool = False\neval_val: bool = True\neval_test: bool = False\neval_batch_size: Optional[int] = None\n</code></pre>"},{"location":"api/ocl/cli/eval_cluster_metrics/","title":"ocl.cli.eval_cluster_metrics","text":"<p>Evaluate a trained model for object discovery by clustering object representations.</p> <p>Given a set of images, each with a set of ground truth masks and a set of object masks and representations, we perform the following steps:     1) Assign each object a cluster id by clustering the corresponding representations over all     images.     2) Merge object masks with the same cluster id on the same image to form a semantic mask.     3) Compute IoU between masks of predicted clusters and ground truth classes over all images.     4) Assign clusters to classes based on the IoU and a matching strategy.</p>"},{"location":"api/ocl/cli/eval_cluster_metrics/#ocl.cli.eval_cluster_metrics.EvaluationClusteringConfig","title":"<code>EvaluationClusteringConfig</code>  <code>dataclass</code>","text":"<p>Configuration for evaluation.</p> Source code in <code>ocl/cli/eval_cluster_metrics.py</code> <pre><code>@dataclasses.dataclass\nclass EvaluationClusteringConfig:\n\"\"\"Configuration for evaluation.\"\"\"\n# Path to training configuration file or configuration dir. If dir, train_config_name\n# needs to be set as well.\ntrain_config_path: str\n# Number of classes. Note that on COCO, this should be one larger than the maximum class ID that\n# can appear, which does not correspond to the real number of classes.\nn_classes: int\n# Clustering methods to get cluster ID per object by clustering representations\n# This only supports clustering metrics.\nclusterings: Optional[Dict[str, Any]] = None\n# Paths for model outputs to get cluster ID per object\nmodel_clusterings: Optional[Dict[str, str]] = None\ntrain_config_overrides: Optional[List[str]] = None\ntrain_config_name: Optional[str] = None\ncheckpoint_path: Optional[str] = None\noutput_dir: Optional[str] = None\nreport_filename: str = \"clustering_metrics.json\"\nbatch_size: int = 25\nclass_discovery_threshold: float = 0.02\nuse_mask_threshold: bool = False\nmask_threshold: float = 0.5\nignore_background: bool = False\nuse_unmatched_as_background: bool = False\nuse_ignore_masks: bool = False\nn_min_mask_pixels: int = 1  # Minimum number of pixels a mask must occupy to be considered valid\nn_min_max_mask_values: float = 1e-4  # Mask must have at least one value above threshold\n# Type of representation to use for clustering.\nrepresentation_type: RepresentationType = RepresentationType.SLOTS\n# Setting this allows to add modules to the model that are executed during evaluation\nmodules: Optional[Dict[str, Any]] = None\n# Setting this allows to evaluate on a different dataset than the model was trained on\ndataset: Optional[Any] = None\n# Path to slot representations\nslots_path: str = \"perceptual_grouping.objects\"\n# Path to feature representations\nfeatures_path: str = \"feature_extractor.features\"\n# Path to slot masks, image shaped\nmasks_path: str = \"object_decoder.masks_as_image\"\n# Path to slot masks, but flattened to match the size of features\nmasks_flat_path: str = \"object_decoder.masks\"\n# Path to reference masks\ntarget_masks_path: str = \"input.segmentation_mask\"\n# Path to ignore masks\nignore_masks_path: str = \"input.ignore_mask\"\n# Path under which representation to cluster is stored\ncluster_representation_path: str = \"representation\"\n# Path under which empty slot mask is stored\nempty_slots_path: str = \"empty_slots\"\nclass_name_by_category_id: Optional[Dict[int, str]] = None\n</code></pre>"},{"location":"api/ocl/cli/eval_probing_metrics/","title":"ocl.cli.eval_probing_metrics","text":""},{"location":"api/ocl/cli/eval_utils/","title":"ocl.cli.eval_utils","text":""},{"location":"api/ocl/cli/eval_utils/#ocl.cli.eval_utils.ExtractDataFromPredictions","title":"<code>ExtractDataFromPredictions</code>","text":"<p>         Bases: <code>pl.callbacks.Callback</code></p> <p>Callback used for extracting model outputs during validation and prediction.</p> Source code in <code>ocl/cli/eval_utils.py</code> <pre><code>class ExtractDataFromPredictions(pl.callbacks.Callback):\n\"\"\"Callback used for extracting model outputs during validation and prediction.\"\"\"\ndef __init__(\nself,\npaths: List[str],\noutput_paths: Optional[List[str]] = None,\ntransform: Optional[Callable] = None,\nmax_samples: Optional[int] = None,\nflatten_batches: bool = False,\n):\nself.paths = paths\nself.output_paths = output_paths if output_paths is not None else paths\nself.transform = transform\nself.max_samples = max_samples\nself.flatten_batches = flatten_batches\nself.outputs = defaultdict(list)\nself._n_samples = 0\ndef _start(self):\nself._n_samples = 0\nself.outputs = defaultdict(list)\ndef _process_outputs(self, outputs, batch):\nif self.max_samples is not None and self._n_samples &gt;= self.max_samples:\nreturn\ndata = {\"input\": batch, **outputs}\ndata = {path: get_tree_element(outputs, path.split(\".\")) for path in self.paths}\nif self.transform:\ndata = self.transform(data)\nfirst_path = True\nfor path in self.output_paths:\nelems = data[path].detach().cpu()\nif not self.flatten_batches:\nelems = [elems]\nfor idx in range(len(elems)):\nself.outputs[path].append(elems[idx])\nif first_path:\nself._n_samples += 1\nfirst_path = False\ndef on_validation_start(self, trainer, pl_module):\nself._start()\ndef on_validation_batch_end(\nself, trainer, pl_module, outputs, batch, batch_idx, dataloader_idx=0\n):\nassert (\noutputs is not None\n), \"Model returned no outputs. Set `model.return_outputs_on_validation=True`\"\nself._process_outputs(outputs, batch)\ndef on_predict_start(self, trainer, pl_module):\nself._start()\ndef on_predict_batch_end(self, trainer, pl_module, outputs, batch, batch_idx, dataloader_idx=0):\nself._process_outputs(outputs, batch)\ndef get_outputs(self) -&gt; List[Dict[str, Any]]:\nstate = []\nfor idx in range(self._n_samples):\nstate.append({})\nfor key, values in self.outputs.items():\nstate[-1][key] = values[idx]\nreturn state\n</code></pre>"},{"location":"api/ocl/cli/eval_utils/#ocl.cli.eval_utils.save_outputs","title":"<code>save_outputs</code>","text":"<p>Save outputs to disk in numpy or pickle format.</p> Source code in <code>ocl/cli/eval_utils.py</code> <pre><code>def save_outputs(dir_path: str, outputs: List[Dict[str, Any]], verbose: bool = False):\n\"\"\"Save outputs to disk in numpy or pickle format.\"\"\"\ndir_path = pathlib.Path(dir_path)\ndir_path.mkdir(parents=True, exist_ok=True)\ndef get_path(path, prefix, key, extension):\nreturn str(path / f\"{prefix}.{key}.{extension}\")\nidx_fmt = \"{:0\" + str(len(str(len(outputs)))) + \"d}\"  # Get number of total digits\nfor idx, entry in enumerate(outputs):\nidx_prefix = idx_fmt.format(idx)\nfor key, value in entry.items():\nif isinstance(value, torch.Tensor):\nvalue = value.numpy()\nif isinstance(value, numpy.ndarray):\npath = get_path(dir_path, idx_prefix, key, \"npy\")\nif verbose:\nprint(f\"Saving numpy array to {path}.\")\nnumpy.save(path, value)\nelse:\npath = get_path(dir_path, idx_prefix, key, \"pkl\")\nif verbose:\nprint(f\"Saving pickle to {path}.\")\nwith open(path, \"wb\") as f:\npickle.dump(value, f)\n</code></pre>"},{"location":"api/ocl/cli/run_bridging_eval/","title":"ocl.cli.run_bridging_eval","text":"<p>Script to run evaluations for the bridging-the-gap project.</p>"},{"location":"api/ocl/cli/train/","title":"ocl.cli.train","text":"<p>Train a slot attention type model.</p>"},{"location":"api/ocl/cli/train/#ocl.cli.train.TrainingConfig","title":"<code>TrainingConfig</code>  <code>dataclass</code>","text":"<p>Configuration of a training run.</p> <p>For losses, metrics and visualizations it can be of use to utilize the routed module as these are simply provided with a dictionary of all model inputs and outputs.</p> <p>Attributes:</p> Name Type Description <code>dataset</code> <code>Any</code> <p>The pytorch lightning datamodule that will be used for training</p> <code>models</code> <code>Any</code> <p>Either a dictionary of torch.nn.Modules which will be interpreted as a Combined model or a torch.nn.Module itself that accepts a dictionary as input.</p> <code>optimizers</code> <code>Dict[str, Any]</code> <p>Dictionary of functools.partial wrapped optimizers or OptimizationWrapper instances</p> <code>losses</code> <code>Dict[str, Any]</code> <p>Dict of callables that return scalar values which will be summed to compute a total loss.  Typically should contain routed versions of callables.</p> <code>visualizations</code> <code>Dict[str, Any]</code> <p>Dictionary of visualizations.  Typically should contain routed versions of visualizations.</p> <code>trainer</code> <code>TrainerConf</code> <p>Pytorch lightning trainer</p> <code>training_vis_frequency</code> <code>Optional[int]</code> <p>Number of optimization steps between generation and storage of visualizations.</p> <code>training_metrics</code> <code>Optional[Dict[str, Any]]</code> <p>Dictionary of torchmetrics that should be used to log training progress. Typically should contain routed versions of torchmetrics.</p> <code>evaluation_metrics</code> <code>Optional[Dict[str, Any]]</code> <p>Dictionary of torchmetrics that should be used to log progress on evaluation splits of the data.  Typically should contain routed versions of Torchmetrics.</p> <code>load_checkpoint</code> <code>Optional[str]</code> <p>Path to checkpoint file that should be loaded prior to starting training.</p> <code>seed</code> <code>Optional[int]</code> <p>Seed used to ensure reproducability.</p> <code>experiment</code> <code>Dict[str, Any]</code> <p>Dictionary with arbitrary additional information.  Useful when building configurations as it can be used as central point for a single parameter that might influence multiple model components.</p> Source code in <code>ocl/cli/train.py</code> <pre><code>@dataclasses.dataclass\nclass TrainingConfig:\n\"\"\"Configuration of a training run.\n    For losses, metrics and visualizations it can be of use to utilize the\n    [routed][] module as these are simply provided with a dictionary of all\n    model inputs and outputs.\n    Attributes:\n        dataset: The pytorch lightning datamodule that will be used for training\n        models: Either a dictionary of [torch.nn.Module][]s which will be interpreted\n            as a [Combined][ocl.utils.routing.Combined] model or a [torch.nn.Module][] itself\n            that accepts a dictionary as input.\n        optimizers: Dictionary of [functools.partial][] wrapped optimizers or\n            [OptimizationWrapper][ocl.optimization.OptimizationWrapper] instances\n        losses: Dict of callables that return scalar values which will be summed to\n            compute a total loss.  Typically should contain [routed][] versions of callables.\n        visualizations: Dictionary of [visualizations][ocl.visualizations].  Typically\n            should contain [routed][] versions of visualizations.\n        trainer: Pytorch lightning trainer\n        training_vis_frequency: Number of optimization steps between generation and\n            storage of visualizations.\n        training_metrics: Dictionary of torchmetrics that should be used to log training progress.\n            Typically should contain [routed][] versions of torchmetrics.\n        evaluation_metrics: Dictionary of torchmetrics that should be used to log progress on\n            evaluation splits of the data.  Typically should contain [routed][] versions of\n            Torchmetrics.\n        load_checkpoint: Path to checkpoint file that should be loaded prior to starting training.\n        seed: Seed used to ensure reproducability.\n        experiment: Dictionary with arbitrary additional information.  Useful when building\n            configurations as it can be used as central point for a single parameter that might\n            influence multiple model components.\n    \"\"\"\ndataset: Any\nmodels: Any  # When provided with dict wrap in `utils.Combined`, otherwise interpret as model.\noptimizers: Dict[str, Any]\nlosses: Dict[str, Any]\nvisualizations: Dict[str, Any] = dataclasses.field(default_factory=dict)\ntrainer: TrainerConf = dataclasses.field(default_factory=lambda: TrainerConf())\ntraining_vis_frequency: Optional[int] = None\ntraining_metrics: Optional[Dict[str, Any]] = None\nevaluation_metrics: Optional[Dict[str, Any]] = None\nload_checkpoint: Optional[str] = None\nseed: Optional[int] = None\nexperiment: Dict[str, Any] = dataclasses.field(default_factory=lambda: {\"callbacks\": {}})\n</code></pre>"},{"location":"api/ocl/feature_extractors/","title":"ocl.feature_extractors","text":"<p>Implementation of feature extractors that can be used for object centric learning.</p> <p>These are grouped into 3 modules</p> <ul> <li>ocl.feature_extractors.misc Feature extractors implemented in object    centric learning papers</li> <li>ocl.feature_extractors.timm Feature extractors based on timm models</li> <li>ocl.feature_extractors.clip Feature extractors for multi-modal data    using CLIP</li> </ul> <p>Utilities used by all modules are found in ocl.feature_extractors.utils.</p> <p>Important note: In order to use feature extractors in timm and clip this package has to be installed with the <code>timm</code> and/or <code>clip</code> extras (see Installation for further information on installing extras).</p>"},{"location":"api/ocl/feature_extractors/#ocl.feature_extractors.SlotAttentionFeatureExtractor","title":"<code>SlotAttentionFeatureExtractor</code>","text":"<p>         Bases: <code>ImageFeatureExtractor</code></p> <p>CNN-based feature extractor as used in the slot attention paper.</p> <p>Reference: Locatello et al., Object-Centric Learning with Slot Attention, NeurIPS 2020</p> Source code in <code>ocl/feature_extractors/misc.py</code> <pre><code>class SlotAttentionFeatureExtractor(ImageFeatureExtractor):\n\"\"\"CNN-based feature extractor as used in the slot attention paper.\n    Reference: Locatello et al., Object-Centric Learning with Slot Attention, NeurIPS 2020\n    \"\"\"\ndef __init__(self):\nsuper().__init__()\nself.layers = nn.Sequential(\nnn.Conv2d(3, out_channels=64, kernel_size=5, padding=\"same\"),\nnn.ReLU(inplace=True),\nnn.Conv2d(64, out_channels=64, kernel_size=5, padding=\"same\"),\nnn.ReLU(inplace=True),\nnn.Conv2d(64, out_channels=64, kernel_size=5, padding=\"same\"),\nnn.ReLU(inplace=True),\nnn.Conv2d(64, out_channels=64, kernel_size=5, padding=\"same\"),\nnn.ReLU(inplace=True),\n)\n@property\ndef feature_dim(self):\nreturn 64\ndef forward_images(self, images: torch.Tensor):\nfeatures = self.layers(images)\nflattened, positions = cnn_compute_positions_and_flatten(features)\nreturn flattened, positions\n</code></pre>"},{"location":"api/ocl/feature_extractors/#ocl.feature_extractors.SAViFeatureExtractor","title":"<code>SAViFeatureExtractor</code>","text":"<p>         Bases: <code>ImageFeatureExtractor</code></p> <p>CNN-based feature extractor as used in the slot attention for video paper.</p> <p>Reference: Kipf et al., Conditional Object-Centric Learning from Video, ICLR 2020</p> Source code in <code>ocl/feature_extractors/misc.py</code> <pre><code>class SAViFeatureExtractor(ImageFeatureExtractor):\n\"\"\"CNN-based feature extractor as used in the slot attention for video paper.\n    Reference: Kipf et al., Conditional Object-Centric Learning from Video, ICLR 2020\n    \"\"\"\ndef __init__(self, larger_input_arch: bool = False):\n\"\"\"Initialize SAVi feature extractor.\n        Args:\n            larger_input_arch: Use the architecture for larger image datasets such as MOVi++, which\n                contains more a stride in the first layer and a higher number of feature channels in\n                the CNN backbone.\n        \"\"\"\nsuper().__init__()\nself.larger_input_arch = larger_input_arch\nif larger_input_arch:\nself.layers = nn.Sequential(\n# Pytorch does not support stride&gt;1 with padding=same.\n# Implement tensorflow behaviour manually.\n# See: https://discuss.pytorch.org/t/same-padding-equivalent-in-pytorch/85121/4\nnn.ZeroPad2d((1, 2, 1, 2)),\nnn.Conv2d(3, out_channels=64, kernel_size=5, stride=2, padding=\"valid\"),\nnn.ReLU(inplace=True),\nnn.Conv2d(64, out_channels=64, kernel_size=5, padding=\"same\"),\nnn.ReLU(inplace=True),\nnn.Conv2d(64, out_channels=64, kernel_size=5, padding=\"same\"),\nnn.ReLU(inplace=True),\nnn.Conv2d(64, out_channels=64, kernel_size=5, padding=\"same\"),\n)\nelse:\nself.layers = nn.Sequential(\nnn.Conv2d(3, out_channels=32, kernel_size=5, padding=\"same\"),\nnn.ReLU(inplace=True),\nnn.Conv2d(32, out_channels=32, kernel_size=5, padding=\"same\"),\nnn.ReLU(inplace=True),\nnn.Conv2d(32, out_channels=32, kernel_size=5, padding=\"same\"),\nnn.ReLU(inplace=True),\nnn.Conv2d(32, out_channels=32, kernel_size=5, padding=\"same\"),\n)\n@property\ndef feature_dim(self):\nreturn 64 if self.larger_input_arch else 32\ndef forward_images(self, images: torch.Tensor):\nfeatures = self.layers(images)\nflattened, positions = cnn_compute_positions_and_flatten(features)\nreturn flattened, positions\n</code></pre>"},{"location":"api/ocl/feature_extractors/#ocl.feature_extractors.misc.SAViFeatureExtractor.__init__","title":"<code>__init__</code>","text":"<p>Initialize SAVi feature extractor.</p> <p>Parameters:</p> Name Type Description Default <code>larger_input_arch</code> <code>bool</code> <p>Use the architecture for larger image datasets such as MOVi++, which contains more a stride in the first layer and a higher number of feature channels in the CNN backbone.</p> <code>False</code> Source code in <code>ocl/feature_extractors/misc.py</code> <pre><code>def __init__(self, larger_input_arch: bool = False):\n\"\"\"Initialize SAVi feature extractor.\n    Args:\n        larger_input_arch: Use the architecture for larger image datasets such as MOVi++, which\n            contains more a stride in the first layer and a higher number of feature channels in\n            the CNN backbone.\n    \"\"\"\nsuper().__init__()\nself.larger_input_arch = larger_input_arch\nif larger_input_arch:\nself.layers = nn.Sequential(\n# Pytorch does not support stride&gt;1 with padding=same.\n# Implement tensorflow behaviour manually.\n# See: https://discuss.pytorch.org/t/same-padding-equivalent-in-pytorch/85121/4\nnn.ZeroPad2d((1, 2, 1, 2)),\nnn.Conv2d(3, out_channels=64, kernel_size=5, stride=2, padding=\"valid\"),\nnn.ReLU(inplace=True),\nnn.Conv2d(64, out_channels=64, kernel_size=5, padding=\"same\"),\nnn.ReLU(inplace=True),\nnn.Conv2d(64, out_channels=64, kernel_size=5, padding=\"same\"),\nnn.ReLU(inplace=True),\nnn.Conv2d(64, out_channels=64, kernel_size=5, padding=\"same\"),\n)\nelse:\nself.layers = nn.Sequential(\nnn.Conv2d(3, out_channels=32, kernel_size=5, padding=\"same\"),\nnn.ReLU(inplace=True),\nnn.Conv2d(32, out_channels=32, kernel_size=5, padding=\"same\"),\nnn.ReLU(inplace=True),\nnn.Conv2d(32, out_channels=32, kernel_size=5, padding=\"same\"),\nnn.ReLU(inplace=True),\nnn.Conv2d(32, out_channels=32, kernel_size=5, padding=\"same\"),\n)\n</code></pre>"},{"location":"api/ocl/feature_extractors/#ocl.feature_extractors.DVAEFeatureExtractor","title":"<code>DVAEFeatureExtractor</code>","text":"<p>         Bases: <code>ImageFeatureExtractor</code></p> <p>DVAE VQ Encoder as used in SLATE.</p> Reference <p>Singh et al., Simple Unsupervised Object-Centric Learning for Complex and Naturalistic Videos, NeurIPS 2022</p> Source code in <code>ocl/feature_extractors/misc.py</code> <pre><code>class DVAEFeatureExtractor(ImageFeatureExtractor):\n\"\"\"DVAE VQ Encoder as used in SLATE.\n    Reference:\n        Singh et al., Simple Unsupervised Object-Centric Learning for Complex and\n        Naturalistic Videos, NeurIPS 2022\n    \"\"\"\ndef __init__(\nself,\nencoder: nn.Module,\npositional_encoder: nn.Module,\ndictionary: nn.Module,\ntau: float = 1.0,\nhard: bool = False,\n):\n\"\"\"Feature extractor as used in the SLATE paper.\n        Args:\n            encoder: torch Module that transforms image to the patch representations.\n            positional_encoder: torch Module that adds pos encoding.\n            dictionary: map from onehot vectors to embeddings.\n            tau: temporature for gumbel_softmax.\n            hard: hard gumbel_softmax if True.\n        \"\"\"\nsuper().__init__()\nself.tau = tau\nself.hard = hard\nself.dictionary = dictionary\nself.positional_encoder = positional_encoder\nself.encoder = encoder\n@property\ndef feature_dim(self):\nreturn 64\ndef forward_images(self, images: torch.Tensor):\nz_logits = nn.functional.log_softmax(self.encoder(images), dim=1)\n_, _, H_enc, W_enc = z_logits.size()\nz = nn.functional.gumbel_softmax(z_logits, float(self.tau), self.hard, dim=1)\nz_hard = nn.functional.gumbel_softmax(z_logits, float(self.tau), True, dim=1).detach()\n# add beginning of sequence (BOS) token\n# [1, 0, 0, 0, ...] is encoding for BOS token\n# and each sequence starts from such token\nz_hard = z_hard.permute(0, 2, 3, 1).flatten(start_dim=1, end_dim=2)\n# add first zeros column to the z_hard matrix\nz_transformer_input = torch.cat([torch.zeros_like(z_hard[..., :1]), z_hard], dim=-1)\n# add first zeros row to the z_hard matrix\nz_transformer_input = torch.cat(\n[torch.zeros_like(z_transformer_input[..., :1, :]), z_transformer_input], dim=-2\n)\n# fill new row and column with one,\n# so that we added [1, 0, 0, 0, ...] token\nz_transformer_input[:, 0, 0] = 1.0\n# tokens to embeddings\nfeatures = self.dictionary(z_transformer_input)\nfeatures = self.positional_encoder(features)\nslot_attention_features = features[:, 1:]\ntransformer_input = features[:, :-1]\naux_features = {\n\"z\": z,\n\"targets\": transformer_input,\n\"z_hard\": z_hard,\n}\nreturn slot_attention_features, None, aux_features\n</code></pre>"},{"location":"api/ocl/feature_extractors/#ocl.feature_extractors.misc.DVAEFeatureExtractor.__init__","title":"<code>__init__</code>","text":"<p>Feature extractor as used in the SLATE paper.</p> <p>Parameters:</p> Name Type Description Default <code>encoder</code> <code>nn.Module</code> <p>torch Module that transforms image to the patch representations.</p> required <code>positional_encoder</code> <code>nn.Module</code> <p>torch Module that adds pos encoding.</p> required <code>dictionary</code> <code>nn.Module</code> <p>map from onehot vectors to embeddings.</p> required <code>tau</code> <code>float</code> <p>temporature for gumbel_softmax.</p> <code>1.0</code> <code>hard</code> <code>bool</code> <p>hard gumbel_softmax if True.</p> <code>False</code> Source code in <code>ocl/feature_extractors/misc.py</code> <pre><code>def __init__(\nself,\nencoder: nn.Module,\npositional_encoder: nn.Module,\ndictionary: nn.Module,\ntau: float = 1.0,\nhard: bool = False,\n):\n\"\"\"Feature extractor as used in the SLATE paper.\n    Args:\n        encoder: torch Module that transforms image to the patch representations.\n        positional_encoder: torch Module that adds pos encoding.\n        dictionary: map from onehot vectors to embeddings.\n        tau: temporature for gumbel_softmax.\n        hard: hard gumbel_softmax if True.\n    \"\"\"\nsuper().__init__()\nself.tau = tau\nself.hard = hard\nself.dictionary = dictionary\nself.positional_encoder = positional_encoder\nself.encoder = encoder\n</code></pre>"},{"location":"api/ocl/feature_extractors/clip/","title":"ocl.feature_extractors.clip","text":"<p>Module implementing support for pretrained clip models.</p> Reference <p>Radford et al., Learning transferable visual models from natural language supervision, ICML 2021</p>"},{"location":"api/ocl/feature_extractors/clip/#ocl.feature_extractors.clip.ClipImageModel","title":"<code>ClipImageModel</code>","text":"<p>         Bases: <code>ImageFeatureExtractor</code></p> <p>Image part of pretrained clip model.</p> Source code in <code>ocl/feature_extractors/clip.py</code> <pre><code>class ClipImageModel(ImageFeatureExtractor):\n\"\"\"Image part of pretrained clip model.\"\"\"\ndef __init__(\nself,\nmodel_type: str,\nfreeze_model: bool = False,\nreset_weights: bool = False,\nremove_pooling: bool = False,\n):\n\"\"\"Initialize ClipImageModel.\n        Args:\n            model_type: Model type matching `clip.load`.\n            freeze_model: Freeze weights of model.\n            reset_weights: Reset model weights and dont used pretrained ones.\n            remove_pooling: Remove final pooling layer and return features\n                instead of single token.\n        \"\"\"\nsuper().__init__()\nself.freeze_model = freeze_model\nself.clip_vision_model = clip.load(\nmodel_type,\n# Initially force cpu to ensure tensors are float32 (load routine automatically converts\n# to half precision if GPUs are detected).  We can still do half-precision training via\n# pytorch lightning if we want to.\ndevice=\"cpu\",\n)[0].visual\nif self.freeze_model:\nfor parameter in self.clip_vision_model.parameters():\nparameter.requires_grad_(False)\nif reset_weights:\ndef weight_reset(module):\nif hasattr(module, \"reset_parameters\"):\nmodule.reset_parameters()\nself.clip_vision_model.apply(weight_reset)\nself.clip_vision_model.initialize_parameters()\nif remove_pooling:\nif isinstance(self.clip_vision_model, clip.model.VisionTransformer):\nself.get_output = self._get_features_from_vision_transformer\nelse:\nself.get_output = self._get_features_from_resnet\nelse:\nself.get_output = self.clip_vision_model\ndef _get_features_from_vision_transformer(self, x):\n# Commands from:\n# https://github.com/openai/CLIP/blob/d50d76daa670286dd6cacf3bcd80b5e4823fc8e1/clip/model.py#L223\nmodel = self.clip_vision_model\nx = model.conv1(x)  # shape = [*, width, grid, grid]\nx = x.reshape(x.shape[0], x.shape[1], -1)  # shape = [*, width, grid ** 2]\nx = x.permute(0, 2, 1)  # shape = [*, grid ** 2, width]\nx = torch.cat(\n[\nmodel.class_embedding\n+ torch.zeros(x.shape[0], 1, x.shape[-1], dtype=x.dtype, device=x.device),\nx,\n],\ndim=1,\n)  # shape = [*, grid ** 2 + 1, width]\nx = x + model.positional_embedding\nx = model.ln_pre(x)\nx = x.permute(1, 0, 2)  # NLD -&gt; LND\nx = model.transformer(x)\nx = x.permute(1, 0, 2)  # LND -&gt; NLD\nx = self.ln_post(x)\nreturn x, transformer_compute_positions(x)\ndef _get_features_from_resnet(self, x: ocl.typing.ImageData):\n# Commands from:\n# https://github.com/openai/CLIP/blob/d50d76daa670286dd6cacf3bcd80b5e4823fc8e1/clip/model.py#L138\nmodel = self.clip_vision_model\n# Apply \"stem\".\nx = model.relu1(model.bn1(model.conv1(x)))\nx = model.relu2(model.bn2(model.conv2(x)))\nx = model.relu3(model.bn3(model.conv3(x)))\nx = model.avgpool(x)\nx = model.layer1(x)\nx = model.layer2(x)\nx = model.layer3(x)\nx = model.layer4(x)\nreturn cnn_compute_positions_and_flatten(x)\ndef forward_images(\nself, image: ocl.typing.ImageData\n) -&gt; Tuple[ocl.typing.ImageFeatures, ocl.typing.Positions]:\nif self.freeze_model:\nwith torch.no_grad():\nreturn self.get_output(image)\nelse:\nreturn self.get_output(image)\n</code></pre>"},{"location":"api/ocl/feature_extractors/clip/#ocl.feature_extractors.clip.ClipImageModel.__init__","title":"<code>__init__</code>","text":"<p>Initialize ClipImageModel.</p> <p>Parameters:</p> Name Type Description Default <code>model_type</code> <code>str</code> <p>Model type matching <code>clip.load</code>.</p> required <code>freeze_model</code> <code>bool</code> <p>Freeze weights of model.</p> <code>False</code> <code>reset_weights</code> <code>bool</code> <p>Reset model weights and dont used pretrained ones.</p> <code>False</code> <code>remove_pooling</code> <code>bool</code> <p>Remove final pooling layer and return features instead of single token.</p> <code>False</code> Source code in <code>ocl/feature_extractors/clip.py</code> <pre><code>def __init__(\nself,\nmodel_type: str,\nfreeze_model: bool = False,\nreset_weights: bool = False,\nremove_pooling: bool = False,\n):\n\"\"\"Initialize ClipImageModel.\n    Args:\n        model_type: Model type matching `clip.load`.\n        freeze_model: Freeze weights of model.\n        reset_weights: Reset model weights and dont used pretrained ones.\n        remove_pooling: Remove final pooling layer and return features\n            instead of single token.\n    \"\"\"\nsuper().__init__()\nself.freeze_model = freeze_model\nself.clip_vision_model = clip.load(\nmodel_type,\n# Initially force cpu to ensure tensors are float32 (load routine automatically converts\n# to half precision if GPUs are detected).  We can still do half-precision training via\n# pytorch lightning if we want to.\ndevice=\"cpu\",\n)[0].visual\nif self.freeze_model:\nfor parameter in self.clip_vision_model.parameters():\nparameter.requires_grad_(False)\nif reset_weights:\ndef weight_reset(module):\nif hasattr(module, \"reset_parameters\"):\nmodule.reset_parameters()\nself.clip_vision_model.apply(weight_reset)\nself.clip_vision_model.initialize_parameters()\nif remove_pooling:\nif isinstance(self.clip_vision_model, clip.model.VisionTransformer):\nself.get_output = self._get_features_from_vision_transformer\nelse:\nself.get_output = self._get_features_from_resnet\nelse:\nself.get_output = self.clip_vision_model\n</code></pre>"},{"location":"api/ocl/feature_extractors/clip/#ocl.feature_extractors.clip.ClipTextModel","title":"<code>ClipTextModel</code>","text":"<p>         Bases: <code>nn.Module</code></p> <p>Text part of pretrained clip model.</p> Source code in <code>ocl/feature_extractors/clip.py</code> <pre><code>class ClipTextModel(nn.Module):\n\"\"\"Text part of pretrained clip model.\"\"\"\ndef __init__(\nself,\nmodel_type: str,\nfreeze_model: bool = False,\nreset_weights: bool = False,\nremove_pooling: bool = False,\nremove_eot: bool = False,\n):\n\"\"\"Initialize ClipImageModel.\n        Args:\n            model_type: Model type matching `clip.load`.\n            freeze_model: Freeze weights of model.\n            reset_weights: Reset model weights and dont used pretrained ones.\n            remove_pooling: Remove final pooling layer and return features\n                instead of single token.\n            remove_eot: Mask out any that are padding including the eot token.\n        \"\"\"\nsuper().__init__()\nself.freeze_model = freeze_model\nself.remove_pooling = remove_pooling\nclip_model = clip.load(\nmodel_type,\n# Initially force cpu to ensure tensors are float32 (load routine automatically converts\n# to half precision if GPUs are detected).  We can still do half-precision training via\n# pytorch lightning if we want to.\ndevice=\"cpu\",\n)[0]\nif reset_weights:\ndef weight_reset(module):\nif hasattr(module, \"reset_parameters\"):\nmodule.reset_parameters()\nclip_model.apply(weight_reset)\nclip_model.initialize_parameters()\nself.token_embedding = clip_model.token_embedding\nself.positional_embedding = clip_model.positional_embedding\nself.transformer = clip_model.transformer\nself.ln_final = clip_model.ln_final\nself.text_projection = clip_model.text_projection\nif self.freeze_model:\nfor parameter in self.parameters():\nparameter.requires_grad_(False)\nself.remove_pooling = remove_pooling\nself.remove_eot = remove_eot\ndef get_output(self, text):\n# Based on:\n# https://github.com/openai/CLIP/blob/d50d76daa670286dd6cacf3bcd80b5e4823fc8e1/clip/model.py#L343\nx = self.token_embedding(text)  # [batch_size, n_ctx, d_model]\nx = x + self.positional_embedding\nx = x.permute(1, 0, 2)  # NLD -&gt; LND\nx = self.transformer(x)\nx = x.permute(1, 0, 2)  # LND -&gt; NLD\nx = self.ln_final(x)\nif self.remove_pooling:\n# Mask out tokens which are part of the padding.\n# Get position of eot token, it has the highest value of all tokens.\nlengths = text.argmax(dim=-1)\nif self.remove_eot:\n# Also mask out the eot token.\nlengths = lengths - 1\nindices = torch.arange(x.shape[1], device=text.device)\nmask = indices.unsqueeze(0) &gt;= lengths\nx.masked_fill_(mask, 0.0)\nx = x @ self.text_projection\nelse:\n# Do what is done in the standard clip text encoder.\n# x.shape = [batch_size, n_ctx, transformer.width]\n# take features from the eot embedding (eot_token is the highest number in each sequence)\nx = x[torch.arange(x.shape[0]), text.argmax(dim=-1)] @ self.text_projection\nreturn x\ndef forward(self, text: ocl.typing.TextData):\nif self.freeze_model:\nwith torch.no_grad():\nreturn self.get_output(text)\nelse:\nreturn self.get_output(text)\n</code></pre>"},{"location":"api/ocl/feature_extractors/clip/#ocl.feature_extractors.clip.ClipTextModel.__init__","title":"<code>__init__</code>","text":"<p>Initialize ClipImageModel.</p> <p>Parameters:</p> Name Type Description Default <code>model_type</code> <code>str</code> <p>Model type matching <code>clip.load</code>.</p> required <code>freeze_model</code> <code>bool</code> <p>Freeze weights of model.</p> <code>False</code> <code>reset_weights</code> <code>bool</code> <p>Reset model weights and dont used pretrained ones.</p> <code>False</code> <code>remove_pooling</code> <code>bool</code> <p>Remove final pooling layer and return features instead of single token.</p> <code>False</code> <code>remove_eot</code> <code>bool</code> <p>Mask out any that are padding including the eot token.</p> <code>False</code> Source code in <code>ocl/feature_extractors/clip.py</code> <pre><code>def __init__(\nself,\nmodel_type: str,\nfreeze_model: bool = False,\nreset_weights: bool = False,\nremove_pooling: bool = False,\nremove_eot: bool = False,\n):\n\"\"\"Initialize ClipImageModel.\n    Args:\n        model_type: Model type matching `clip.load`.\n        freeze_model: Freeze weights of model.\n        reset_weights: Reset model weights and dont used pretrained ones.\n        remove_pooling: Remove final pooling layer and return features\n            instead of single token.\n        remove_eot: Mask out any that are padding including the eot token.\n    \"\"\"\nsuper().__init__()\nself.freeze_model = freeze_model\nself.remove_pooling = remove_pooling\nclip_model = clip.load(\nmodel_type,\n# Initially force cpu to ensure tensors are float32 (load routine automatically converts\n# to half precision if GPUs are detected).  We can still do half-precision training via\n# pytorch lightning if we want to.\ndevice=\"cpu\",\n)[0]\nif reset_weights:\ndef weight_reset(module):\nif hasattr(module, \"reset_parameters\"):\nmodule.reset_parameters()\nclip_model.apply(weight_reset)\nclip_model.initialize_parameters()\nself.token_embedding = clip_model.token_embedding\nself.positional_embedding = clip_model.positional_embedding\nself.transformer = clip_model.transformer\nself.ln_final = clip_model.ln_final\nself.text_projection = clip_model.text_projection\nif self.freeze_model:\nfor parameter in self.parameters():\nparameter.requires_grad_(False)\nself.remove_pooling = remove_pooling\nself.remove_eot = remove_eot\n</code></pre>"},{"location":"api/ocl/feature_extractors/misc/","title":"ocl.feature_extractors.misc","text":"<p>Feature extractors from diverse papers.</p> <p>In particular this module implements feature extractors from</p> <ul> <li>SlotAttention    Locatello et al., Object-Centric Learning with Slot Attention, NeurIPS 2020</li> <li>SAVi    Kipf et al., Conditional Object-Centric Learning from Video, ICLR 2020</li> <li>SLATE    Singh et al., Simple Unsupervised Object-Centric Learning for Complex and    Naturalistic Videos, NeurIPS 2022</li> </ul>"},{"location":"api/ocl/feature_extractors/misc/#ocl.feature_extractors.misc.SlotAttentionFeatureExtractor","title":"<code>SlotAttentionFeatureExtractor</code>","text":"<p>         Bases: <code>ImageFeatureExtractor</code></p> <p>CNN-based feature extractor as used in the slot attention paper.</p> <p>Reference: Locatello et al., Object-Centric Learning with Slot Attention, NeurIPS 2020</p> Source code in <code>ocl/feature_extractors/misc.py</code> <pre><code>class SlotAttentionFeatureExtractor(ImageFeatureExtractor):\n\"\"\"CNN-based feature extractor as used in the slot attention paper.\n    Reference: Locatello et al., Object-Centric Learning with Slot Attention, NeurIPS 2020\n    \"\"\"\ndef __init__(self):\nsuper().__init__()\nself.layers = nn.Sequential(\nnn.Conv2d(3, out_channels=64, kernel_size=5, padding=\"same\"),\nnn.ReLU(inplace=True),\nnn.Conv2d(64, out_channels=64, kernel_size=5, padding=\"same\"),\nnn.ReLU(inplace=True),\nnn.Conv2d(64, out_channels=64, kernel_size=5, padding=\"same\"),\nnn.ReLU(inplace=True),\nnn.Conv2d(64, out_channels=64, kernel_size=5, padding=\"same\"),\nnn.ReLU(inplace=True),\n)\n@property\ndef feature_dim(self):\nreturn 64\ndef forward_images(self, images: torch.Tensor):\nfeatures = self.layers(images)\nflattened, positions = cnn_compute_positions_and_flatten(features)\nreturn flattened, positions\n</code></pre>"},{"location":"api/ocl/feature_extractors/misc/#ocl.feature_extractors.misc.SAViFeatureExtractor","title":"<code>SAViFeatureExtractor</code>","text":"<p>         Bases: <code>ImageFeatureExtractor</code></p> <p>CNN-based feature extractor as used in the slot attention for video paper.</p> <p>Reference: Kipf et al., Conditional Object-Centric Learning from Video, ICLR 2020</p> Source code in <code>ocl/feature_extractors/misc.py</code> <pre><code>class SAViFeatureExtractor(ImageFeatureExtractor):\n\"\"\"CNN-based feature extractor as used in the slot attention for video paper.\n    Reference: Kipf et al., Conditional Object-Centric Learning from Video, ICLR 2020\n    \"\"\"\ndef __init__(self, larger_input_arch: bool = False):\n\"\"\"Initialize SAVi feature extractor.\n        Args:\n            larger_input_arch: Use the architecture for larger image datasets such as MOVi++, which\n                contains more a stride in the first layer and a higher number of feature channels in\n                the CNN backbone.\n        \"\"\"\nsuper().__init__()\nself.larger_input_arch = larger_input_arch\nif larger_input_arch:\nself.layers = nn.Sequential(\n# Pytorch does not support stride&gt;1 with padding=same.\n# Implement tensorflow behaviour manually.\n# See: https://discuss.pytorch.org/t/same-padding-equivalent-in-pytorch/85121/4\nnn.ZeroPad2d((1, 2, 1, 2)),\nnn.Conv2d(3, out_channels=64, kernel_size=5, stride=2, padding=\"valid\"),\nnn.ReLU(inplace=True),\nnn.Conv2d(64, out_channels=64, kernel_size=5, padding=\"same\"),\nnn.ReLU(inplace=True),\nnn.Conv2d(64, out_channels=64, kernel_size=5, padding=\"same\"),\nnn.ReLU(inplace=True),\nnn.Conv2d(64, out_channels=64, kernel_size=5, padding=\"same\"),\n)\nelse:\nself.layers = nn.Sequential(\nnn.Conv2d(3, out_channels=32, kernel_size=5, padding=\"same\"),\nnn.ReLU(inplace=True),\nnn.Conv2d(32, out_channels=32, kernel_size=5, padding=\"same\"),\nnn.ReLU(inplace=True),\nnn.Conv2d(32, out_channels=32, kernel_size=5, padding=\"same\"),\nnn.ReLU(inplace=True),\nnn.Conv2d(32, out_channels=32, kernel_size=5, padding=\"same\"),\n)\n@property\ndef feature_dim(self):\nreturn 64 if self.larger_input_arch else 32\ndef forward_images(self, images: torch.Tensor):\nfeatures = self.layers(images)\nflattened, positions = cnn_compute_positions_and_flatten(features)\nreturn flattened, positions\n</code></pre>"},{"location":"api/ocl/feature_extractors/misc/#ocl.feature_extractors.misc.SAViFeatureExtractor.__init__","title":"<code>__init__</code>","text":"<p>Initialize SAVi feature extractor.</p> <p>Parameters:</p> Name Type Description Default <code>larger_input_arch</code> <code>bool</code> <p>Use the architecture for larger image datasets such as MOVi++, which contains more a stride in the first layer and a higher number of feature channels in the CNN backbone.</p> <code>False</code> Source code in <code>ocl/feature_extractors/misc.py</code> <pre><code>def __init__(self, larger_input_arch: bool = False):\n\"\"\"Initialize SAVi feature extractor.\n    Args:\n        larger_input_arch: Use the architecture for larger image datasets such as MOVi++, which\n            contains more a stride in the first layer and a higher number of feature channels in\n            the CNN backbone.\n    \"\"\"\nsuper().__init__()\nself.larger_input_arch = larger_input_arch\nif larger_input_arch:\nself.layers = nn.Sequential(\n# Pytorch does not support stride&gt;1 with padding=same.\n# Implement tensorflow behaviour manually.\n# See: https://discuss.pytorch.org/t/same-padding-equivalent-in-pytorch/85121/4\nnn.ZeroPad2d((1, 2, 1, 2)),\nnn.Conv2d(3, out_channels=64, kernel_size=5, stride=2, padding=\"valid\"),\nnn.ReLU(inplace=True),\nnn.Conv2d(64, out_channels=64, kernel_size=5, padding=\"same\"),\nnn.ReLU(inplace=True),\nnn.Conv2d(64, out_channels=64, kernel_size=5, padding=\"same\"),\nnn.ReLU(inplace=True),\nnn.Conv2d(64, out_channels=64, kernel_size=5, padding=\"same\"),\n)\nelse:\nself.layers = nn.Sequential(\nnn.Conv2d(3, out_channels=32, kernel_size=5, padding=\"same\"),\nnn.ReLU(inplace=True),\nnn.Conv2d(32, out_channels=32, kernel_size=5, padding=\"same\"),\nnn.ReLU(inplace=True),\nnn.Conv2d(32, out_channels=32, kernel_size=5, padding=\"same\"),\nnn.ReLU(inplace=True),\nnn.Conv2d(32, out_channels=32, kernel_size=5, padding=\"same\"),\n)\n</code></pre>"},{"location":"api/ocl/feature_extractors/misc/#ocl.feature_extractors.misc.DVAEFeatureExtractor","title":"<code>DVAEFeatureExtractor</code>","text":"<p>         Bases: <code>ImageFeatureExtractor</code></p> <p>DVAE VQ Encoder as used in SLATE.</p> Reference <p>Singh et al., Simple Unsupervised Object-Centric Learning for Complex and Naturalistic Videos, NeurIPS 2022</p> Source code in <code>ocl/feature_extractors/misc.py</code> <pre><code>class DVAEFeatureExtractor(ImageFeatureExtractor):\n\"\"\"DVAE VQ Encoder as used in SLATE.\n    Reference:\n        Singh et al., Simple Unsupervised Object-Centric Learning for Complex and\n        Naturalistic Videos, NeurIPS 2022\n    \"\"\"\ndef __init__(\nself,\nencoder: nn.Module,\npositional_encoder: nn.Module,\ndictionary: nn.Module,\ntau: float = 1.0,\nhard: bool = False,\n):\n\"\"\"Feature extractor as used in the SLATE paper.\n        Args:\n            encoder: torch Module that transforms image to the patch representations.\n            positional_encoder: torch Module that adds pos encoding.\n            dictionary: map from onehot vectors to embeddings.\n            tau: temporature for gumbel_softmax.\n            hard: hard gumbel_softmax if True.\n        \"\"\"\nsuper().__init__()\nself.tau = tau\nself.hard = hard\nself.dictionary = dictionary\nself.positional_encoder = positional_encoder\nself.encoder = encoder\n@property\ndef feature_dim(self):\nreturn 64\ndef forward_images(self, images: torch.Tensor):\nz_logits = nn.functional.log_softmax(self.encoder(images), dim=1)\n_, _, H_enc, W_enc = z_logits.size()\nz = nn.functional.gumbel_softmax(z_logits, float(self.tau), self.hard, dim=1)\nz_hard = nn.functional.gumbel_softmax(z_logits, float(self.tau), True, dim=1).detach()\n# add beginning of sequence (BOS) token\n# [1, 0, 0, 0, ...] is encoding for BOS token\n# and each sequence starts from such token\nz_hard = z_hard.permute(0, 2, 3, 1).flatten(start_dim=1, end_dim=2)\n# add first zeros column to the z_hard matrix\nz_transformer_input = torch.cat([torch.zeros_like(z_hard[..., :1]), z_hard], dim=-1)\n# add first zeros row to the z_hard matrix\nz_transformer_input = torch.cat(\n[torch.zeros_like(z_transformer_input[..., :1, :]), z_transformer_input], dim=-2\n)\n# fill new row and column with one,\n# so that we added [1, 0, 0, 0, ...] token\nz_transformer_input[:, 0, 0] = 1.0\n# tokens to embeddings\nfeatures = self.dictionary(z_transformer_input)\nfeatures = self.positional_encoder(features)\nslot_attention_features = features[:, 1:]\ntransformer_input = features[:, :-1]\naux_features = {\n\"z\": z,\n\"targets\": transformer_input,\n\"z_hard\": z_hard,\n}\nreturn slot_attention_features, None, aux_features\n</code></pre>"},{"location":"api/ocl/feature_extractors/misc/#ocl.feature_extractors.misc.DVAEFeatureExtractor.__init__","title":"<code>__init__</code>","text":"<p>Feature extractor as used in the SLATE paper.</p> <p>Parameters:</p> Name Type Description Default <code>encoder</code> <code>nn.Module</code> <p>torch Module that transforms image to the patch representations.</p> required <code>positional_encoder</code> <code>nn.Module</code> <p>torch Module that adds pos encoding.</p> required <code>dictionary</code> <code>nn.Module</code> <p>map from onehot vectors to embeddings.</p> required <code>tau</code> <code>float</code> <p>temporature for gumbel_softmax.</p> <code>1.0</code> <code>hard</code> <code>bool</code> <p>hard gumbel_softmax if True.</p> <code>False</code> Source code in <code>ocl/feature_extractors/misc.py</code> <pre><code>def __init__(\nself,\nencoder: nn.Module,\npositional_encoder: nn.Module,\ndictionary: nn.Module,\ntau: float = 1.0,\nhard: bool = False,\n):\n\"\"\"Feature extractor as used in the SLATE paper.\n    Args:\n        encoder: torch Module that transforms image to the patch representations.\n        positional_encoder: torch Module that adds pos encoding.\n        dictionary: map from onehot vectors to embeddings.\n        tau: temporature for gumbel_softmax.\n        hard: hard gumbel_softmax if True.\n    \"\"\"\nsuper().__init__()\nself.tau = tau\nself.hard = hard\nself.dictionary = dictionary\nself.positional_encoder = positional_encoder\nself.encoder = encoder\n</code></pre>"},{"location":"api/ocl/feature_extractors/timm/","title":"ocl.feature_extractors.timm","text":"<p>Module implementing support for timm models and some additional models based on timm.</p> <p>The classes here additionally allow the extraction of features at multiple levels for both ViTs and CNNs.</p> Additional models <ul> <li><code>resnet34_savi</code>: ResNet34 as used in SAVi and SAVi++</li> <li><code>resnet50_dino</code>: ResNet50 trained with DINO self-supervision</li> <li><code>vit_small_patch16_224_mocov3</code>: ViT Small trained with MoCo v3 self-supervision</li> <li><code>vit_base_patch16_224_mocov3</code>: ViT Base trained with MoCo v3 self-supervision</li> <li><code>resnet50_mocov3</code>: ViT Base trained with MoCo v3 self-supervision</li> <li><code>vit_small_patch16_224_msn</code>: ViT Small trained with MSN self-supervision</li> <li><code>vit_base_patch16_224_msn</code>: ViT Base trained with MSN self-supervision</li> <li><code>vit_base_patch16_224_mae</code>: ViT Base trained with Masked Autoencoder self-supervision</li> </ul>"},{"location":"api/ocl/feature_extractors/timm/#ocl.feature_extractors.timm.TimmFeatureExtractor","title":"<code>TimmFeatureExtractor</code>","text":"<p>         Bases: <code>ImageFeatureExtractor</code></p> <p>Feature extractor implementation for timm models.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>Name of model. See <code>timm.list_models(\"*\")</code> for available options.</p> required <code>feature_level</code> <code>Optional[Union[int, str, List[Union[int, str]]]]</code> <p>Level of features to return. For CNN-based models, a single integer. For ViT models, either a single or a list of feature descriptors. If a list is passed, multiple levels of features are extracted and concatenated. A ViT feature descriptor consists of the type of feature to extract, followed by an integer indicating the ViT block whose features to use. The type of features can be one of \"block\", \"key\", \"query\", \"value\", specifying that the block's output, attention keys, query or value should be used. If omitted, assumes \"block\" as the type. Example: \"block1\" or [\"block1\", \"value2\"].</p> <code>None</code> <code>aux_features</code> <code>Optional[Union[int, str, List[Union[int, str]]]]</code> <p>Features to store as auxilliary features. The format is the same as in the <code>feature_level</code> argument. Features are stored as a dictionary, using their string representation (e.g. \"block1\") as the key. Only valid for ViT models.</p> <code>None</code> <code>pretrained</code> <code>bool</code> <p>Whether to load pretrained weights.</p> <code>False</code> <code>freeze</code> <code>bool</code> <p>Whether the weights of the feature extractor should be trainable.</p> <code>False</code> <code>n_blocks_to_unfreeze</code> <code>int</code> <p>Number of blocks that should be trainable, beginning from the last block.</p> <code>0</code> <code>unfreeze_attention</code> <code>bool</code> <p>Whether weights of ViT attention layers should be trainable (only valid for ViT models). According to http://arxiv.org/abs/2203.09795, finetuning attention layers only can yield better results in some cases, while being slightly cheaper in terms of computation and memory.</p> <code>False</code> Source code in <code>ocl/feature_extractors/timm.py</code> <pre><code>class TimmFeatureExtractor(ImageFeatureExtractor):\n\"\"\"Feature extractor implementation for timm models.\n    Args:\n        model_name: Name of model. See `timm.list_models(\"*\")` for available options.\n        feature_level: Level of features to return. For CNN-based models, a single integer. For ViT\n            models, either a single or a list of feature descriptors. If a list is passed, multiple\n            levels of features are extracted and concatenated. A ViT feature descriptor consists of\n            the type of feature to extract, followed by an integer indicating the ViT block whose\n            features to use. The type of features can be one of \"block\", \"key\", \"query\", \"value\",\n            specifying that the block's output, attention keys, query or value should be used. If\n            omitted, assumes \"block\" as the type. Example: \"block1\" or [\"block1\", \"value2\"].\n        aux_features: Features to store as auxilliary features. The format is the same as in the\n            `feature_level` argument. Features are stored as a dictionary, using their string\n            representation (e.g. \"block1\") as the key. Only valid for ViT models.\n        pretrained: Whether to load pretrained weights.\n        freeze: Whether the weights of the feature extractor should be trainable.\n        n_blocks_to_unfreeze: Number of blocks that should be trainable, beginning from the last\n            block.\n        unfreeze_attention: Whether weights of ViT attention layers should be trainable (only valid\n            for ViT models). According to http://arxiv.org/abs/2203.09795, finetuning attention\n            layers only can yield better results in some cases, while being slightly cheaper in terms\n            of computation and memory.\n    \"\"\"\ndef __init__(\nself,\nmodel_name: str,\nfeature_level: Optional[Union[int, str, List[Union[int, str]]]] = None,\naux_features: Optional[Union[int, str, List[Union[int, str]]]] = None,\npretrained: bool = False,\nfreeze: bool = False,\nn_blocks_to_unfreeze: int = 0,\nunfreeze_attention: bool = False,\n):\nsuper().__init__()\nself.is_vit = model_name.startswith(\"vit\") or model_name.startswith(\"beit\")\ndef feature_level_to_list(feature_level):\nif feature_level is None:\nreturn []\nelif isinstance(feature_level, (int, str)):\nreturn [feature_level]\nelse:\nreturn list(feature_level)\nself.feature_levels = feature_level_to_list(feature_level)\nself.aux_features = feature_level_to_list(aux_features)\nif self.is_vit:\nmodel = timm.create_model(model_name, pretrained=pretrained)\n# Delete unused parameters from classification head\nif hasattr(model, \"head\"):\ndel model.head\nif hasattr(model, \"fc_norm\"):\ndel model.fc_norm\nif len(self.feature_levels) &gt; 0 or len(self.aux_features) &gt; 0:\nself._feature_hooks = [\n_VitFeatureHook.create_hook_from_feature_level(level).register_with(model)\nfor level in itertools.chain(self.feature_levels, self.aux_features)\n]\nif len(self.feature_levels) &gt; 0:\nfeature_dim = model.num_features * len(self.feature_levels)\n# Remove modules not needed in computation of features\nmax_block = max(hook.block for hook in self._feature_hooks)\nnew_blocks = model.blocks[:max_block]  # Creates a copy\ndel model.blocks\nmodel.blocks = new_blocks\nmodel.norm = nn.Identity()\nelse:\nfeature_dim = model.num_features\nelse:\nself._feature_hooks = None\nfeature_dim = model.num_features\nelse:\nif len(self.feature_levels) == 0:\nraise ValueError(\nf\"Feature extractor {model_name} requires specifying `feature_level`\"\n)\nelif len(self.feature_levels) != 1:\nraise ValueError(\nf\"Feature extractor {model_name} only supports a single `feature_level`\"\n)\nelif not isinstance(self.feature_levels[0], int):\nraise ValueError(\"`feature_level` needs to be an integer\")\nif len(self.aux_features) &gt; 0:\nraise ValueError(\"`aux_features` not supported by feature extractor {model_name}\")\nmodel = timm.create_model(\nmodel_name,\npretrained=pretrained,\nfeatures_only=True,\nout_indices=self.feature_levels,\n)\nfeature_dim = model.feature_info.channels()[0]\nself.model = model\nself.freeze = freeze\nself.n_blocks_to_unfreeze = n_blocks_to_unfreeze\nself._feature_dim = feature_dim\nif freeze:\nself.model.requires_grad_(False)\n# BatchNorm layers update their statistics in train mode. This is probably not desired\n# when the model is supposed to be frozen.\ncontains_bn = any(\nisinstance(m, (nn.BatchNorm1d, nn.BatchNorm2d, nn.BatchNorm3d))\nfor m in self.model.modules()\n)\nself.run_in_eval_mode = contains_bn\nelse:\nself.run_in_eval_mode = False\nif self.n_blocks_to_unfreeze &gt; 0:\nif not self.is_vit:\nraise NotImplementedError(\n\"`unfreeze_n_blocks` option only implemented for ViT models\"\n)\nself.model.blocks[-self.n_blocks_to_unfreeze :].requires_grad_(True)\nif self.model.norm is not None:\nself.model.norm.requires_grad_(True)\nif unfreeze_attention:\nif not self.is_vit:\nraise ValueError(\"`unfreeze_attention` option only works with ViT models\")\nfor module in self.model.modules():\nif isinstance(module, timm.models.vision_transformer.Attention):\nmodule.requires_grad_(True)\n@property\ndef feature_dim(self):\nreturn self._feature_dim\ndef forward_images(self, images: torch.Tensor):\nif self.run_in_eval_mode and self.training:\nself.eval()\nif self.is_vit:\nif self.freeze and self.n_blocks_to_unfreeze == 0:\n# Speed things up a bit by not requiring grad computation.\nwith torch.no_grad():\nfeatures = self.model.forward_features(images)\nelse:\nfeatures = self.model.forward_features(images)\nif self._feature_hooks is not None:\nhook_features = [hook.pop() for hook in self._feature_hooks]\nif len(self.feature_levels) == 0:\n# Remove class token when not using hooks.\nfeatures = features[:, 1:]\npositions = transformer_compute_positions(features)\nelse:\nfeatures = hook_features[: len(self.feature_levels)]\npositions = transformer_compute_positions(features[0])\nfeatures = torch.cat(features, dim=-1)\nif len(self.aux_features) &gt; 0:\naux_hooks = self._feature_hooks[len(self.feature_levels) :]\naux_features = hook_features[len(self.feature_levels) :]\naux_features = {hook.name: feat for hook, feat in zip(aux_hooks, aux_features)}\nelse:\naux_features = None\nelse:\nfeatures = self.model(images)[0]\nfeatures, positions = cnn_compute_positions_and_flatten(features)\naux_features = None\nreturn features, positions, aux_features\n</code></pre>"},{"location":"api/ocl/feature_extractors/timm/#ocl.feature_extractors.timm.resnet34_savi","title":"<code>resnet34_savi</code>","text":"<p>ResNet34 as used in SAVi and SAVi++.</p> <p>As of now, no official code including the ResNet was released, so we can only guess which of the numerous ResNet variants was used. This modifies the basic timm ResNet34 to have 1x1 strides in the stem, and replaces batch norm with group norm. It gives 16x16 feature maps with an input size of 224x224.</p> <p>From SAVi:</p> <p>For the modified SAVi (ResNet) model on MOVi++, we replace the convolutional backbone [...] with a ResNet-34 backbone. We use a modified ResNet root block without strides (i.e. 1\u00d71 stride), resulting in 16\u00d716 feature maps after the backbone [w. 128x128 images]. We further use group normalization throughout the ResNet backbone.</p> <p>From SAVi++:</p> <p>We used a ResNet-34 backbone with modified root convolutional layer that has 1\u00d71 stride. For all layers, we replaced the batch normalization operation by group normalization.</p> Source code in <code>ocl/feature_extractors/timm.py</code> <pre><code>@timm.models.registry.register_model\ndef resnet34_savi(pretrained=False, **kwargs):\n\"\"\"ResNet34 as used in SAVi and SAVi++.\n    As of now, no official code including the ResNet was released, so we can only guess which of\n    the numerous ResNet variants was used. This modifies the basic timm ResNet34 to have 1x1\n    strides in the stem, and replaces batch norm with group norm. It gives 16x16 feature maps with\n    an input size of 224x224.\n    From SAVi:\n    &gt; For the modified SAVi (ResNet) model on MOVi++, we replace the convolutional backbone [...]\n    &gt; with a ResNet-34 backbone. We use a modified ResNet root block without strides\n    &gt; (i.e. 1\u00d71 stride), resulting in 16\u00d716 feature maps after the backbone [w. 128x128 images].\n    &gt; We further use group normalization throughout the ResNet backbone.\n    From SAVi++:\n    &gt; We used a ResNet-34 backbone with modified root convolutional layer that has 1\u00d71 stride.\n    &gt; For all layers, we replaced the batch normalization operation by group normalization.\n    \"\"\"\nif pretrained:\nraise ValueError(\"No pretrained weights available for `savi_resnet34`.\")\nmodel_args = dict(\nblock=resnet.BasicBlock, layers=[3, 4, 6, 3], norm_layer=layers.GroupNorm, **kwargs\n)\nmodel = resnet._create_resnet(\"resnet34\", pretrained=pretrained, **model_args)\nmodel.conv1.stride = (1, 1)\nmodel.maxpool.stride = (1, 1)\nreturn model\n</code></pre>"},{"location":"api/ocl/feature_extractors/utils/","title":"ocl.feature_extractors.utils","text":"<p>Utility functions used for feature extractors.</p>"},{"location":"api/ocl/feature_extractors/utils/#ocl.feature_extractors.utils.FeatureExtractor","title":"<code>FeatureExtractor</code>","text":"<p>         Bases: <code>nn.Module</code></p> <p>Abstract base class for Feature Extractors.</p> <p>We expect that the forward method returns a flattened representation of the features, to make outputs consistent and not dependent on equal spacing or the dimensionality of the spatial information.</p> Source code in <code>ocl/feature_extractors/utils.py</code> <pre><code>class FeatureExtractor(nn.Module, metaclass=abc.ABCMeta):\n\"\"\"Abstract base class for Feature Extractors.\n    We expect that the forward method returns a flattened representation of the features, to make\n    outputs consistent and not dependent on equal spacing or the dimensionality of the spatial\n    information.\n    \"\"\"\n@abc.abstractmethod\ndef forward(self, inputs: ocl.typing.ImageOrVideoFeatures) -&gt; ocl.typing.FeatureExtractorOutput:\npass\n</code></pre>"},{"location":"api/ocl/feature_extractors/utils/#ocl.feature_extractors.utils.ImageFeatureExtractor","title":"<code>ImageFeatureExtractor</code>","text":"<p>         Bases: <code>FeatureExtractor</code></p> <p>Base class that allows operation of image based feature extractors on videos.</p> <p>This is implemented by reshaping the frame dimesion into the batch dimension and inversing the process after extraction of the features.</p> <p>Subclasses override the <code>forward_images</code> method.</p> Source code in <code>ocl/feature_extractors/utils.py</code> <pre><code>class ImageFeatureExtractor(FeatureExtractor):\n\"\"\"Base class that allows operation of image based feature extractors on videos.\n    This is implemented by reshaping the frame dimesion into the batch dimension and\n    inversing the process after extraction of the features.\n    Subclasses override the `forward_images` method.\n    \"\"\"\n@abc.abstractmethod\ndef forward_images(\nself, images: ocl.typing.ImageData\n) -&gt; Union[\nTuple[ocl.typing.ImageFeatures, ocl.typing.Positions],\nTuple[ocl.typing.ImageFeatures, ocl.typing.Positions, Dict],\n]:\n\"\"\"Apply feature extractor to image tensor.\n        Returns:\n            - `torch.Tensor` of extracted features\n            - `torch.Tensor` of spatial positions of extracted features\n            - Optional dict with additional auxilliary features or information\n                from the feature extractor.\n        \"\"\"\ndef forward(self, video: ocl.typing.ImageOrVideoData) -&gt; ocl.typing.FeatureExtractorOutput:\n\"\"\"Apply subclass image feature extractor to potential video data.\n        Args:\n            video: 5D tensor for video data or 4D tensor for image data.\n        Returns:\n            ocl.typing.FeatureExtractorOutput: The extracted features with positiional information\n                and potential auxilliary features.\n        \"\"\"\nndim = video.dim()\nassert ndim == 4 or ndim == 5\nif ndim == 5:\n# Handling video data.\nbs, frames, channels, height, width = video.shape\nimages = video.view(bs * frames, channels, height, width).contiguous()\nelse:\nimages = video\nresult = self.forward_images(images)\nif isinstance(result, (Tuple, List)):\nif len(result) == 2:\nfeatures, positions = result\naux_features = None\nelif len(result) == 3:\nfeatures, positions, aux_features = result\nelse:\nraise RuntimeError(\"Expected either 2 or 3 element tuple from `forward_images`.\")\nelse:\n# Assume output is simply a tensor without positional information.\nreturn ocl.typing.FeatureExtractorOutput(result, None, None)\nif ndim == 5:\nfeatures = features.unflatten(0, (bs, frames))\nif aux_features is not None:\naux_features = {k: f.unflatten(0, (bs, frames)) for k, f in aux_features.items()}\nreturn ocl.typing.FeatureExtractorOutput(features, positions, aux_features)\n</code></pre>"},{"location":"api/ocl/feature_extractors/utils/#ocl.feature_extractors.utils.ImageFeatureExtractor.forward_images","title":"<code>forward_images</code>  <code>abstractmethod</code>","text":"<p>Apply feature extractor to image tensor.</p> <p>Returns:</p> Type Description <code>Union[Tuple[ocl.typing.ImageFeatures, ocl.typing.Positions], Tuple[ocl.typing.ImageFeatures, ocl.typing.Positions, Dict]]</code> <ul> <li><code>torch.Tensor</code> of extracted features</li> </ul> <code>Union[Tuple[ocl.typing.ImageFeatures, ocl.typing.Positions], Tuple[ocl.typing.ImageFeatures, ocl.typing.Positions, Dict]]</code> <ul> <li><code>torch.Tensor</code> of spatial positions of extracted features</li> </ul> <code>Union[Tuple[ocl.typing.ImageFeatures, ocl.typing.Positions], Tuple[ocl.typing.ImageFeatures, ocl.typing.Positions, Dict]]</code> <ul> <li>Optional dict with additional auxilliary features or information from the feature extractor.</li> </ul> Source code in <code>ocl/feature_extractors/utils.py</code> <pre><code>@abc.abstractmethod\ndef forward_images(\nself, images: ocl.typing.ImageData\n) -&gt; Union[\nTuple[ocl.typing.ImageFeatures, ocl.typing.Positions],\nTuple[ocl.typing.ImageFeatures, ocl.typing.Positions, Dict],\n]:\n\"\"\"Apply feature extractor to image tensor.\n    Returns:\n        - `torch.Tensor` of extracted features\n        - `torch.Tensor` of spatial positions of extracted features\n        - Optional dict with additional auxilliary features or information\n            from the feature extractor.\n    \"\"\"\n</code></pre>"},{"location":"api/ocl/feature_extractors/utils/#ocl.feature_extractors.utils.ImageFeatureExtractor.forward","title":"<code>forward</code>","text":"<p>Apply subclass image feature extractor to potential video data.</p> <p>Parameters:</p> Name Type Description Default <code>video</code> <code>ocl.typing.ImageOrVideoData</code> <p>5D tensor for video data or 4D tensor for image data.</p> required <p>Returns:</p> Type Description <code>ocl.typing.FeatureExtractorOutput</code> <p>ocl.typing.FeatureExtractorOutput: The extracted features with positiional information and potential auxilliary features.</p> Source code in <code>ocl/feature_extractors/utils.py</code> <pre><code>def forward(self, video: ocl.typing.ImageOrVideoData) -&gt; ocl.typing.FeatureExtractorOutput:\n\"\"\"Apply subclass image feature extractor to potential video data.\n    Args:\n        video: 5D tensor for video data or 4D tensor for image data.\n    Returns:\n        ocl.typing.FeatureExtractorOutput: The extracted features with positiional information\n            and potential auxilliary features.\n    \"\"\"\nndim = video.dim()\nassert ndim == 4 or ndim == 5\nif ndim == 5:\n# Handling video data.\nbs, frames, channels, height, width = video.shape\nimages = video.view(bs * frames, channels, height, width).contiguous()\nelse:\nimages = video\nresult = self.forward_images(images)\nif isinstance(result, (Tuple, List)):\nif len(result) == 2:\nfeatures, positions = result\naux_features = None\nelif len(result) == 3:\nfeatures, positions, aux_features = result\nelse:\nraise RuntimeError(\"Expected either 2 or 3 element tuple from `forward_images`.\")\nelse:\n# Assume output is simply a tensor without positional information.\nreturn ocl.typing.FeatureExtractorOutput(result, None, None)\nif ndim == 5:\nfeatures = features.unflatten(0, (bs, frames))\nif aux_features is not None:\naux_features = {k: f.unflatten(0, (bs, frames)) for k, f in aux_features.items()}\nreturn ocl.typing.FeatureExtractorOutput(features, positions, aux_features)\n</code></pre>"},{"location":"api/ocl/feature_extractors/utils/#ocl.feature_extractors.utils.cnn_compute_positions_and_flatten","title":"<code>cnn_compute_positions_and_flatten</code>","text":"<p>Flatten CNN features to remove spatial dims and return them with correspoding positions.</p> Source code in <code>ocl/feature_extractors/utils.py</code> <pre><code>def cnn_compute_positions_and_flatten(\nfeatures: ocl.typing.CNNImageFeatures,\n) -&gt; Tuple[ocl.typing.ImageFeatures, ocl.typing.Positions]:\n\"\"\"Flatten CNN features to remove spatial dims and return them with correspoding positions.\"\"\"\nspatial_dims = features.shape[2:]\npositions = torch.cartesian_prod(\n*[torch.linspace(0.0, 1.0, steps=dim, device=features.device) for dim in spatial_dims]\n)\n# reorder into format (batch_size, flattened_spatial_dims, feature_dim).\nflattened = torch.permute(features.view(features.shape[:2] + (-1,)), (0, 2, 1)).contiguous()\nreturn flattened, positions\n</code></pre>"},{"location":"api/ocl/feature_extractors/utils/#ocl.feature_extractors.utils.transformer_compute_positions","title":"<code>transformer_compute_positions</code>","text":"<p>Compute positions for Transformer features.</p> Source code in <code>ocl/feature_extractors/utils.py</code> <pre><code>def transformer_compute_positions(\nfeatures: ocl.typing.TransformerImageFeatures,\n) -&gt; ocl.typing.Positions:\n\"\"\"Compute positions for Transformer features.\"\"\"\nn_tokens = features.shape[1]\nimage_size = math.sqrt(n_tokens)\nimage_size_int = int(image_size)\nassert (\nimage_size_int == image_size\n), \"Position computation for Transformers requires square image\"\nspatial_dims = (image_size_int, image_size_int)\npositions = torch.cartesian_prod(\n*[torch.linspace(0.0, 1.0, steps=dim, device=features.device) for dim in spatial_dims]\n)\nreturn positions\n</code></pre>"},{"location":"api/ocl/metrics/","title":"ocl.metrics","text":"<p>Package for metrics.</p> <p>The implemetation of metrics are grouped into submodules according to their datatype and use</p> <ul> <li>ocl.metrics.bbox: Metrics for bounding boxes</li> <li>ocl.metrics.masks: Metrics for masks</li> <li>ocl.metrics.tracking: Metrics for multiple object tracking.</li> <li>ocl.metrics.diagnosis: Metrics for diagnosing model training.</li> <li>ocl.metrics.dataset: Metrics that are computed on the whole dataset.</li> </ul>"},{"location":"api/ocl/metrics/#ocl.metrics.TensorStatistic","title":"<code>TensorStatistic</code>","text":"<p>         Bases: <code>torchmetrics.Metric</code></p> <p>Metric that computes summary statistic of tensors for logging purposes.</p> <p>First dimension of tensor is assumed to be batch dimension. Other dimensions are reduced to a scalar by the chosen reduction approach (sum or mean).</p> Source code in <code>ocl/metrics/diagnosis.py</code> <pre><code>class TensorStatistic(torchmetrics.Metric):\n\"\"\"Metric that computes summary statistic of tensors for logging purposes.\n    First dimension of tensor is assumed to be batch dimension. Other dimensions are reduced to a\n    scalar by the chosen reduction approach (sum or mean).\n    \"\"\"\ndef __init__(self, reduction: str = \"mean\"):\nsuper().__init__()\nif reduction not in (\"sum\", \"mean\"):\nraise ValueError(f\"Unknown reduction {reduction}\")\nself.reduction = reduction\nself.add_state(\n\"values\", default=torch.tensor(0.0, dtype=torch.float64), dist_reduce_fx=\"sum\"\n)\nself.add_state(\"total\", default=torch.tensor(0), dist_reduce_fx=\"sum\")\ndef update(self, tensor: torch.Tensor):\ntensor = torch.atleast_2d(tensor).flatten(1, -1).to(dtype=torch.float64)\nif self.reduction == \"mean\":\ntensor = torch.mean(tensor, dim=1)\nelif self.reduction == \"sum\":\ntensor = torch.sum(tensor, dim=1)\nself.values += tensor.sum()\nself.total += len(tensor)\ndef compute(self) -&gt; torch.Tensor:\nreturn self.values / self.total\n</code></pre>"},{"location":"api/ocl/metrics/#ocl.metrics.UnsupervisedBboxIoUMetric","title":"<code>UnsupervisedBboxIoUMetric</code>","text":"<p>         Bases: <code>torchmetrics.Metric</code></p> <p>Computes IoU metric for bounding boxes when correspondences to ground truth are not known.</p> <p>Currently, assumes segmentation masks as input for both prediction and targets.</p> <p>Parameters:</p> Name Type Description Default <code>target_is_mask</code> <code>bool</code> <p>If <code>True</code>, assume input is a segmentation mask, in which case the masks are converted to bounding boxes before computing IoU. If <code>False</code>, assume the input for the targets are already bounding boxes.</p> <code>False</code> <code>use_threshold</code> <code>bool</code> <p>If <code>True</code>, convert predicted class probabilities to mask using a threshold. If <code>False</code>, class probabilities are turned into mask using a softmax instead.</p> <code>False</code> <code>threshold</code> <code>float</code> <p>Value to use for thresholding masks.</p> <code>0.5</code> <code>matching</code> <code>str</code> <p>How to match predicted boxes to ground truth boxes. For \"hungarian\", computes assignment that maximizes total IoU between all boxes. For \"best_overlap\", uses the predicted box with maximum overlap for each ground truth box (each predicted box can be assigned to multiple ground truth boxes).</p> <code>'hungarian'</code> <code>compute_discovery_fraction</code> <code>bool</code> <p>Instead of the IoU, compute the fraction of ground truth classes that were \"discovered\", meaning that they have an IoU greater than some threshold. This is recall, or sometimes called the detection rate metric.</p> <code>False</code> <code>correct_localization</code> <code>bool</code> <p>Instead of the IoU, compute the fraction of images on which at least one ground truth bounding box was correctly localised, meaning that they have an IoU greater than some threshold.</p> <code>False</code> <code>discovery_threshold</code> <code>float</code> <p>Minimum IoU to count a class as discovered/correctly localized.</p> <code>0.5</code> Source code in <code>ocl/metrics/bbox.py</code> <pre><code>class UnsupervisedBboxIoUMetric(torchmetrics.Metric):\n\"\"\"Computes IoU metric for bounding boxes when correspondences to ground truth are not known.\n    Currently, assumes segmentation masks as input for both prediction and targets.\n    Args:\n        target_is_mask: If `True`, assume input is a segmentation mask, in which case the masks are\n            converted to bounding boxes before computing IoU. If `False`, assume the input for the\n            targets are already bounding boxes.\n        use_threshold: If `True`, convert predicted class probabilities to mask using a threshold.\n            If `False`, class probabilities are turned into mask using a softmax instead.\n        threshold: Value to use for thresholding masks.\n        matching: How to match predicted boxes to ground truth boxes. For \"hungarian\", computes\n            assignment that maximizes total IoU between all boxes. For \"best_overlap\", uses the\n            predicted box with maximum overlap for each ground truth box (each predicted box\n            can be assigned to multiple ground truth boxes).\n        compute_discovery_fraction: Instead of the IoU, compute the fraction of ground truth classes\n            that were \"discovered\", meaning that they have an IoU greater than some threshold. This\n            is recall, or sometimes called the detection rate metric.\n        correct_localization: Instead of the IoU, compute the fraction of images on which at least\n            one ground truth bounding box was correctly localised, meaning that they have an IoU\n            greater than some threshold.\n        discovery_threshold: Minimum IoU to count a class as discovered/correctly localized.\n    \"\"\"\ndef __init__(\nself,\ntarget_is_mask: bool = False,\nuse_threshold: bool = False,\nthreshold: float = 0.5,\nmatching: str = \"hungarian\",\ncompute_discovery_fraction: bool = False,\ncorrect_localization: bool = False,\ndiscovery_threshold: float = 0.5,\n):\nsuper().__init__()\nself.target_is_mask = target_is_mask\nself.use_threshold = use_threshold\nself.threshold = threshold\nself.discovery_threshold = discovery_threshold\nself.compute_discovery_fraction = compute_discovery_fraction\nself.correct_localization = correct_localization\nif compute_discovery_fraction and correct_localization:\nraise ValueError(\n\"Only one of `compute_discovery_fraction` and `correct_localization` can be enabled.\"\n)\nmatchings = (\"hungarian\", \"best_overlap\")\nif matching not in matchings:\nraise ValueError(f\"Unknown matching type {matching}. Valid values are {matchings}.\")\nself.matching = matching\nself.add_state(\n\"values\", default=torch.tensor(0.0, dtype=torch.float64), dist_reduce_fx=\"sum\"\n)\nself.add_state(\"total\", default=torch.tensor(0), dist_reduce_fx=\"sum\")\ndef update(self, prediction: torch.Tensor, target: torch.Tensor):\n\"\"\"Update this metric.\n        Args:\n            prediction: Predicted mask of shape (B, C, H, W) or (B, F, C, H, W), where C is the\n                number of instances. Assumes class probabilities as inputs.\n            target: Ground truth mask of shape (B, K, H, W) or (B, F, K, H, W), where K is the\n                number of instance, if using masks as input, or bounding boxes of shape (B, K, 4)\n                or (B, F, K, 4).\n        \"\"\"\nif prediction.ndim == 5:\n# Merge batch and frame dimensions\nprediction = prediction.flatten(0, 1)\ntarget = target.flatten(0, 1)\nelif prediction.ndim != 4:\nraise ValueError(f\"Incorrect input shape: f{prediction.shape}\")\nbs, n_pred_classes = prediction.shape[:2]\nn_gt_classes = target.shape[1]\nif self.use_threshold:\nprediction = prediction &gt; self.threshold\nelse:\nindices = torch.argmax(prediction, dim=1)\nprediction = torch.nn.functional.one_hot(indices, num_classes=n_pred_classes)\nprediction = prediction.permute(0, 3, 1, 2)\npred_bboxes = masks_to_bboxes(prediction.flatten(0, 1)).unflatten(0, (bs, n_pred_classes))\nif self.target_is_mask:\ntarget_bboxes = masks_to_bboxes(target.flatten(0, 1)).unflatten(0, (bs, n_gt_classes))\nelse:\nassert target.shape[-1] == 4\n# Convert all-zero boxes added during padding to invalid boxes\ntarget[torch.all(target == 0.0, dim=-1)] = -1.0\ntarget_bboxes = target\nfor pred, target in zip(pred_bboxes, target_bboxes):\nvalid_pred_bboxes = pred[:, 0] != -1.0\nvalid_target_bboxes = target[:, 0] != -1.0\nif valid_target_bboxes.sum() == 0:\ncontinue  # Skip data points without any target bbox\npred = pred[valid_pred_bboxes]\ntarget = target[valid_target_bboxes]\nif valid_pred_bboxes.sum() &gt; 0:\niou_per_bbox = unsupervised_bbox_iou(\npred, target, matching=self.matching, reduction=\"none\"\n)\nelse:\niou_per_bbox = torch.zeros_like(valid_target_bboxes, dtype=torch.float32)\nif self.compute_discovery_fraction:\ndiscovered = iou_per_bbox &gt; self.discovery_threshold\nself.values += discovered.sum() / len(iou_per_bbox)\nelif self.correct_localization:\ncorrectly_localized = torch.any(iou_per_bbox &gt; self.discovery_threshold)\nself.values += correctly_localized.sum()\nelse:\nself.values += iou_per_bbox.mean()\nself.total += 1\ndef compute(self) -&gt; torch.Tensor:\nif self.total == 0:\nreturn torch.zeros_like(self.values)\nelse:\nreturn self.values / self.total\n</code></pre>"},{"location":"api/ocl/metrics/#ocl.metrics.bbox.UnsupervisedBboxIoUMetric.update","title":"<code>update</code>","text":"<p>Update this metric.</p> <p>Parameters:</p> Name Type Description Default <code>prediction</code> <code>torch.Tensor</code> <p>Predicted mask of shape (B, C, H, W) or (B, F, C, H, W), where C is the number of instances. Assumes class probabilities as inputs.</p> required <code>target</code> <code>torch.Tensor</code> <p>Ground truth mask of shape (B, K, H, W) or (B, F, K, H, W), where K is the number of instance, if using masks as input, or bounding boxes of shape (B, K, 4) or (B, F, K, 4).</p> required Source code in <code>ocl/metrics/bbox.py</code> <pre><code>def update(self, prediction: torch.Tensor, target: torch.Tensor):\n\"\"\"Update this metric.\n    Args:\n        prediction: Predicted mask of shape (B, C, H, W) or (B, F, C, H, W), where C is the\n            number of instances. Assumes class probabilities as inputs.\n        target: Ground truth mask of shape (B, K, H, W) or (B, F, K, H, W), where K is the\n            number of instance, if using masks as input, or bounding boxes of shape (B, K, 4)\n            or (B, F, K, 4).\n    \"\"\"\nif prediction.ndim == 5:\n# Merge batch and frame dimensions\nprediction = prediction.flatten(0, 1)\ntarget = target.flatten(0, 1)\nelif prediction.ndim != 4:\nraise ValueError(f\"Incorrect input shape: f{prediction.shape}\")\nbs, n_pred_classes = prediction.shape[:2]\nn_gt_classes = target.shape[1]\nif self.use_threshold:\nprediction = prediction &gt; self.threshold\nelse:\nindices = torch.argmax(prediction, dim=1)\nprediction = torch.nn.functional.one_hot(indices, num_classes=n_pred_classes)\nprediction = prediction.permute(0, 3, 1, 2)\npred_bboxes = masks_to_bboxes(prediction.flatten(0, 1)).unflatten(0, (bs, n_pred_classes))\nif self.target_is_mask:\ntarget_bboxes = masks_to_bboxes(target.flatten(0, 1)).unflatten(0, (bs, n_gt_classes))\nelse:\nassert target.shape[-1] == 4\n# Convert all-zero boxes added during padding to invalid boxes\ntarget[torch.all(target == 0.0, dim=-1)] = -1.0\ntarget_bboxes = target\nfor pred, target in zip(pred_bboxes, target_bboxes):\nvalid_pred_bboxes = pred[:, 0] != -1.0\nvalid_target_bboxes = target[:, 0] != -1.0\nif valid_target_bboxes.sum() == 0:\ncontinue  # Skip data points without any target bbox\npred = pred[valid_pred_bboxes]\ntarget = target[valid_target_bboxes]\nif valid_pred_bboxes.sum() &gt; 0:\niou_per_bbox = unsupervised_bbox_iou(\npred, target, matching=self.matching, reduction=\"none\"\n)\nelse:\niou_per_bbox = torch.zeros_like(valid_target_bboxes, dtype=torch.float32)\nif self.compute_discovery_fraction:\ndiscovered = iou_per_bbox &gt; self.discovery_threshold\nself.values += discovered.sum() / len(iou_per_bbox)\nelif self.correct_localization:\ncorrectly_localized = torch.any(iou_per_bbox &gt; self.discovery_threshold)\nself.values += correctly_localized.sum()\nelse:\nself.values += iou_per_bbox.mean()\nself.total += 1\n</code></pre>"},{"location":"api/ocl/metrics/#ocl.metrics.DatasetSemanticMaskIoUMetric","title":"<code>DatasetSemanticMaskIoUMetric</code>","text":"<p>         Bases: <code>torchmetrics.Metric</code></p> <p>Unsupervised IoU metric for semantic segmentation using dataset-wide matching of classes.</p> <p>The input to this metric is an instance-level mask with objects, and a class id for each object. This is required to convert the mask to semantic classes. The number of classes for the predictions does not have to match the true number of classes.</p> <p>Note that contrary to the other metrics in this module, this metric is not supposed to be added in the online metric computation loop, which is why it does not inherit from <code>RoutableMixin</code>.</p> <p>Parameters:</p> Name Type Description Default <code>n_predicted_classes</code> <code>int</code> <p>Number of predictable classes, i.e. highest prediction class id that can occur.</p> required <code>n_classes</code> <code>int</code> <p>Total number of classes, i.e. highest class id that can occur.</p> required <code>threshold</code> <code>float</code> <p>Value to use for thresholding masks.</p> <code>0.5</code> <code>use_threshold</code> <code>bool</code> <p>If <code>True</code>, convert predicted class probabilities to mask using a threshold. If <code>False</code>, class probabilities are turned into mask using an argmax instead.</p> <code>False</code> <code>matching</code> <code>str</code> <p>Method to produce matching between clusters and ground truth classes. If \"hungarian\", assigns each class one cluster such that the total IoU is maximized. If \"majority\", assigns each cluster to the class with the highest IoU (each class can be assigned multiple clusters).</p> <code>'hungarian'</code> <code>ignore_background</code> <code>bool</code> <p>If true, pixels labeled as background (class zero) in the ground truth are not taken into account when computing IoU.</p> <code>False</code> <code>use_unmatched_as_background</code> <code>bool</code> <p>If true, count predicted classes not selected after Hungarian matching as the background predictions.</p> <code>False</code> Source code in <code>ocl/metrics/dataset.py</code> <pre><code>class DatasetSemanticMaskIoUMetric(torchmetrics.Metric):\n\"\"\"Unsupervised IoU metric for semantic segmentation using dataset-wide matching of classes.\n    The input to this metric is an instance-level mask with objects, and a class id for each object.\n    This is required to convert the mask to semantic classes. The number of classes for the\n    predictions does not have to match the true number of classes.\n    Note that contrary to the other metrics in this module, this metric is not supposed to be added\n    in the online metric computation loop, which is why it does not inherit from `RoutableMixin`.\n    Args:\n        n_predicted_classes: Number of predictable classes, i.e. highest prediction class id that can\n            occur.\n        n_classes: Total number of classes, i.e. highest class id that can occur.\n        threshold: Value to use for thresholding masks.\n        use_threshold: If `True`, convert predicted class probabilities to mask using a threshold.\n            If `False`, class probabilities are turned into mask using an argmax instead.\n        matching: Method to produce matching between clusters and ground truth classes. If\n            \"hungarian\", assigns each class one cluster such that the total IoU is maximized. If\n            \"majority\", assigns each cluster to the class with the highest IoU (each class can be\n            assigned multiple clusters).\n        ignore_background: If true, pixels labeled as background (class zero) in the ground truth\n            are not taken into account when computing IoU.\n        use_unmatched_as_background: If true, count predicted classes not selected after Hungarian\n            matching as the background predictions.\n    \"\"\"\ndef __init__(\nself,\nn_predicted_classes: int,\nn_classes: int,\nuse_threshold: bool = False,\nthreshold: float = 0.5,\nmatching: str = \"hungarian\",\nignore_background: bool = False,\nuse_unmatched_as_background: bool = False,\n):\nsuper().__init__()\nmatching_methods = {\"hungarian\", \"majority\"}\nif matching not in matching_methods:\nraise ValueError(\nf\"Unknown matching method {matching}. Valid values are {matching_methods}.\"\n)\nself.matching = matching\nself.n_predicted_classes = n_predicted_classes\nself.n_predicted_classes_with_bg = n_predicted_classes + 1\nself.n_classes = n_classes\nself.n_classes_with_bg = n_classes + 1\nself.matching = matching\nself.use_threshold = use_threshold\nself.threshold = threshold\nself.ignore_background = ignore_background\nself.use_unmatched_as_background = use_unmatched_as_background\nif use_unmatched_as_background and ignore_background:\nraise ValueError(\n\"Option `use_unmatched_as_background` not compatible with option `ignore_background`\"\n)\nif use_unmatched_as_background and matching == \"majority\":\nraise ValueError(\n\"Option `use_unmatched_as_background` not compatible with matching `majority`\"\n)\nconfusion_mat = torch.zeros(\nself.n_predicted_classes_with_bg, self.n_classes_with_bg, dtype=torch.int64\n)\nself.add_state(\"confusion_mat\", default=confusion_mat, dist_reduce_fx=\"sum\", persistent=True)\ndef update(\nself,\npredictions: torch.Tensor,\ntargets: torch.Tensor,\nprediction_class_ids: torch.Tensor,\nignore: Optional[torch.Tensor] = None,\n):\n\"\"\"Update metric by computing confusion matrix between predicted and target classes.\n        Args:\n            predictions: Probability mask of shape (B, K, H, W) or (B, F, K, H, W), where K is the\n                number of object instances in the image.\n            targets: Mask of shape (B, K, H, W) or (B, F, K, H, W), where K is the number of object\n                instances in the image. Class ID of objects is encoded as the value, i.e. densely\n                represented.\n            prediction_class_ids: Tensor of shape (B, K), containing the class id of each predicted\n                object instance in the image. Id must be 0 &lt;= id &lt;= n_predicted_classes.\n            ignore: Ignore mask of shape (B, 1, H, W) or (B, 1, K, H, W)\n        \"\"\"\npredictions = self.preprocess_predicted_mask(predictions)\npredictions = _remap_one_hot_mask(\npredictions, prediction_class_ids, self.n_predicted_classes, strip_empty=False\n)\nassert predictions.shape[-1] == self.n_predicted_classes_with_bg\ntargets = self.preprocess_ground_truth_mask(targets)\nassert targets.shape[-1] == self.n_classes_with_bg\nif ignore is not None:\nif ignore.ndim == 5:  # Video case\nignore = ignore.flatten(0, 1)\nassert ignore.ndim == 4 and ignore.shape[1] == 1\nignore = ignore.to(torch.bool).flatten(-2, -1).squeeze(1)  # B x P\npredictions[ignore] = 0\ntargets[ignore] = 0\n# We are doing the multiply in float64 instead of int64 because it proved to be significantly\n# faster on GPU. We need to use 64 bits because we can easily exceed the range of 32 bits\n# if we aggregate over a full dataset.\nconfusion_mat = torch.einsum(\n\"bpk,bpc-&gt;kc\", predictions.to(torch.float64), targets.to(torch.float64)\n)\nself.confusion_mat += confusion_mat.to(torch.int64)\ndef preprocess_predicted_mask(self, mask: torch.Tensor) -&gt; torch.Tensor:\n\"\"\"Preprocess predicted masks for metric computation.\n        Args:\n            mask: Probability mask of shape (B, K, H, W) or (B, F, K, H, W), where K is the number\n                of object instances in the prediction.\n        Returns:\n            Binary tensor of shape (B, P, K), where P is the number of points. If `use_threshold` is\n            True, overlapping objects for the same point are possible.\n        \"\"\"\nif mask.ndim == 5:  # Video case\nmask = mask.flatten(0, 1)\nmask = mask.flatten(-2, -1)\nif self.use_threshold:\nmask = mask &gt; self.threshold\nmask = mask.transpose(1, 2)\nelse:\nmaximum, indices = torch.max(mask, dim=1)\nmask = torch.nn.functional.one_hot(indices, num_classes=mask.shape[1])\nmask[:, :, 0][maximum == 0.0] = 0\nreturn mask\ndef preprocess_ground_truth_mask(self, mask: torch.Tensor) -&gt; torch.Tensor:\n\"\"\"Preprocess ground truth mask for metric computation.\n        Args:\n            mask: Mask of shape (B, K, H, W) or (B, F, K, H, W), where K is the number of object\n                instances in the image. Class ID of objects is encoded as the value, i.e. densely\n                represented.\n        Returns:\n            One-hot tensor of shape (B, P, J), where J is the number of the classes and P the number\n            of points, with object instances with the same class ID merged together. In the case of\n            an overlap of classes for a point, the class with the highest ID is assigned to that\n            point.\n        \"\"\"\nif mask.ndim == 5:  # Video case\nmask = mask.flatten(0, 1)\nmask = mask.flatten(-2, -1)\n# Pixels which contain no object get assigned the background class 0. This also handles the\n# padding of zero masks which is done in preprocessing for batching.\nmask = torch.nn.functional.one_hot(\nmask.max(dim=1).values.to(torch.long), num_classes=self.n_classes_with_bg\n)\nreturn mask\ndef compute(self):\n\"\"\"Compute per-class IoU using matching.\"\"\"\nif self.ignore_background:\nn_classes = self.n_classes\nconfusion_mat = self.confusion_mat[:, 1:]\nelse:\nn_classes = self.n_classes_with_bg\nconfusion_mat = self.confusion_mat\npairwise_iou, _, _, area_gt = self._compute_iou_from_confusion_mat(confusion_mat)\nif self.use_unmatched_as_background:\n# Match only in foreground\npairwise_iou = pairwise_iou[1:, 1:]\nconfusion_mat = confusion_mat[1:, 1:]\nelse:\n# Predicted class zero is not matched against anything\npairwise_iou = pairwise_iou[1:]\nconfusion_mat = confusion_mat[1:]\nif self.matching == \"hungarian\":\ncluster_idxs, class_idxs = scipy.optimize.linear_sum_assignment(\npairwise_iou.cpu(), maximize=True\n)\ncluster_idxs = torch.as_tensor(\ncluster_idxs, dtype=torch.int64, device=self.confusion_mat.device\n)\nclass_idxs = torch.as_tensor(\nclass_idxs, dtype=torch.int64, device=self.confusion_mat.device\n)\nmatched_iou = pairwise_iou[cluster_idxs, class_idxs]\ntrue_pos = confusion_mat[cluster_idxs, class_idxs]\nif self.use_unmatched_as_background:\ncluster_oh = torch.nn.functional.one_hot(\ncluster_idxs, num_classes=pairwise_iou.shape[0]\n)\nmatched_clusters = cluster_oh.max(dim=0).values.to(torch.bool)\nbg_pred = self.confusion_mat[:1]\nbg_pred += self.confusion_mat[1:][~matched_clusters].sum(dim=0)\nbg_iou, _, _, _ = self._compute_iou_from_confusion_mat(bg_pred, area_gt)\nclass_idxs = torch.cat((torch.zeros_like(class_idxs[:1]), class_idxs + 1))\nmatched_iou = torch.cat((bg_iou[0, :1], matched_iou))\ntrue_pos = torch.cat((bg_pred[0, :1], true_pos))\nelif self.matching == \"majority\":\nmax_iou, class_idxs = torch.max(pairwise_iou, dim=1)\n# Form new clusters by merging old clusters which are assigned the same ground truth\n# class. After merging, the number of clusters equals the number of classes.\n_, old_to_new_cluster_idx = torch.unique(class_idxs, return_inverse=True)\nconfusion_mat_new = torch.zeros(\nn_classes, n_classes, dtype=torch.int64, device=self.confusion_mat.device\n)\nfor old_cluster_idx, new_cluster_idx in enumerate(old_to_new_cluster_idx):\nif max_iou[old_cluster_idx] &gt; 0.0:\nconfusion_mat_new[new_cluster_idx] += confusion_mat[old_cluster_idx]\n# Important: use previously computed area_gt because it includes background predictions,\n# whereas the new confusion matrix does not contain the bg predicted class anymore.\npairwise_iou, _, _, _ = self._compute_iou_from_confusion_mat(confusion_mat_new, area_gt)\nmax_iou, class_idxs = torch.max(pairwise_iou, dim=1)\nvalid = max_iou &gt; 0.0  # Ignore clusters without any kind of overlap\nclass_idxs = class_idxs[valid]\ncluster_idxs = torch.arange(pairwise_iou.shape[1])[valid]\nmatched_iou = pairwise_iou[cluster_idxs, class_idxs]\ntrue_pos = confusion_mat_new[cluster_idxs, class_idxs]\nelse:\nraise RuntimeError(f\"Unsupported matching: {self.matching}\")\niou = torch.zeros(n_classes, dtype=torch.float64, device=pairwise_iou.device)\niou[class_idxs] = matched_iou\naccuracy = true_pos.sum().to(torch.float64) / area_gt.sum()\nempty_classes = area_gt == 0\nreturn iou, accuracy, empty_classes\n@staticmethod\ndef _compute_iou_from_confusion_mat(\nconfusion_mat: torch.Tensor, area_gt: Optional[torch.Tensor] = None\n):\narea_pred = torch.sum(confusion_mat, axis=1)\nif area_gt is None:\narea_gt = torch.sum(confusion_mat, axis=0)\nunion = area_pred.unsqueeze(1) + area_gt.unsqueeze(0) - confusion_mat\npairwise_iou = confusion_mat.to(torch.float64) / union\n# Ignore classes that occured on no image.\npairwise_iou[union == 0] = 0.0\nreturn pairwise_iou, union, area_pred, area_gt\n</code></pre>"},{"location":"api/ocl/metrics/#ocl.metrics.dataset.DatasetSemanticMaskIoUMetric.update","title":"<code>update</code>","text":"<p>Update metric by computing confusion matrix between predicted and target classes.</p> <p>Parameters:</p> Name Type Description Default <code>predictions</code> <code>torch.Tensor</code> <p>Probability mask of shape (B, K, H, W) or (B, F, K, H, W), where K is the number of object instances in the image.</p> required <code>targets</code> <code>torch.Tensor</code> <p>Mask of shape (B, K, H, W) or (B, F, K, H, W), where K is the number of object instances in the image. Class ID of objects is encoded as the value, i.e. densely represented.</p> required <code>prediction_class_ids</code> <code>torch.Tensor</code> <p>Tensor of shape (B, K), containing the class id of each predicted object instance in the image. Id must be 0 &lt;= id &lt;= n_predicted_classes.</p> required <code>ignore</code> <code>Optional[torch.Tensor]</code> <p>Ignore mask of shape (B, 1, H, W) or (B, 1, K, H, W)</p> <code>None</code> Source code in <code>ocl/metrics/dataset.py</code> <pre><code>def update(\nself,\npredictions: torch.Tensor,\ntargets: torch.Tensor,\nprediction_class_ids: torch.Tensor,\nignore: Optional[torch.Tensor] = None,\n):\n\"\"\"Update metric by computing confusion matrix between predicted and target classes.\n    Args:\n        predictions: Probability mask of shape (B, K, H, W) or (B, F, K, H, W), where K is the\n            number of object instances in the image.\n        targets: Mask of shape (B, K, H, W) or (B, F, K, H, W), where K is the number of object\n            instances in the image. Class ID of objects is encoded as the value, i.e. densely\n            represented.\n        prediction_class_ids: Tensor of shape (B, K), containing the class id of each predicted\n            object instance in the image. Id must be 0 &lt;= id &lt;= n_predicted_classes.\n        ignore: Ignore mask of shape (B, 1, H, W) or (B, 1, K, H, W)\n    \"\"\"\npredictions = self.preprocess_predicted_mask(predictions)\npredictions = _remap_one_hot_mask(\npredictions, prediction_class_ids, self.n_predicted_classes, strip_empty=False\n)\nassert predictions.shape[-1] == self.n_predicted_classes_with_bg\ntargets = self.preprocess_ground_truth_mask(targets)\nassert targets.shape[-1] == self.n_classes_with_bg\nif ignore is not None:\nif ignore.ndim == 5:  # Video case\nignore = ignore.flatten(0, 1)\nassert ignore.ndim == 4 and ignore.shape[1] == 1\nignore = ignore.to(torch.bool).flatten(-2, -1).squeeze(1)  # B x P\npredictions[ignore] = 0\ntargets[ignore] = 0\n# We are doing the multiply in float64 instead of int64 because it proved to be significantly\n# faster on GPU. We need to use 64 bits because we can easily exceed the range of 32 bits\n# if we aggregate over a full dataset.\nconfusion_mat = torch.einsum(\n\"bpk,bpc-&gt;kc\", predictions.to(torch.float64), targets.to(torch.float64)\n)\nself.confusion_mat += confusion_mat.to(torch.int64)\n</code></pre>"},{"location":"api/ocl/metrics/#ocl.metrics.dataset.DatasetSemanticMaskIoUMetric.preprocess_predicted_mask","title":"<code>preprocess_predicted_mask</code>","text":"<p>Preprocess predicted masks for metric computation.</p> <p>Parameters:</p> Name Type Description Default <code>mask</code> <code>torch.Tensor</code> <p>Probability mask of shape (B, K, H, W) or (B, F, K, H, W), where K is the number of object instances in the prediction.</p> required <p>Returns:</p> Type Description <code>torch.Tensor</code> <p>Binary tensor of shape (B, P, K), where P is the number of points. If <code>use_threshold</code> is</p> <code>torch.Tensor</code> <p>True, overlapping objects for the same point are possible.</p> Source code in <code>ocl/metrics/dataset.py</code> <pre><code>def preprocess_predicted_mask(self, mask: torch.Tensor) -&gt; torch.Tensor:\n\"\"\"Preprocess predicted masks for metric computation.\n    Args:\n        mask: Probability mask of shape (B, K, H, W) or (B, F, K, H, W), where K is the number\n            of object instances in the prediction.\n    Returns:\n        Binary tensor of shape (B, P, K), where P is the number of points. If `use_threshold` is\n        True, overlapping objects for the same point are possible.\n    \"\"\"\nif mask.ndim == 5:  # Video case\nmask = mask.flatten(0, 1)\nmask = mask.flatten(-2, -1)\nif self.use_threshold:\nmask = mask &gt; self.threshold\nmask = mask.transpose(1, 2)\nelse:\nmaximum, indices = torch.max(mask, dim=1)\nmask = torch.nn.functional.one_hot(indices, num_classes=mask.shape[1])\nmask[:, :, 0][maximum == 0.0] = 0\nreturn mask\n</code></pre>"},{"location":"api/ocl/metrics/#ocl.metrics.dataset.DatasetSemanticMaskIoUMetric.preprocess_ground_truth_mask","title":"<code>preprocess_ground_truth_mask</code>","text":"<p>Preprocess ground truth mask for metric computation.</p> <p>Parameters:</p> Name Type Description Default <code>mask</code> <code>torch.Tensor</code> <p>Mask of shape (B, K, H, W) or (B, F, K, H, W), where K is the number of object instances in the image. Class ID of objects is encoded as the value, i.e. densely represented.</p> required <p>Returns:</p> Type Description <code>torch.Tensor</code> <p>One-hot tensor of shape (B, P, J), where J is the number of the classes and P the number</p> <code>torch.Tensor</code> <p>of points, with object instances with the same class ID merged together. In the case of</p> <code>torch.Tensor</code> <p>an overlap of classes for a point, the class with the highest ID is assigned to that</p> <code>torch.Tensor</code> <p>point.</p> Source code in <code>ocl/metrics/dataset.py</code> <pre><code>def preprocess_ground_truth_mask(self, mask: torch.Tensor) -&gt; torch.Tensor:\n\"\"\"Preprocess ground truth mask for metric computation.\n    Args:\n        mask: Mask of shape (B, K, H, W) or (B, F, K, H, W), where K is the number of object\n            instances in the image. Class ID of objects is encoded as the value, i.e. densely\n            represented.\n    Returns:\n        One-hot tensor of shape (B, P, J), where J is the number of the classes and P the number\n        of points, with object instances with the same class ID merged together. In the case of\n        an overlap of classes for a point, the class with the highest ID is assigned to that\n        point.\n    \"\"\"\nif mask.ndim == 5:  # Video case\nmask = mask.flatten(0, 1)\nmask = mask.flatten(-2, -1)\n# Pixels which contain no object get assigned the background class 0. This also handles the\n# padding of zero masks which is done in preprocessing for batching.\nmask = torch.nn.functional.one_hot(\nmask.max(dim=1).values.to(torch.long), num_classes=self.n_classes_with_bg\n)\nreturn mask\n</code></pre>"},{"location":"api/ocl/metrics/#ocl.metrics.dataset.DatasetSemanticMaskIoUMetric.compute","title":"<code>compute</code>","text":"<p>Compute per-class IoU using matching.</p> Source code in <code>ocl/metrics/dataset.py</code> <pre><code>def compute(self):\n\"\"\"Compute per-class IoU using matching.\"\"\"\nif self.ignore_background:\nn_classes = self.n_classes\nconfusion_mat = self.confusion_mat[:, 1:]\nelse:\nn_classes = self.n_classes_with_bg\nconfusion_mat = self.confusion_mat\npairwise_iou, _, _, area_gt = self._compute_iou_from_confusion_mat(confusion_mat)\nif self.use_unmatched_as_background:\n# Match only in foreground\npairwise_iou = pairwise_iou[1:, 1:]\nconfusion_mat = confusion_mat[1:, 1:]\nelse:\n# Predicted class zero is not matched against anything\npairwise_iou = pairwise_iou[1:]\nconfusion_mat = confusion_mat[1:]\nif self.matching == \"hungarian\":\ncluster_idxs, class_idxs = scipy.optimize.linear_sum_assignment(\npairwise_iou.cpu(), maximize=True\n)\ncluster_idxs = torch.as_tensor(\ncluster_idxs, dtype=torch.int64, device=self.confusion_mat.device\n)\nclass_idxs = torch.as_tensor(\nclass_idxs, dtype=torch.int64, device=self.confusion_mat.device\n)\nmatched_iou = pairwise_iou[cluster_idxs, class_idxs]\ntrue_pos = confusion_mat[cluster_idxs, class_idxs]\nif self.use_unmatched_as_background:\ncluster_oh = torch.nn.functional.one_hot(\ncluster_idxs, num_classes=pairwise_iou.shape[0]\n)\nmatched_clusters = cluster_oh.max(dim=0).values.to(torch.bool)\nbg_pred = self.confusion_mat[:1]\nbg_pred += self.confusion_mat[1:][~matched_clusters].sum(dim=0)\nbg_iou, _, _, _ = self._compute_iou_from_confusion_mat(bg_pred, area_gt)\nclass_idxs = torch.cat((torch.zeros_like(class_idxs[:1]), class_idxs + 1))\nmatched_iou = torch.cat((bg_iou[0, :1], matched_iou))\ntrue_pos = torch.cat((bg_pred[0, :1], true_pos))\nelif self.matching == \"majority\":\nmax_iou, class_idxs = torch.max(pairwise_iou, dim=1)\n# Form new clusters by merging old clusters which are assigned the same ground truth\n# class. After merging, the number of clusters equals the number of classes.\n_, old_to_new_cluster_idx = torch.unique(class_idxs, return_inverse=True)\nconfusion_mat_new = torch.zeros(\nn_classes, n_classes, dtype=torch.int64, device=self.confusion_mat.device\n)\nfor old_cluster_idx, new_cluster_idx in enumerate(old_to_new_cluster_idx):\nif max_iou[old_cluster_idx] &gt; 0.0:\nconfusion_mat_new[new_cluster_idx] += confusion_mat[old_cluster_idx]\n# Important: use previously computed area_gt because it includes background predictions,\n# whereas the new confusion matrix does not contain the bg predicted class anymore.\npairwise_iou, _, _, _ = self._compute_iou_from_confusion_mat(confusion_mat_new, area_gt)\nmax_iou, class_idxs = torch.max(pairwise_iou, dim=1)\nvalid = max_iou &gt; 0.0  # Ignore clusters without any kind of overlap\nclass_idxs = class_idxs[valid]\ncluster_idxs = torch.arange(pairwise_iou.shape[1])[valid]\nmatched_iou = pairwise_iou[cluster_idxs, class_idxs]\ntrue_pos = confusion_mat_new[cluster_idxs, class_idxs]\nelse:\nraise RuntimeError(f\"Unsupported matching: {self.matching}\")\niou = torch.zeros(n_classes, dtype=torch.float64, device=pairwise_iou.device)\niou[class_idxs] = matched_iou\naccuracy = true_pos.sum().to(torch.float64) / area_gt.sum()\nempty_classes = area_gt == 0\nreturn iou, accuracy, empty_classes\n</code></pre>"},{"location":"api/ocl/metrics/#ocl.metrics.ARIMetric","title":"<code>ARIMetric</code>","text":"<p>         Bases: <code>torchmetrics.Metric</code></p> <p>Computes ARI metric.</p> Source code in <code>ocl/metrics/masks.py</code> <pre><code>class ARIMetric(torchmetrics.Metric):\n\"\"\"Computes ARI metric.\"\"\"\ndef __init__(\nself,\nforeground: bool = True,\nconvert_target_one_hot: bool = False,\nignore_overlaps: bool = False,\n):\nsuper().__init__()\nself.foreground = foreground\nself.convert_target_one_hot = convert_target_one_hot\nself.ignore_overlaps = ignore_overlaps\nself.add_state(\n\"values\", default=torch.tensor(0.0, dtype=torch.float64), dist_reduce_fx=\"sum\"\n)\nself.add_state(\"total\", default=torch.tensor(0), dist_reduce_fx=\"sum\")\ndef update(\nself, prediction: torch.Tensor, target: torch.Tensor, ignore: Optional[torch.Tensor] = None\n):\n\"\"\"Update this metric.\n        Args:\n            prediction: Predicted mask of shape (B, C, H, W) or (B, F, C, H, W), where C is the\n                number of classes.\n            target: Ground truth mask of shape (B, K, H, W) or (B, F, K, H, W), where K is the\n                number of classes.\n            ignore: Ignore mask of shape (B, 1, H, W) or (B, 1, K, H, W)\n        \"\"\"\nif prediction.ndim == 5:\n# Merge frames, height and width to single dimension.\nprediction = prediction.transpose(1, 2).flatten(-3, -1)\ntarget = target.transpose(1, 2).flatten(-3, -1)\nif ignore is not None:\nignore = ignore.to(torch.bool).transpose(1, 2).flatten(-3, -1)\nelif prediction.ndim == 4:\n# Merge height and width to single dimension.\nprediction = prediction.flatten(-2, -1)\ntarget = target.flatten(-2, -1)\nif ignore is not None:\nignore = ignore.to(torch.bool).flatten(-2, -1)\nelse:\nraise ValueError(f\"Incorrect input shape: f{prediction.shape}\")\nif self.ignore_overlaps:\noverlaps = (target &gt; 0).sum(1, keepdim=True) &gt; 1\nif ignore is None:\nignore = overlaps\nelse:\nignore = ignore | overlaps\nif ignore is not None:\nassert ignore.ndim == 3 and ignore.shape[1] == 1\nprediction = prediction.clone()\nprediction[ignore.expand_as(prediction)] = 0\ntarget = target.clone()\ntarget[ignore.expand_as(target)] = 0\n# Make channels / gt labels the last dimension.\nprediction = prediction.transpose(-2, -1)\ntarget = target.transpose(-2, -1)\nif self.convert_target_one_hot:\ntarget_oh = tensor_to_one_hot(target, dim=2)\n# For empty pixels (all values zero), one-hot assigns 1 to the first class, correct for\n# this (then it is technically not one-hot anymore).\ntarget_oh[:, :, 0][target.sum(dim=2) == 0] = 0\ntarget = target_oh\n# Should be either 0 (empty, padding) or 1 (single object).\nassert torch.all(target.sum(dim=-1) &lt; 2), \"Issues with target format, mask non-exclusive\"\nif self.foreground:\nari = fg_adjusted_rand_index(prediction, target)\nelse:\nari = adjusted_rand_index(prediction, target)\nself.values += ari.sum()\nself.total += len(ari)\ndef compute(self) -&gt; torch.Tensor:\nreturn self.values / self.total\n</code></pre>"},{"location":"api/ocl/metrics/#ocl.metrics.masks.ARIMetric.update","title":"<code>update</code>","text":"<p>Update this metric.</p> <p>Parameters:</p> Name Type Description Default <code>prediction</code> <code>torch.Tensor</code> <p>Predicted mask of shape (B, C, H, W) or (B, F, C, H, W), where C is the number of classes.</p> required <code>target</code> <code>torch.Tensor</code> <p>Ground truth mask of shape (B, K, H, W) or (B, F, K, H, W), where K is the number of classes.</p> required <code>ignore</code> <code>Optional[torch.Tensor]</code> <p>Ignore mask of shape (B, 1, H, W) or (B, 1, K, H, W)</p> <code>None</code> Source code in <code>ocl/metrics/masks.py</code> <pre><code>def update(\nself, prediction: torch.Tensor, target: torch.Tensor, ignore: Optional[torch.Tensor] = None\n):\n\"\"\"Update this metric.\n    Args:\n        prediction: Predicted mask of shape (B, C, H, W) or (B, F, C, H, W), where C is the\n            number of classes.\n        target: Ground truth mask of shape (B, K, H, W) or (B, F, K, H, W), where K is the\n            number of classes.\n        ignore: Ignore mask of shape (B, 1, H, W) or (B, 1, K, H, W)\n    \"\"\"\nif prediction.ndim == 5:\n# Merge frames, height and width to single dimension.\nprediction = prediction.transpose(1, 2).flatten(-3, -1)\ntarget = target.transpose(1, 2).flatten(-3, -1)\nif ignore is not None:\nignore = ignore.to(torch.bool).transpose(1, 2).flatten(-3, -1)\nelif prediction.ndim == 4:\n# Merge height and width to single dimension.\nprediction = prediction.flatten(-2, -1)\ntarget = target.flatten(-2, -1)\nif ignore is not None:\nignore = ignore.to(torch.bool).flatten(-2, -1)\nelse:\nraise ValueError(f\"Incorrect input shape: f{prediction.shape}\")\nif self.ignore_overlaps:\noverlaps = (target &gt; 0).sum(1, keepdim=True) &gt; 1\nif ignore is None:\nignore = overlaps\nelse:\nignore = ignore | overlaps\nif ignore is not None:\nassert ignore.ndim == 3 and ignore.shape[1] == 1\nprediction = prediction.clone()\nprediction[ignore.expand_as(prediction)] = 0\ntarget = target.clone()\ntarget[ignore.expand_as(target)] = 0\n# Make channels / gt labels the last dimension.\nprediction = prediction.transpose(-2, -1)\ntarget = target.transpose(-2, -1)\nif self.convert_target_one_hot:\ntarget_oh = tensor_to_one_hot(target, dim=2)\n# For empty pixels (all values zero), one-hot assigns 1 to the first class, correct for\n# this (then it is technically not one-hot anymore).\ntarget_oh[:, :, 0][target.sum(dim=2) == 0] = 0\ntarget = target_oh\n# Should be either 0 (empty, padding) or 1 (single object).\nassert torch.all(target.sum(dim=-1) &lt; 2), \"Issues with target format, mask non-exclusive\"\nif self.foreground:\nari = fg_adjusted_rand_index(prediction, target)\nelse:\nari = adjusted_rand_index(prediction, target)\nself.values += ari.sum()\nself.total += len(ari)\n</code></pre>"},{"location":"api/ocl/metrics/#ocl.metrics.MOTMetric","title":"<code>MOTMetric</code>","text":"<p>         Bases: <code>torchmetrics.Metric</code></p> <p>Multiple object tracking metric.</p> Source code in <code>ocl/metrics/tracking.py</code> <pre><code>class MOTMetric(torchmetrics.Metric):\n\"\"\"Multiple object tracking metric.\"\"\"\ndef __init__(\nself,\ntarget_is_mask: bool = True,\nuse_threshold: bool = True,\nthreshold: float = 0.5,\n):\n\"\"\"Initialize MOTMetric.\n        Args:\n            target_is_mask: Is the metrics evaluated on masks\n            use_threshold: Use threshold to binarize predicted mask\n            threshold: Threshold value\n        \"\"\"\nsuper().__init__()\nself.target_is_mask = target_is_mask\nself.use_threshold = use_threshold\nself.threshold = threshold\nself.reset_accumulator()\nself.accuracy = []\nself.add_state(\n\"values\", default=torch.tensor(0.0, dtype=torch.float64), dist_reduce_fx=\"sum\"\n)\nself.add_state(\"total\", default=torch.tensor(0), dist_reduce_fx=\"sum\")\ndef reset_accumulator(self):\nself.acc = mm.MOTAccumulator(auto_id=True)\ndef update(self, prediction: torch.Tensor, target: torch.Tensor):\n# Merge batch and frame dimensions\nB, F = prediction.shape[:2]\nprediction = prediction.flatten(0, 1)\ntarget = target.flatten(0, 1)\nn_pred_classes = prediction.shape[1]\nn_gt_classes = target.shape[1]\nif self.use_threshold:\nprediction = prediction &gt; self.threshold\nelse:\nindices = torch.argmax(prediction, dim=1)\nprediction = torch.nn.functional.one_hot(indices, num_classes=n_pred_classes)\nprediction = prediction.permute(0, 3, 1, 2)\npred_bboxes = masks_to_bboxes(prediction.flatten(0, 1)).unflatten(0, (B, F, n_pred_classes))\nif self.target_is_mask:\ntarget_bboxes = masks_to_bboxes(target.flatten(0, 1)).unflatten(0, (B, F, n_gt_classes))\nelse:\nassert target.shape[-1] == 4\n# Convert all-zero boxes added during padding to invalid boxes\ntarget[torch.all(target == 0.0, dim=-1)] = -1.0\ntarget_bboxes = target\nself.reset_accumulator()\nfor preds, targets in zip(pred_bboxes, target_bboxes):\n# seq evaluation\nself.reset_accumulator()\nfor pred, target, mask in zip(preds, targets, prediction):\nvalid_track_box = pred[:, 0] != -1.0\nvalid_target_box = target[:, 0] != -1.0\ntrack_id = valid_track_box.nonzero()[:, 0].detach().cpu().numpy()\ntarget_id = valid_target_box.nonzero()[:, 0].detach().cpu().numpy()\n# move background\nidx = track_id.tolist()\nfor id in idx:\nh, w = mask[id].shape\nthres = h * w * 0.25\nif pred[id][2] * pred[id][3] &gt;= thres:\nidx.remove(id)\ncur_obj_idx = np.array(idx)\nif valid_target_box.sum() == 0:\ncontinue  # Skip data points without any target bbox\npred = pred[cur_obj_idx].detach().cpu().numpy()\ntarget = target[valid_target_box].detach().cpu().numpy()\n# frame evaluation\nself.eval_frame(pred, target, cur_obj_idx, target_id)\nself.accuracy.append(self.acc)\nself.total += 1\ndef eval_frame(self, trk_tlwhs, tgt_tlwhs, trk_ids, tgt_ids):\n# get distance matrix\ntrk_tlwhs = np.copy(trk_tlwhs)\ntgt_tlwhs = np.copy(tgt_tlwhs)\ntrk_ids = np.copy(trk_ids)\ntgt_ids = np.copy(tgt_ids)\niou_distance = mm.distances.iou_matrix(tgt_tlwhs, trk_tlwhs, max_iou=0.5)\n# acc\nself.acc.update(tgt_ids, trk_ids, iou_distance)\ndef convert_motmetric_to_value(self, res):\ndp = res.replace(\" \", \";\").replace(\";;\", \";\").replace(\";;\", \";\").replace(\";;\", \";\")\ntmp = list(dp)\ntmp[0] = \"-\"\ndp = \"\".join(tmp)\nreturn io.StringIO(dp)\ndef compute(self) -&gt; torch.Tensor:\nif self.total == 0:\nreturn torch.zeros_like(self.values)\nelse:\nmetrics = mm.metrics.motchallenge_metrics\nmh = mm.metrics.create()\nsummary = mh.compute_many(\nself.accuracy, metrics=metrics, names=None, generate_overall=True\n)\nstrsummary = mm.io.render_summary(\nsummary, formatters=mh.formatters, namemap=mm.io.motchallenge_metric_names\n)\nres = self.convert_motmetric_to_value(strsummary)\ndf = pd.read_csv(res, sep=\";\", engine=\"python\")\nmota = df.iloc[-1][\"MOTA\"]\nself.values = torch.tensor(float(mota[:-1]), dtype=torch.float64).to(self.values.device)\nself.reset_accumulator()\nself.accuracy = []\nreturn self.values\n</code></pre>"},{"location":"api/ocl/metrics/#ocl.metrics.tracking.MOTMetric.__init__","title":"<code>__init__</code>","text":"<p>Initialize MOTMetric.</p> <p>Parameters:</p> Name Type Description Default <code>target_is_mask</code> <code>bool</code> <p>Is the metrics evaluated on masks</p> <code>True</code> <code>use_threshold</code> <code>bool</code> <p>Use threshold to binarize predicted mask</p> <code>True</code> <code>threshold</code> <code>float</code> <p>Threshold value</p> <code>0.5</code> Source code in <code>ocl/metrics/tracking.py</code> <pre><code>def __init__(\nself,\ntarget_is_mask: bool = True,\nuse_threshold: bool = True,\nthreshold: float = 0.5,\n):\n\"\"\"Initialize MOTMetric.\n    Args:\n        target_is_mask: Is the metrics evaluated on masks\n        use_threshold: Use threshold to binarize predicted mask\n        threshold: Threshold value\n    \"\"\"\nsuper().__init__()\nself.target_is_mask = target_is_mask\nself.use_threshold = use_threshold\nself.threshold = threshold\nself.reset_accumulator()\nself.accuracy = []\nself.add_state(\n\"values\", default=torch.tensor(0.0, dtype=torch.float64), dist_reduce_fx=\"sum\"\n)\nself.add_state(\"total\", default=torch.tensor(0), dist_reduce_fx=\"sum\")\n</code></pre>"},{"location":"api/ocl/metrics/#ocl.metrics.PatchARIMetric","title":"<code>PatchARIMetric</code>","text":"<p>         Bases: <code>ARIMetric</code></p> <p>Computes ARI metric assuming patch masks as input.</p> Source code in <code>ocl/metrics/masks.py</code> <pre><code>class PatchARIMetric(ARIMetric):\n\"\"\"Computes ARI metric assuming patch masks as input.\"\"\"\ndef __init__(\nself,\nforeground=True,\nresize_masks_mode: str = \"bilinear\",\n**kwargs,\n):\nsuper().__init__(foreground=foreground, **kwargs)\nself.resize_masks_mode = resize_masks_mode\ndef update(self, prediction: torch.Tensor, target: torch.Tensor):\n\"\"\"Update this metric.\n        Args:\n            prediction: Predicted mask of shape (B, C, P) or (B, F, C, P), where C is the\n                number of classes and P the number of patches.\n            target: Ground truth mask of shape (B, K, H, W) or (B, F, K, H, W), where K is the\n                number of classes.\n        \"\"\"\nh, w = target.shape[-2:]\nassert h == w\nprediction_resized = resize_patches_to_image(\nprediction, size=h, resize_mode=self.resize_masks_mode\n)\nreturn super().update(prediction=prediction_resized, target=target)\n</code></pre>"},{"location":"api/ocl/metrics/#ocl.metrics.masks.PatchARIMetric.update","title":"<code>update</code>","text":"<p>Update this metric.</p> <p>Parameters:</p> Name Type Description Default <code>prediction</code> <code>torch.Tensor</code> <p>Predicted mask of shape (B, C, P) or (B, F, C, P), where C is the number of classes and P the number of patches.</p> required <code>target</code> <code>torch.Tensor</code> <p>Ground truth mask of shape (B, K, H, W) or (B, F, K, H, W), where K is the number of classes.</p> required Source code in <code>ocl/metrics/masks.py</code> <pre><code>def update(self, prediction: torch.Tensor, target: torch.Tensor):\n\"\"\"Update this metric.\n    Args:\n        prediction: Predicted mask of shape (B, C, P) or (B, F, C, P), where C is the\n            number of classes and P the number of patches.\n        target: Ground truth mask of shape (B, K, H, W) or (B, F, K, H, W), where K is the\n            number of classes.\n    \"\"\"\nh, w = target.shape[-2:]\nassert h == w\nprediction_resized = resize_patches_to_image(\nprediction, size=h, resize_mode=self.resize_masks_mode\n)\nreturn super().update(prediction=prediction_resized, target=target)\n</code></pre>"},{"location":"api/ocl/metrics/#ocl.metrics.UnsupervisedMaskIoUMetric","title":"<code>UnsupervisedMaskIoUMetric</code>","text":"<p>         Bases: <code>torchmetrics.Metric</code></p> <p>Computes IoU metric for segmentation masks when correspondences to ground truth are not known.</p> <p>Uses Hungarian matching to compute the assignment between predicted classes and ground truth classes.</p> <p>Parameters:</p> Name Type Description Default <code>use_threshold</code> <code>bool</code> <p>If <code>True</code>, convert predicted class probabilities to mask using a threshold. If <code>False</code>, class probabilities are turned into mask using a softmax instead.</p> <code>False</code> <code>threshold</code> <code>float</code> <p>Value to use for thresholding masks.</p> <code>0.5</code> <code>matching</code> <code>str</code> <p>Approach to match predicted to ground truth classes. For \"hungarian\", computes assignment that maximizes total IoU between all classes. For \"best_overlap\", uses the predicted class with maximum overlap for each ground truth class. Using \"best_overlap\" leads to the \"average best overlap\" metric.</p> <code>'hungarian'</code> <code>compute_discovery_fraction</code> <code>bool</code> <p>Instead of the IoU, compute the fraction of ground truth classes that were \"discovered\", meaning that they have an IoU greater than some threshold.</p> <code>False</code> <code>correct_localization</code> <code>bool</code> <p>Instead of the IoU, compute the fraction of images on which at least one ground truth class was correctly localised, meaning that they have an IoU greater than some threshold.</p> <code>False</code> <code>discovery_threshold</code> <code>float</code> <p>Minimum IoU to count a class as discovered/correctly localized.</p> <code>0.5</code> <code>ignore_background</code> <code>bool</code> <p>If true, assume class at index 0 of ground truth masks is background class that is removed before computing IoU.</p> <code>False</code> <code>ignore_overlaps</code> <code>bool</code> <p>If true, remove points where ground truth masks has overlappign classes from predictions and ground truth masks.</p> <code>False</code> Source code in <code>ocl/metrics/masks.py</code> <pre><code>class UnsupervisedMaskIoUMetric(torchmetrics.Metric):\n\"\"\"Computes IoU metric for segmentation masks when correspondences to ground truth are not known.\n    Uses Hungarian matching to compute the assignment between predicted classes and ground truth\n    classes.\n    Args:\n        use_threshold: If `True`, convert predicted class probabilities to mask using a threshold.\n            If `False`, class probabilities are turned into mask using a softmax instead.\n        threshold: Value to use for thresholding masks.\n        matching: Approach to match predicted to ground truth classes. For \"hungarian\", computes\n            assignment that maximizes total IoU between all classes. For \"best_overlap\", uses the\n            predicted class with maximum overlap for each ground truth class. Using \"best_overlap\"\n            leads to the \"average best overlap\" metric.\n        compute_discovery_fraction: Instead of the IoU, compute the fraction of ground truth classes\n            that were \"discovered\", meaning that they have an IoU greater than some threshold.\n        correct_localization: Instead of the IoU, compute the fraction of images on which at least\n            one ground truth class was correctly localised, meaning that they have an IoU\n            greater than some threshold.\n        discovery_threshold: Minimum IoU to count a class as discovered/correctly localized.\n        ignore_background: If true, assume class at index 0 of ground truth masks is background class\n            that is removed before computing IoU.\n        ignore_overlaps: If true, remove points where ground truth masks has overlappign classes from\n            predictions and ground truth masks.\n    \"\"\"\ndef __init__(\nself,\nuse_threshold: bool = False,\nthreshold: float = 0.5,\nmatching: str = \"hungarian\",\ncompute_discovery_fraction: bool = False,\ncorrect_localization: bool = False,\ndiscovery_threshold: float = 0.5,\nignore_background: bool = False,\nignore_overlaps: bool = False,\n):\nsuper().__init__()\nself.use_threshold = use_threshold\nself.threshold = threshold\nself.discovery_threshold = discovery_threshold\nself.compute_discovery_fraction = compute_discovery_fraction\nself.correct_localization = correct_localization\nif compute_discovery_fraction and correct_localization:\nraise ValueError(\n\"Only one of `compute_discovery_fraction` and `correct_localization` can be enabled.\"\n)\nmatchings = (\"hungarian\", \"best_overlap\")\nif matching not in matchings:\nraise ValueError(f\"Unknown matching type {matching}. Valid values are {matchings}.\")\nself.matching = matching\nself.ignore_background = ignore_background\nself.ignore_overlaps = ignore_overlaps\nself.add_state(\n\"values\", default=torch.tensor(0.0, dtype=torch.float64), dist_reduce_fx=\"sum\"\n)\nself.add_state(\"total\", default=torch.tensor(0), dist_reduce_fx=\"sum\")\ndef update(\nself, prediction: torch.Tensor, target: torch.Tensor, ignore: Optional[torch.Tensor] = None\n):\n\"\"\"Update this metric.\n        Args:\n            prediction: Predicted mask of shape (B, C, H, W) or (B, F, C, H, W), where C is the\n                number of classes. Assumes class probabilities as inputs.\n            target: Ground truth mask of shape (B, K, H, W) or (B, F, K, H, W), where K is the\n                number of classes.\n            ignore: Ignore mask of shape (B, 1, H, W) or (B, 1, K, H, W)\n        \"\"\"\nif prediction.ndim == 5:\n# Merge frames, height and width to single dimension.\npredictions = prediction.transpose(1, 2).flatten(-3, -1)\ntargets = target.transpose(1, 2).flatten(-3, -1)\nif ignore is not None:\nignore = ignore.to(torch.bool).transpose(1, 2).flatten(-3, -1)\nelif prediction.ndim == 4:\n# Merge height and width to single dimension.\npredictions = prediction.flatten(-2, -1)\ntargets = target.flatten(-2, -1)\nif ignore is not None:\nignore = ignore.to(torch.bool).flatten(-2, -1)\nelse:\nraise ValueError(f\"Incorrect input shape: f{prediction.shape}\")\nif self.use_threshold:\npredictions = predictions &gt; self.threshold\nelse:\nindices = torch.argmax(predictions, dim=1)\npredictions = torch.nn.functional.one_hot(indices, num_classes=predictions.shape[1])\npredictions = predictions.transpose(1, 2)\nif self.ignore_background:\ntargets = targets[:, 1:]\ntargets = targets &gt; 0  # Ensure masks are binary\nif self.ignore_overlaps:\noverlaps = targets.sum(1, keepdim=True) &gt; 1\nif ignore is None:\nignore = overlaps\nelse:\nignore = ignore | overlaps\nif ignore is not None:\nassert ignore.ndim == 3 and ignore.shape[1] == 1\npredictions[ignore.expand_as(predictions)] = 0\ntargets[ignore.expand_as(targets)] = 0\n# Should be either 0 (empty, padding) or 1 (single object).\nassert torch.all(targets.sum(dim=1) &lt; 2), \"Issues with target format, mask non-exclusive\"\nfor pred, target in zip(predictions, targets):\nnonzero_classes = torch.sum(target, dim=-1) &gt; 0\ntarget = target[nonzero_classes]  # Remove empty (e.g. padded) classes\nif len(target) == 0:\ncontinue  # Skip elements without any target mask\niou_per_class = unsupervised_mask_iou(\npred, target, matching=self.matching, reduction=\"none\"\n)\nif self.compute_discovery_fraction:\ndiscovered = iou_per_class &gt; self.discovery_threshold\nself.values += discovered.sum() / len(discovered)\nelif self.correct_localization:\ncorrectly_localized = torch.any(iou_per_class &gt; self.discovery_threshold)\nself.values += correctly_localized.sum()\nelse:\nself.values += iou_per_class.mean()\nself.total += 1\ndef compute(self) -&gt; torch.Tensor:\nif self.total == 0:\nreturn torch.zeros_like(self.values)\nelse:\nreturn self.values / self.total\n</code></pre>"},{"location":"api/ocl/metrics/#ocl.metrics.masks.UnsupervisedMaskIoUMetric.update","title":"<code>update</code>","text":"<p>Update this metric.</p> <p>Parameters:</p> Name Type Description Default <code>prediction</code> <code>torch.Tensor</code> <p>Predicted mask of shape (B, C, H, W) or (B, F, C, H, W), where C is the number of classes. Assumes class probabilities as inputs.</p> required <code>target</code> <code>torch.Tensor</code> <p>Ground truth mask of shape (B, K, H, W) or (B, F, K, H, W), where K is the number of classes.</p> required <code>ignore</code> <code>Optional[torch.Tensor]</code> <p>Ignore mask of shape (B, 1, H, W) or (B, 1, K, H, W)</p> <code>None</code> Source code in <code>ocl/metrics/masks.py</code> <pre><code>def update(\nself, prediction: torch.Tensor, target: torch.Tensor, ignore: Optional[torch.Tensor] = None\n):\n\"\"\"Update this metric.\n    Args:\n        prediction: Predicted mask of shape (B, C, H, W) or (B, F, C, H, W), where C is the\n            number of classes. Assumes class probabilities as inputs.\n        target: Ground truth mask of shape (B, K, H, W) or (B, F, K, H, W), where K is the\n            number of classes.\n        ignore: Ignore mask of shape (B, 1, H, W) or (B, 1, K, H, W)\n    \"\"\"\nif prediction.ndim == 5:\n# Merge frames, height and width to single dimension.\npredictions = prediction.transpose(1, 2).flatten(-3, -1)\ntargets = target.transpose(1, 2).flatten(-3, -1)\nif ignore is not None:\nignore = ignore.to(torch.bool).transpose(1, 2).flatten(-3, -1)\nelif prediction.ndim == 4:\n# Merge height and width to single dimension.\npredictions = prediction.flatten(-2, -1)\ntargets = target.flatten(-2, -1)\nif ignore is not None:\nignore = ignore.to(torch.bool).flatten(-2, -1)\nelse:\nraise ValueError(f\"Incorrect input shape: f{prediction.shape}\")\nif self.use_threshold:\npredictions = predictions &gt; self.threshold\nelse:\nindices = torch.argmax(predictions, dim=1)\npredictions = torch.nn.functional.one_hot(indices, num_classes=predictions.shape[1])\npredictions = predictions.transpose(1, 2)\nif self.ignore_background:\ntargets = targets[:, 1:]\ntargets = targets &gt; 0  # Ensure masks are binary\nif self.ignore_overlaps:\noverlaps = targets.sum(1, keepdim=True) &gt; 1\nif ignore is None:\nignore = overlaps\nelse:\nignore = ignore | overlaps\nif ignore is not None:\nassert ignore.ndim == 3 and ignore.shape[1] == 1\npredictions[ignore.expand_as(predictions)] = 0\ntargets[ignore.expand_as(targets)] = 0\n# Should be either 0 (empty, padding) or 1 (single object).\nassert torch.all(targets.sum(dim=1) &lt; 2), \"Issues with target format, mask non-exclusive\"\nfor pred, target in zip(predictions, targets):\nnonzero_classes = torch.sum(target, dim=-1) &gt; 0\ntarget = target[nonzero_classes]  # Remove empty (e.g. padded) classes\nif len(target) == 0:\ncontinue  # Skip elements without any target mask\niou_per_class = unsupervised_mask_iou(\npred, target, matching=self.matching, reduction=\"none\"\n)\nif self.compute_discovery_fraction:\ndiscovered = iou_per_class &gt; self.discovery_threshold\nself.values += discovered.sum() / len(discovered)\nelif self.correct_localization:\ncorrectly_localized = torch.any(iou_per_class &gt; self.discovery_threshold)\nself.values += correctly_localized.sum()\nelse:\nself.values += iou_per_class.mean()\nself.total += 1\n</code></pre>"},{"location":"api/ocl/metrics/#ocl.metrics.SklearnClustering","title":"<code>SklearnClustering</code>","text":"<p>Wrapper around scikit-learn clustering algorithms.</p> <p>Parameters:</p> Name Type Description Default <code>n_clusters</code> <code>int</code> <p>Number of clusters.</p> required <code>method</code> <code>str</code> <p>Clustering method to use.</p> <code>'kmeans'</code> <code>clustering_kwargs</code> <code>Optional[Dict[str, Any]]</code> <p>Dictionary of additional keyword arguments to pass to clustering object.</p> <code>None</code> <code>use_l2_normalization</code> <code>bool</code> <p>Whether to L2 normalize the representations before clustering (but after PCA).</p> <code>False</code> <code>use_pca</code> <code>bool</code> <p>Whether to apply PCA before fitting the clusters.</p> <code>False</code> <code>pca_dimensions</code> <code>Optional[int]</code> <p>Number of dimensions for PCA dimensionality reduction. If <code>None</code>, do not reduce dimensions with PCA.</p> <code>None</code> <code>pca_kwargs</code> <code>Optional[Dict[str, Any]]</code> <p>Dictionary of additional keyword arguments to pass to PCA object.</p> <code>None</code> Source code in <code>ocl/metrics/dataset.py</code> <pre><code>class SklearnClustering:\n\"\"\"Wrapper around scikit-learn clustering algorithms.\n    Args:\n        n_clusters: Number of clusters.\n        method: Clustering method to use.\n        clustering_kwargs: Dictionary of additional keyword arguments to pass to clustering object.\n        use_l2_normalization: Whether to L2 normalize the representations before clustering (but\n            after PCA).\n        use_pca: Whether to apply PCA before fitting the clusters.\n        pca_dimensions: Number of dimensions for PCA dimensionality reduction. If `None`, do not\n            reduce dimensions with PCA.\n        pca_kwargs: Dictionary of additional keyword arguments to pass to PCA object.\n    \"\"\"\ndef __init__(\nself,\nn_clusters: int,\nmethod: str = \"kmeans\",\nclustering_kwargs: Optional[Dict[str, Any]] = None,\nuse_l2_normalization: bool = False,\nuse_pca: bool = False,\npca_dimensions: Optional[int] = None,\npca_kwargs: Optional[Dict[str, Any]] = None,\n):\nmethods = (\"kmeans\", \"spectral\")\nif method not in methods:\nraise ValueError(f\"Unknown clustering method {method}. Valid values are {methods}.\")\nself._n_clusters = n_clusters\nself.method = method\nself.clustering_kwargs = clustering_kwargs\nself.use_l2_normalization = use_l2_normalization\nself.use_pca = use_pca\nself.pca_dimensions = pca_dimensions\nself.pca_kwargs = pca_kwargs\nself._clustering = None\nself._pca = None\n@property\ndef n_clusters(self):\nreturn self._n_clusters\ndef _init(self):\nfrom sklearn import cluster, decomposition\nkwargs = self.clustering_kwargs if self.clustering_kwargs is not None else {}\nif self.method == \"kmeans\":\nself._clustering = cluster.KMeans(n_clusters=self.n_clusters, **kwargs)\nelif self.method == \"spectral\":\nself._clustering = cluster.SpectralClustering(n_clusters=self.n_clusters, **kwargs)\nelse:\nraise NotImplementedError(f\"Clustering {self.method} not implemented.\")\nif self.use_pca:\nkwargs = self.pca_kwargs if self.pca_kwargs is not None else {}\nself._pca = decomposition.PCA(n_components=self.pca_dimensions, **kwargs)\ndef fit_predict(self, features: torch.Tensor):\nself._init()\nfeatures = features.detach().cpu().numpy()\nif self.use_pca:\nfeatures = self._pca.fit_transform(features)\nif self.use_l2_normalization:\nfeatures /= np.maximum(np.linalg.norm(features, ord=2, axis=1, keepdims=True), 1e-8)\ncluster_ids = self._clustering.fit_predict(features).astype(np.int64)\nreturn torch.from_numpy(cluster_ids)\ndef predict(self, features: torch.Tensor) -&gt; torch.Tensor:\nif self._clustering is None:\nraise ValueError(\"Clustering was not fitted. Call `fit_predict` first.\")\nfeatures = features.detach().cpu().numpy()\nif self.use_pca:\nfeatures = self._pca.transform(features)\nif self.use_l2_normalization:\nfeatures /= np.maximum(np.linalg.norm(features, ord=2, axis=1, keepdims=True), 1e-8)\ncluster_ids = self._clustering.predict(features).astype(np.int64)\nreturn torch.from_numpy(cluster_ids)\n</code></pre>"},{"location":"api/ocl/metrics/bbox/","title":"ocl.metrics.bbox","text":"<p>Metrics related to the evaluation of bounding boxes.</p>"},{"location":"api/ocl/metrics/bbox/#ocl.metrics.bbox.UnsupervisedBboxIoUMetric","title":"<code>UnsupervisedBboxIoUMetric</code>","text":"<p>         Bases: <code>torchmetrics.Metric</code></p> <p>Computes IoU metric for bounding boxes when correspondences to ground truth are not known.</p> <p>Currently, assumes segmentation masks as input for both prediction and targets.</p> <p>Parameters:</p> Name Type Description Default <code>target_is_mask</code> <code>bool</code> <p>If <code>True</code>, assume input is a segmentation mask, in which case the masks are converted to bounding boxes before computing IoU. If <code>False</code>, assume the input for the targets are already bounding boxes.</p> <code>False</code> <code>use_threshold</code> <code>bool</code> <p>If <code>True</code>, convert predicted class probabilities to mask using a threshold. If <code>False</code>, class probabilities are turned into mask using a softmax instead.</p> <code>False</code> <code>threshold</code> <code>float</code> <p>Value to use for thresholding masks.</p> <code>0.5</code> <code>matching</code> <code>str</code> <p>How to match predicted boxes to ground truth boxes. For \"hungarian\", computes assignment that maximizes total IoU between all boxes. For \"best_overlap\", uses the predicted box with maximum overlap for each ground truth box (each predicted box can be assigned to multiple ground truth boxes).</p> <code>'hungarian'</code> <code>compute_discovery_fraction</code> <code>bool</code> <p>Instead of the IoU, compute the fraction of ground truth classes that were \"discovered\", meaning that they have an IoU greater than some threshold. This is recall, or sometimes called the detection rate metric.</p> <code>False</code> <code>correct_localization</code> <code>bool</code> <p>Instead of the IoU, compute the fraction of images on which at least one ground truth bounding box was correctly localised, meaning that they have an IoU greater than some threshold.</p> <code>False</code> <code>discovery_threshold</code> <code>float</code> <p>Minimum IoU to count a class as discovered/correctly localized.</p> <code>0.5</code> Source code in <code>ocl/metrics/bbox.py</code> <pre><code>class UnsupervisedBboxIoUMetric(torchmetrics.Metric):\n\"\"\"Computes IoU metric for bounding boxes when correspondences to ground truth are not known.\n    Currently, assumes segmentation masks as input for both prediction and targets.\n    Args:\n        target_is_mask: If `True`, assume input is a segmentation mask, in which case the masks are\n            converted to bounding boxes before computing IoU. If `False`, assume the input for the\n            targets are already bounding boxes.\n        use_threshold: If `True`, convert predicted class probabilities to mask using a threshold.\n            If `False`, class probabilities are turned into mask using a softmax instead.\n        threshold: Value to use for thresholding masks.\n        matching: How to match predicted boxes to ground truth boxes. For \"hungarian\", computes\n            assignment that maximizes total IoU between all boxes. For \"best_overlap\", uses the\n            predicted box with maximum overlap for each ground truth box (each predicted box\n            can be assigned to multiple ground truth boxes).\n        compute_discovery_fraction: Instead of the IoU, compute the fraction of ground truth classes\n            that were \"discovered\", meaning that they have an IoU greater than some threshold. This\n            is recall, or sometimes called the detection rate metric.\n        correct_localization: Instead of the IoU, compute the fraction of images on which at least\n            one ground truth bounding box was correctly localised, meaning that they have an IoU\n            greater than some threshold.\n        discovery_threshold: Minimum IoU to count a class as discovered/correctly localized.\n    \"\"\"\ndef __init__(\nself,\ntarget_is_mask: bool = False,\nuse_threshold: bool = False,\nthreshold: float = 0.5,\nmatching: str = \"hungarian\",\ncompute_discovery_fraction: bool = False,\ncorrect_localization: bool = False,\ndiscovery_threshold: float = 0.5,\n):\nsuper().__init__()\nself.target_is_mask = target_is_mask\nself.use_threshold = use_threshold\nself.threshold = threshold\nself.discovery_threshold = discovery_threshold\nself.compute_discovery_fraction = compute_discovery_fraction\nself.correct_localization = correct_localization\nif compute_discovery_fraction and correct_localization:\nraise ValueError(\n\"Only one of `compute_discovery_fraction` and `correct_localization` can be enabled.\"\n)\nmatchings = (\"hungarian\", \"best_overlap\")\nif matching not in matchings:\nraise ValueError(f\"Unknown matching type {matching}. Valid values are {matchings}.\")\nself.matching = matching\nself.add_state(\n\"values\", default=torch.tensor(0.0, dtype=torch.float64), dist_reduce_fx=\"sum\"\n)\nself.add_state(\"total\", default=torch.tensor(0), dist_reduce_fx=\"sum\")\ndef update(self, prediction: torch.Tensor, target: torch.Tensor):\n\"\"\"Update this metric.\n        Args:\n            prediction: Predicted mask of shape (B, C, H, W) or (B, F, C, H, W), where C is the\n                number of instances. Assumes class probabilities as inputs.\n            target: Ground truth mask of shape (B, K, H, W) or (B, F, K, H, W), where K is the\n                number of instance, if using masks as input, or bounding boxes of shape (B, K, 4)\n                or (B, F, K, 4).\n        \"\"\"\nif prediction.ndim == 5:\n# Merge batch and frame dimensions\nprediction = prediction.flatten(0, 1)\ntarget = target.flatten(0, 1)\nelif prediction.ndim != 4:\nraise ValueError(f\"Incorrect input shape: f{prediction.shape}\")\nbs, n_pred_classes = prediction.shape[:2]\nn_gt_classes = target.shape[1]\nif self.use_threshold:\nprediction = prediction &gt; self.threshold\nelse:\nindices = torch.argmax(prediction, dim=1)\nprediction = torch.nn.functional.one_hot(indices, num_classes=n_pred_classes)\nprediction = prediction.permute(0, 3, 1, 2)\npred_bboxes = masks_to_bboxes(prediction.flatten(0, 1)).unflatten(0, (bs, n_pred_classes))\nif self.target_is_mask:\ntarget_bboxes = masks_to_bboxes(target.flatten(0, 1)).unflatten(0, (bs, n_gt_classes))\nelse:\nassert target.shape[-1] == 4\n# Convert all-zero boxes added during padding to invalid boxes\ntarget[torch.all(target == 0.0, dim=-1)] = -1.0\ntarget_bboxes = target\nfor pred, target in zip(pred_bboxes, target_bboxes):\nvalid_pred_bboxes = pred[:, 0] != -1.0\nvalid_target_bboxes = target[:, 0] != -1.0\nif valid_target_bboxes.sum() == 0:\ncontinue  # Skip data points without any target bbox\npred = pred[valid_pred_bboxes]\ntarget = target[valid_target_bboxes]\nif valid_pred_bboxes.sum() &gt; 0:\niou_per_bbox = unsupervised_bbox_iou(\npred, target, matching=self.matching, reduction=\"none\"\n)\nelse:\niou_per_bbox = torch.zeros_like(valid_target_bboxes, dtype=torch.float32)\nif self.compute_discovery_fraction:\ndiscovered = iou_per_bbox &gt; self.discovery_threshold\nself.values += discovered.sum() / len(iou_per_bbox)\nelif self.correct_localization:\ncorrectly_localized = torch.any(iou_per_bbox &gt; self.discovery_threshold)\nself.values += correctly_localized.sum()\nelse:\nself.values += iou_per_bbox.mean()\nself.total += 1\ndef compute(self) -&gt; torch.Tensor:\nif self.total == 0:\nreturn torch.zeros_like(self.values)\nelse:\nreturn self.values / self.total\n</code></pre>"},{"location":"api/ocl/metrics/bbox/#ocl.metrics.bbox.UnsupervisedBboxIoUMetric.update","title":"<code>update</code>","text":"<p>Update this metric.</p> <p>Parameters:</p> Name Type Description Default <code>prediction</code> <code>torch.Tensor</code> <p>Predicted mask of shape (B, C, H, W) or (B, F, C, H, W), where C is the number of instances. Assumes class probabilities as inputs.</p> required <code>target</code> <code>torch.Tensor</code> <p>Ground truth mask of shape (B, K, H, W) or (B, F, K, H, W), where K is the number of instance, if using masks as input, or bounding boxes of shape (B, K, 4) or (B, F, K, 4).</p> required Source code in <code>ocl/metrics/bbox.py</code> <pre><code>def update(self, prediction: torch.Tensor, target: torch.Tensor):\n\"\"\"Update this metric.\n    Args:\n        prediction: Predicted mask of shape (B, C, H, W) or (B, F, C, H, W), where C is the\n            number of instances. Assumes class probabilities as inputs.\n        target: Ground truth mask of shape (B, K, H, W) or (B, F, K, H, W), where K is the\n            number of instance, if using masks as input, or bounding boxes of shape (B, K, 4)\n            or (B, F, K, 4).\n    \"\"\"\nif prediction.ndim == 5:\n# Merge batch and frame dimensions\nprediction = prediction.flatten(0, 1)\ntarget = target.flatten(0, 1)\nelif prediction.ndim != 4:\nraise ValueError(f\"Incorrect input shape: f{prediction.shape}\")\nbs, n_pred_classes = prediction.shape[:2]\nn_gt_classes = target.shape[1]\nif self.use_threshold:\nprediction = prediction &gt; self.threshold\nelse:\nindices = torch.argmax(prediction, dim=1)\nprediction = torch.nn.functional.one_hot(indices, num_classes=n_pred_classes)\nprediction = prediction.permute(0, 3, 1, 2)\npred_bboxes = masks_to_bboxes(prediction.flatten(0, 1)).unflatten(0, (bs, n_pred_classes))\nif self.target_is_mask:\ntarget_bboxes = masks_to_bboxes(target.flatten(0, 1)).unflatten(0, (bs, n_gt_classes))\nelse:\nassert target.shape[-1] == 4\n# Convert all-zero boxes added during padding to invalid boxes\ntarget[torch.all(target == 0.0, dim=-1)] = -1.0\ntarget_bboxes = target\nfor pred, target in zip(pred_bboxes, target_bboxes):\nvalid_pred_bboxes = pred[:, 0] != -1.0\nvalid_target_bboxes = target[:, 0] != -1.0\nif valid_target_bboxes.sum() == 0:\ncontinue  # Skip data points without any target bbox\npred = pred[valid_pred_bboxes]\ntarget = target[valid_target_bboxes]\nif valid_pred_bboxes.sum() &gt; 0:\niou_per_bbox = unsupervised_bbox_iou(\npred, target, matching=self.matching, reduction=\"none\"\n)\nelse:\niou_per_bbox = torch.zeros_like(valid_target_bboxes, dtype=torch.float32)\nif self.compute_discovery_fraction:\ndiscovered = iou_per_bbox &gt; self.discovery_threshold\nself.values += discovered.sum() / len(iou_per_bbox)\nelif self.correct_localization:\ncorrectly_localized = torch.any(iou_per_bbox &gt; self.discovery_threshold)\nself.values += correctly_localized.sum()\nelse:\nself.values += iou_per_bbox.mean()\nself.total += 1\n</code></pre>"},{"location":"api/ocl/metrics/bbox/#ocl.metrics.bbox.unsupervised_bbox_iou","title":"<code>unsupervised_bbox_iou</code>","text":"<p>Compute IoU between two sets of bounding boxes.</p> <p>Parameters:</p> Name Type Description Default <code>pred_bboxes</code> <code>torch.Tensor</code> <p>Predicted bounding boxes of shape N x 4.</p> required <code>true_bboxes</code> <code>torch.Tensor</code> <p>True bounding boxes of shape M x 4.</p> required <code>matching</code> <code>str</code> <p>Method to assign predicted to true bounding boxes.</p> <code>'best_overlap'</code> <code>reduction</code> <code>str</code> <p>Whether to average the computes IoUs per true box.</p> <code>'mean'</code> Source code in <code>ocl/metrics/bbox.py</code> <pre><code>def unsupervised_bbox_iou(\npred_bboxes: torch.Tensor,\ntrue_bboxes: torch.Tensor,\nmatching: str = \"best_overlap\",\nreduction: str = \"mean\",\n) -&gt; torch.Tensor:\n\"\"\"Compute IoU between two sets of bounding boxes.\n    Args:\n        pred_bboxes: Predicted bounding boxes of shape N x 4.\n        true_bboxes: True bounding boxes of shape M x 4.\n        matching: Method to assign predicted to true bounding boxes.\n        reduction: Whether to average the computes IoUs per true box.\n    \"\"\"\nn_gt_bboxes = len(true_bboxes)\npairwise_iou = torchvision.ops.box_iou(pred_bboxes, true_bboxes)\nif matching == \"hungarian\":\npred_idxs, true_idxs = scipy.optimize.linear_sum_assignment(\npairwise_iou.cpu(), maximize=True\n)\npred_idxs = torch.as_tensor(pred_idxs, dtype=torch.int64, device=pairwise_iou.device)\ntrue_idxs = torch.as_tensor(true_idxs, dtype=torch.int64, device=pairwise_iou.device)\nelif matching == \"best_overlap\":\npred_idxs = torch.argmax(pairwise_iou, dim=0)\ntrue_idxs = torch.arange(pairwise_iou.shape[1], device=pairwise_iou.device)\nelse:\nraise ValueError(f\"Unknown matching {matching}\")\nmatched_iou = pairwise_iou[pred_idxs, true_idxs]\niou = torch.zeros(n_gt_bboxes, dtype=torch.float32, device=pairwise_iou.device)\niou[true_idxs] = matched_iou\nif reduction == \"mean\":\nreturn iou.mean()\nelse:\nreturn iou\n</code></pre>"},{"location":"api/ocl/metrics/dataset/","title":"ocl.metrics.dataset","text":"<p>Metrics computed on a whole dataset.</p>"},{"location":"api/ocl/metrics/dataset/#ocl.metrics.dataset.DatasetSemanticMaskIoUMetric","title":"<code>DatasetSemanticMaskIoUMetric</code>","text":"<p>         Bases: <code>torchmetrics.Metric</code></p> <p>Unsupervised IoU metric for semantic segmentation using dataset-wide matching of classes.</p> <p>The input to this metric is an instance-level mask with objects, and a class id for each object. This is required to convert the mask to semantic classes. The number of classes for the predictions does not have to match the true number of classes.</p> <p>Note that contrary to the other metrics in this module, this metric is not supposed to be added in the online metric computation loop, which is why it does not inherit from <code>RoutableMixin</code>.</p> <p>Parameters:</p> Name Type Description Default <code>n_predicted_classes</code> <code>int</code> <p>Number of predictable classes, i.e. highest prediction class id that can occur.</p> required <code>n_classes</code> <code>int</code> <p>Total number of classes, i.e. highest class id that can occur.</p> required <code>threshold</code> <code>float</code> <p>Value to use for thresholding masks.</p> <code>0.5</code> <code>use_threshold</code> <code>bool</code> <p>If <code>True</code>, convert predicted class probabilities to mask using a threshold. If <code>False</code>, class probabilities are turned into mask using an argmax instead.</p> <code>False</code> <code>matching</code> <code>str</code> <p>Method to produce matching between clusters and ground truth classes. If \"hungarian\", assigns each class one cluster such that the total IoU is maximized. If \"majority\", assigns each cluster to the class with the highest IoU (each class can be assigned multiple clusters).</p> <code>'hungarian'</code> <code>ignore_background</code> <code>bool</code> <p>If true, pixels labeled as background (class zero) in the ground truth are not taken into account when computing IoU.</p> <code>False</code> <code>use_unmatched_as_background</code> <code>bool</code> <p>If true, count predicted classes not selected after Hungarian matching as the background predictions.</p> <code>False</code> Source code in <code>ocl/metrics/dataset.py</code> <pre><code>class DatasetSemanticMaskIoUMetric(torchmetrics.Metric):\n\"\"\"Unsupervised IoU metric for semantic segmentation using dataset-wide matching of classes.\n    The input to this metric is an instance-level mask with objects, and a class id for each object.\n    This is required to convert the mask to semantic classes. The number of classes for the\n    predictions does not have to match the true number of classes.\n    Note that contrary to the other metrics in this module, this metric is not supposed to be added\n    in the online metric computation loop, which is why it does not inherit from `RoutableMixin`.\n    Args:\n        n_predicted_classes: Number of predictable classes, i.e. highest prediction class id that can\n            occur.\n        n_classes: Total number of classes, i.e. highest class id that can occur.\n        threshold: Value to use for thresholding masks.\n        use_threshold: If `True`, convert predicted class probabilities to mask using a threshold.\n            If `False`, class probabilities are turned into mask using an argmax instead.\n        matching: Method to produce matching between clusters and ground truth classes. If\n            \"hungarian\", assigns each class one cluster such that the total IoU is maximized. If\n            \"majority\", assigns each cluster to the class with the highest IoU (each class can be\n            assigned multiple clusters).\n        ignore_background: If true, pixels labeled as background (class zero) in the ground truth\n            are not taken into account when computing IoU.\n        use_unmatched_as_background: If true, count predicted classes not selected after Hungarian\n            matching as the background predictions.\n    \"\"\"\ndef __init__(\nself,\nn_predicted_classes: int,\nn_classes: int,\nuse_threshold: bool = False,\nthreshold: float = 0.5,\nmatching: str = \"hungarian\",\nignore_background: bool = False,\nuse_unmatched_as_background: bool = False,\n):\nsuper().__init__()\nmatching_methods = {\"hungarian\", \"majority\"}\nif matching not in matching_methods:\nraise ValueError(\nf\"Unknown matching method {matching}. Valid values are {matching_methods}.\"\n)\nself.matching = matching\nself.n_predicted_classes = n_predicted_classes\nself.n_predicted_classes_with_bg = n_predicted_classes + 1\nself.n_classes = n_classes\nself.n_classes_with_bg = n_classes + 1\nself.matching = matching\nself.use_threshold = use_threshold\nself.threshold = threshold\nself.ignore_background = ignore_background\nself.use_unmatched_as_background = use_unmatched_as_background\nif use_unmatched_as_background and ignore_background:\nraise ValueError(\n\"Option `use_unmatched_as_background` not compatible with option `ignore_background`\"\n)\nif use_unmatched_as_background and matching == \"majority\":\nraise ValueError(\n\"Option `use_unmatched_as_background` not compatible with matching `majority`\"\n)\nconfusion_mat = torch.zeros(\nself.n_predicted_classes_with_bg, self.n_classes_with_bg, dtype=torch.int64\n)\nself.add_state(\"confusion_mat\", default=confusion_mat, dist_reduce_fx=\"sum\", persistent=True)\ndef update(\nself,\npredictions: torch.Tensor,\ntargets: torch.Tensor,\nprediction_class_ids: torch.Tensor,\nignore: Optional[torch.Tensor] = None,\n):\n\"\"\"Update metric by computing confusion matrix between predicted and target classes.\n        Args:\n            predictions: Probability mask of shape (B, K, H, W) or (B, F, K, H, W), where K is the\n                number of object instances in the image.\n            targets: Mask of shape (B, K, H, W) or (B, F, K, H, W), where K is the number of object\n                instances in the image. Class ID of objects is encoded as the value, i.e. densely\n                represented.\n            prediction_class_ids: Tensor of shape (B, K), containing the class id of each predicted\n                object instance in the image. Id must be 0 &lt;= id &lt;= n_predicted_classes.\n            ignore: Ignore mask of shape (B, 1, H, W) or (B, 1, K, H, W)\n        \"\"\"\npredictions = self.preprocess_predicted_mask(predictions)\npredictions = _remap_one_hot_mask(\npredictions, prediction_class_ids, self.n_predicted_classes, strip_empty=False\n)\nassert predictions.shape[-1] == self.n_predicted_classes_with_bg\ntargets = self.preprocess_ground_truth_mask(targets)\nassert targets.shape[-1] == self.n_classes_with_bg\nif ignore is not None:\nif ignore.ndim == 5:  # Video case\nignore = ignore.flatten(0, 1)\nassert ignore.ndim == 4 and ignore.shape[1] == 1\nignore = ignore.to(torch.bool).flatten(-2, -1).squeeze(1)  # B x P\npredictions[ignore] = 0\ntargets[ignore] = 0\n# We are doing the multiply in float64 instead of int64 because it proved to be significantly\n# faster on GPU. We need to use 64 bits because we can easily exceed the range of 32 bits\n# if we aggregate over a full dataset.\nconfusion_mat = torch.einsum(\n\"bpk,bpc-&gt;kc\", predictions.to(torch.float64), targets.to(torch.float64)\n)\nself.confusion_mat += confusion_mat.to(torch.int64)\ndef preprocess_predicted_mask(self, mask: torch.Tensor) -&gt; torch.Tensor:\n\"\"\"Preprocess predicted masks for metric computation.\n        Args:\n            mask: Probability mask of shape (B, K, H, W) or (B, F, K, H, W), where K is the number\n                of object instances in the prediction.\n        Returns:\n            Binary tensor of shape (B, P, K), where P is the number of points. If `use_threshold` is\n            True, overlapping objects for the same point are possible.\n        \"\"\"\nif mask.ndim == 5:  # Video case\nmask = mask.flatten(0, 1)\nmask = mask.flatten(-2, -1)\nif self.use_threshold:\nmask = mask &gt; self.threshold\nmask = mask.transpose(1, 2)\nelse:\nmaximum, indices = torch.max(mask, dim=1)\nmask = torch.nn.functional.one_hot(indices, num_classes=mask.shape[1])\nmask[:, :, 0][maximum == 0.0] = 0\nreturn mask\ndef preprocess_ground_truth_mask(self, mask: torch.Tensor) -&gt; torch.Tensor:\n\"\"\"Preprocess ground truth mask for metric computation.\n        Args:\n            mask: Mask of shape (B, K, H, W) or (B, F, K, H, W), where K is the number of object\n                instances in the image. Class ID of objects is encoded as the value, i.e. densely\n                represented.\n        Returns:\n            One-hot tensor of shape (B, P, J), where J is the number of the classes and P the number\n            of points, with object instances with the same class ID merged together. In the case of\n            an overlap of classes for a point, the class with the highest ID is assigned to that\n            point.\n        \"\"\"\nif mask.ndim == 5:  # Video case\nmask = mask.flatten(0, 1)\nmask = mask.flatten(-2, -1)\n# Pixels which contain no object get assigned the background class 0. This also handles the\n# padding of zero masks which is done in preprocessing for batching.\nmask = torch.nn.functional.one_hot(\nmask.max(dim=1).values.to(torch.long), num_classes=self.n_classes_with_bg\n)\nreturn mask\ndef compute(self):\n\"\"\"Compute per-class IoU using matching.\"\"\"\nif self.ignore_background:\nn_classes = self.n_classes\nconfusion_mat = self.confusion_mat[:, 1:]\nelse:\nn_classes = self.n_classes_with_bg\nconfusion_mat = self.confusion_mat\npairwise_iou, _, _, area_gt = self._compute_iou_from_confusion_mat(confusion_mat)\nif self.use_unmatched_as_background:\n# Match only in foreground\npairwise_iou = pairwise_iou[1:, 1:]\nconfusion_mat = confusion_mat[1:, 1:]\nelse:\n# Predicted class zero is not matched against anything\npairwise_iou = pairwise_iou[1:]\nconfusion_mat = confusion_mat[1:]\nif self.matching == \"hungarian\":\ncluster_idxs, class_idxs = scipy.optimize.linear_sum_assignment(\npairwise_iou.cpu(), maximize=True\n)\ncluster_idxs = torch.as_tensor(\ncluster_idxs, dtype=torch.int64, device=self.confusion_mat.device\n)\nclass_idxs = torch.as_tensor(\nclass_idxs, dtype=torch.int64, device=self.confusion_mat.device\n)\nmatched_iou = pairwise_iou[cluster_idxs, class_idxs]\ntrue_pos = confusion_mat[cluster_idxs, class_idxs]\nif self.use_unmatched_as_background:\ncluster_oh = torch.nn.functional.one_hot(\ncluster_idxs, num_classes=pairwise_iou.shape[0]\n)\nmatched_clusters = cluster_oh.max(dim=0).values.to(torch.bool)\nbg_pred = self.confusion_mat[:1]\nbg_pred += self.confusion_mat[1:][~matched_clusters].sum(dim=0)\nbg_iou, _, _, _ = self._compute_iou_from_confusion_mat(bg_pred, area_gt)\nclass_idxs = torch.cat((torch.zeros_like(class_idxs[:1]), class_idxs + 1))\nmatched_iou = torch.cat((bg_iou[0, :1], matched_iou))\ntrue_pos = torch.cat((bg_pred[0, :1], true_pos))\nelif self.matching == \"majority\":\nmax_iou, class_idxs = torch.max(pairwise_iou, dim=1)\n# Form new clusters by merging old clusters which are assigned the same ground truth\n# class. After merging, the number of clusters equals the number of classes.\n_, old_to_new_cluster_idx = torch.unique(class_idxs, return_inverse=True)\nconfusion_mat_new = torch.zeros(\nn_classes, n_classes, dtype=torch.int64, device=self.confusion_mat.device\n)\nfor old_cluster_idx, new_cluster_idx in enumerate(old_to_new_cluster_idx):\nif max_iou[old_cluster_idx] &gt; 0.0:\nconfusion_mat_new[new_cluster_idx] += confusion_mat[old_cluster_idx]\n# Important: use previously computed area_gt because it includes background predictions,\n# whereas the new confusion matrix does not contain the bg predicted class anymore.\npairwise_iou, _, _, _ = self._compute_iou_from_confusion_mat(confusion_mat_new, area_gt)\nmax_iou, class_idxs = torch.max(pairwise_iou, dim=1)\nvalid = max_iou &gt; 0.0  # Ignore clusters without any kind of overlap\nclass_idxs = class_idxs[valid]\ncluster_idxs = torch.arange(pairwise_iou.shape[1])[valid]\nmatched_iou = pairwise_iou[cluster_idxs, class_idxs]\ntrue_pos = confusion_mat_new[cluster_idxs, class_idxs]\nelse:\nraise RuntimeError(f\"Unsupported matching: {self.matching}\")\niou = torch.zeros(n_classes, dtype=torch.float64, device=pairwise_iou.device)\niou[class_idxs] = matched_iou\naccuracy = true_pos.sum().to(torch.float64) / area_gt.sum()\nempty_classes = area_gt == 0\nreturn iou, accuracy, empty_classes\n@staticmethod\ndef _compute_iou_from_confusion_mat(\nconfusion_mat: torch.Tensor, area_gt: Optional[torch.Tensor] = None\n):\narea_pred = torch.sum(confusion_mat, axis=1)\nif area_gt is None:\narea_gt = torch.sum(confusion_mat, axis=0)\nunion = area_pred.unsqueeze(1) + area_gt.unsqueeze(0) - confusion_mat\npairwise_iou = confusion_mat.to(torch.float64) / union\n# Ignore classes that occured on no image.\npairwise_iou[union == 0] = 0.0\nreturn pairwise_iou, union, area_pred, area_gt\n</code></pre>"},{"location":"api/ocl/metrics/dataset/#ocl.metrics.dataset.DatasetSemanticMaskIoUMetric.update","title":"<code>update</code>","text":"<p>Update metric by computing confusion matrix between predicted and target classes.</p> <p>Parameters:</p> Name Type Description Default <code>predictions</code> <code>torch.Tensor</code> <p>Probability mask of shape (B, K, H, W) or (B, F, K, H, W), where K is the number of object instances in the image.</p> required <code>targets</code> <code>torch.Tensor</code> <p>Mask of shape (B, K, H, W) or (B, F, K, H, W), where K is the number of object instances in the image. Class ID of objects is encoded as the value, i.e. densely represented.</p> required <code>prediction_class_ids</code> <code>torch.Tensor</code> <p>Tensor of shape (B, K), containing the class id of each predicted object instance in the image. Id must be 0 &lt;= id &lt;= n_predicted_classes.</p> required <code>ignore</code> <code>Optional[torch.Tensor]</code> <p>Ignore mask of shape (B, 1, H, W) or (B, 1, K, H, W)</p> <code>None</code> Source code in <code>ocl/metrics/dataset.py</code> <pre><code>def update(\nself,\npredictions: torch.Tensor,\ntargets: torch.Tensor,\nprediction_class_ids: torch.Tensor,\nignore: Optional[torch.Tensor] = None,\n):\n\"\"\"Update metric by computing confusion matrix between predicted and target classes.\n    Args:\n        predictions: Probability mask of shape (B, K, H, W) or (B, F, K, H, W), where K is the\n            number of object instances in the image.\n        targets: Mask of shape (B, K, H, W) or (B, F, K, H, W), where K is the number of object\n            instances in the image. Class ID of objects is encoded as the value, i.e. densely\n            represented.\n        prediction_class_ids: Tensor of shape (B, K), containing the class id of each predicted\n            object instance in the image. Id must be 0 &lt;= id &lt;= n_predicted_classes.\n        ignore: Ignore mask of shape (B, 1, H, W) or (B, 1, K, H, W)\n    \"\"\"\npredictions = self.preprocess_predicted_mask(predictions)\npredictions = _remap_one_hot_mask(\npredictions, prediction_class_ids, self.n_predicted_classes, strip_empty=False\n)\nassert predictions.shape[-1] == self.n_predicted_classes_with_bg\ntargets = self.preprocess_ground_truth_mask(targets)\nassert targets.shape[-1] == self.n_classes_with_bg\nif ignore is not None:\nif ignore.ndim == 5:  # Video case\nignore = ignore.flatten(0, 1)\nassert ignore.ndim == 4 and ignore.shape[1] == 1\nignore = ignore.to(torch.bool).flatten(-2, -1).squeeze(1)  # B x P\npredictions[ignore] = 0\ntargets[ignore] = 0\n# We are doing the multiply in float64 instead of int64 because it proved to be significantly\n# faster on GPU. We need to use 64 bits because we can easily exceed the range of 32 bits\n# if we aggregate over a full dataset.\nconfusion_mat = torch.einsum(\n\"bpk,bpc-&gt;kc\", predictions.to(torch.float64), targets.to(torch.float64)\n)\nself.confusion_mat += confusion_mat.to(torch.int64)\n</code></pre>"},{"location":"api/ocl/metrics/dataset/#ocl.metrics.dataset.DatasetSemanticMaskIoUMetric.preprocess_predicted_mask","title":"<code>preprocess_predicted_mask</code>","text":"<p>Preprocess predicted masks for metric computation.</p> <p>Parameters:</p> Name Type Description Default <code>mask</code> <code>torch.Tensor</code> <p>Probability mask of shape (B, K, H, W) or (B, F, K, H, W), where K is the number of object instances in the prediction.</p> required <p>Returns:</p> Type Description <code>torch.Tensor</code> <p>Binary tensor of shape (B, P, K), where P is the number of points. If <code>use_threshold</code> is</p> <code>torch.Tensor</code> <p>True, overlapping objects for the same point are possible.</p> Source code in <code>ocl/metrics/dataset.py</code> <pre><code>def preprocess_predicted_mask(self, mask: torch.Tensor) -&gt; torch.Tensor:\n\"\"\"Preprocess predicted masks for metric computation.\n    Args:\n        mask: Probability mask of shape (B, K, H, W) or (B, F, K, H, W), where K is the number\n            of object instances in the prediction.\n    Returns:\n        Binary tensor of shape (B, P, K), where P is the number of points. If `use_threshold` is\n        True, overlapping objects for the same point are possible.\n    \"\"\"\nif mask.ndim == 5:  # Video case\nmask = mask.flatten(0, 1)\nmask = mask.flatten(-2, -1)\nif self.use_threshold:\nmask = mask &gt; self.threshold\nmask = mask.transpose(1, 2)\nelse:\nmaximum, indices = torch.max(mask, dim=1)\nmask = torch.nn.functional.one_hot(indices, num_classes=mask.shape[1])\nmask[:, :, 0][maximum == 0.0] = 0\nreturn mask\n</code></pre>"},{"location":"api/ocl/metrics/dataset/#ocl.metrics.dataset.DatasetSemanticMaskIoUMetric.preprocess_ground_truth_mask","title":"<code>preprocess_ground_truth_mask</code>","text":"<p>Preprocess ground truth mask for metric computation.</p> <p>Parameters:</p> Name Type Description Default <code>mask</code> <code>torch.Tensor</code> <p>Mask of shape (B, K, H, W) or (B, F, K, H, W), where K is the number of object instances in the image. Class ID of objects is encoded as the value, i.e. densely represented.</p> required <p>Returns:</p> Type Description <code>torch.Tensor</code> <p>One-hot tensor of shape (B, P, J), where J is the number of the classes and P the number</p> <code>torch.Tensor</code> <p>of points, with object instances with the same class ID merged together. In the case of</p> <code>torch.Tensor</code> <p>an overlap of classes for a point, the class with the highest ID is assigned to that</p> <code>torch.Tensor</code> <p>point.</p> Source code in <code>ocl/metrics/dataset.py</code> <pre><code>def preprocess_ground_truth_mask(self, mask: torch.Tensor) -&gt; torch.Tensor:\n\"\"\"Preprocess ground truth mask for metric computation.\n    Args:\n        mask: Mask of shape (B, K, H, W) or (B, F, K, H, W), where K is the number of object\n            instances in the image. Class ID of objects is encoded as the value, i.e. densely\n            represented.\n    Returns:\n        One-hot tensor of shape (B, P, J), where J is the number of the classes and P the number\n        of points, with object instances with the same class ID merged together. In the case of\n        an overlap of classes for a point, the class with the highest ID is assigned to that\n        point.\n    \"\"\"\nif mask.ndim == 5:  # Video case\nmask = mask.flatten(0, 1)\nmask = mask.flatten(-2, -1)\n# Pixels which contain no object get assigned the background class 0. This also handles the\n# padding of zero masks which is done in preprocessing for batching.\nmask = torch.nn.functional.one_hot(\nmask.max(dim=1).values.to(torch.long), num_classes=self.n_classes_with_bg\n)\nreturn mask\n</code></pre>"},{"location":"api/ocl/metrics/dataset/#ocl.metrics.dataset.DatasetSemanticMaskIoUMetric.compute","title":"<code>compute</code>","text":"<p>Compute per-class IoU using matching.</p> Source code in <code>ocl/metrics/dataset.py</code> <pre><code>def compute(self):\n\"\"\"Compute per-class IoU using matching.\"\"\"\nif self.ignore_background:\nn_classes = self.n_classes\nconfusion_mat = self.confusion_mat[:, 1:]\nelse:\nn_classes = self.n_classes_with_bg\nconfusion_mat = self.confusion_mat\npairwise_iou, _, _, area_gt = self._compute_iou_from_confusion_mat(confusion_mat)\nif self.use_unmatched_as_background:\n# Match only in foreground\npairwise_iou = pairwise_iou[1:, 1:]\nconfusion_mat = confusion_mat[1:, 1:]\nelse:\n# Predicted class zero is not matched against anything\npairwise_iou = pairwise_iou[1:]\nconfusion_mat = confusion_mat[1:]\nif self.matching == \"hungarian\":\ncluster_idxs, class_idxs = scipy.optimize.linear_sum_assignment(\npairwise_iou.cpu(), maximize=True\n)\ncluster_idxs = torch.as_tensor(\ncluster_idxs, dtype=torch.int64, device=self.confusion_mat.device\n)\nclass_idxs = torch.as_tensor(\nclass_idxs, dtype=torch.int64, device=self.confusion_mat.device\n)\nmatched_iou = pairwise_iou[cluster_idxs, class_idxs]\ntrue_pos = confusion_mat[cluster_idxs, class_idxs]\nif self.use_unmatched_as_background:\ncluster_oh = torch.nn.functional.one_hot(\ncluster_idxs, num_classes=pairwise_iou.shape[0]\n)\nmatched_clusters = cluster_oh.max(dim=0).values.to(torch.bool)\nbg_pred = self.confusion_mat[:1]\nbg_pred += self.confusion_mat[1:][~matched_clusters].sum(dim=0)\nbg_iou, _, _, _ = self._compute_iou_from_confusion_mat(bg_pred, area_gt)\nclass_idxs = torch.cat((torch.zeros_like(class_idxs[:1]), class_idxs + 1))\nmatched_iou = torch.cat((bg_iou[0, :1], matched_iou))\ntrue_pos = torch.cat((bg_pred[0, :1], true_pos))\nelif self.matching == \"majority\":\nmax_iou, class_idxs = torch.max(pairwise_iou, dim=1)\n# Form new clusters by merging old clusters which are assigned the same ground truth\n# class. After merging, the number of clusters equals the number of classes.\n_, old_to_new_cluster_idx = torch.unique(class_idxs, return_inverse=True)\nconfusion_mat_new = torch.zeros(\nn_classes, n_classes, dtype=torch.int64, device=self.confusion_mat.device\n)\nfor old_cluster_idx, new_cluster_idx in enumerate(old_to_new_cluster_idx):\nif max_iou[old_cluster_idx] &gt; 0.0:\nconfusion_mat_new[new_cluster_idx] += confusion_mat[old_cluster_idx]\n# Important: use previously computed area_gt because it includes background predictions,\n# whereas the new confusion matrix does not contain the bg predicted class anymore.\npairwise_iou, _, _, _ = self._compute_iou_from_confusion_mat(confusion_mat_new, area_gt)\nmax_iou, class_idxs = torch.max(pairwise_iou, dim=1)\nvalid = max_iou &gt; 0.0  # Ignore clusters without any kind of overlap\nclass_idxs = class_idxs[valid]\ncluster_idxs = torch.arange(pairwise_iou.shape[1])[valid]\nmatched_iou = pairwise_iou[cluster_idxs, class_idxs]\ntrue_pos = confusion_mat_new[cluster_idxs, class_idxs]\nelse:\nraise RuntimeError(f\"Unsupported matching: {self.matching}\")\niou = torch.zeros(n_classes, dtype=torch.float64, device=pairwise_iou.device)\niou[class_idxs] = matched_iou\naccuracy = true_pos.sum().to(torch.float64) / area_gt.sum()\nempty_classes = area_gt == 0\nreturn iou, accuracy, empty_classes\n</code></pre>"},{"location":"api/ocl/metrics/dataset/#ocl.metrics.dataset.SklearnClustering","title":"<code>SklearnClustering</code>","text":"<p>Wrapper around scikit-learn clustering algorithms.</p> <p>Parameters:</p> Name Type Description Default <code>n_clusters</code> <code>int</code> <p>Number of clusters.</p> required <code>method</code> <code>str</code> <p>Clustering method to use.</p> <code>'kmeans'</code> <code>clustering_kwargs</code> <code>Optional[Dict[str, Any]]</code> <p>Dictionary of additional keyword arguments to pass to clustering object.</p> <code>None</code> <code>use_l2_normalization</code> <code>bool</code> <p>Whether to L2 normalize the representations before clustering (but after PCA).</p> <code>False</code> <code>use_pca</code> <code>bool</code> <p>Whether to apply PCA before fitting the clusters.</p> <code>False</code> <code>pca_dimensions</code> <code>Optional[int]</code> <p>Number of dimensions for PCA dimensionality reduction. If <code>None</code>, do not reduce dimensions with PCA.</p> <code>None</code> <code>pca_kwargs</code> <code>Optional[Dict[str, Any]]</code> <p>Dictionary of additional keyword arguments to pass to PCA object.</p> <code>None</code> Source code in <code>ocl/metrics/dataset.py</code> <pre><code>class SklearnClustering:\n\"\"\"Wrapper around scikit-learn clustering algorithms.\n    Args:\n        n_clusters: Number of clusters.\n        method: Clustering method to use.\n        clustering_kwargs: Dictionary of additional keyword arguments to pass to clustering object.\n        use_l2_normalization: Whether to L2 normalize the representations before clustering (but\n            after PCA).\n        use_pca: Whether to apply PCA before fitting the clusters.\n        pca_dimensions: Number of dimensions for PCA dimensionality reduction. If `None`, do not\n            reduce dimensions with PCA.\n        pca_kwargs: Dictionary of additional keyword arguments to pass to PCA object.\n    \"\"\"\ndef __init__(\nself,\nn_clusters: int,\nmethod: str = \"kmeans\",\nclustering_kwargs: Optional[Dict[str, Any]] = None,\nuse_l2_normalization: bool = False,\nuse_pca: bool = False,\npca_dimensions: Optional[int] = None,\npca_kwargs: Optional[Dict[str, Any]] = None,\n):\nmethods = (\"kmeans\", \"spectral\")\nif method not in methods:\nraise ValueError(f\"Unknown clustering method {method}. Valid values are {methods}.\")\nself._n_clusters = n_clusters\nself.method = method\nself.clustering_kwargs = clustering_kwargs\nself.use_l2_normalization = use_l2_normalization\nself.use_pca = use_pca\nself.pca_dimensions = pca_dimensions\nself.pca_kwargs = pca_kwargs\nself._clustering = None\nself._pca = None\n@property\ndef n_clusters(self):\nreturn self._n_clusters\ndef _init(self):\nfrom sklearn import cluster, decomposition\nkwargs = self.clustering_kwargs if self.clustering_kwargs is not None else {}\nif self.method == \"kmeans\":\nself._clustering = cluster.KMeans(n_clusters=self.n_clusters, **kwargs)\nelif self.method == \"spectral\":\nself._clustering = cluster.SpectralClustering(n_clusters=self.n_clusters, **kwargs)\nelse:\nraise NotImplementedError(f\"Clustering {self.method} not implemented.\")\nif self.use_pca:\nkwargs = self.pca_kwargs if self.pca_kwargs is not None else {}\nself._pca = decomposition.PCA(n_components=self.pca_dimensions, **kwargs)\ndef fit_predict(self, features: torch.Tensor):\nself._init()\nfeatures = features.detach().cpu().numpy()\nif self.use_pca:\nfeatures = self._pca.fit_transform(features)\nif self.use_l2_normalization:\nfeatures /= np.maximum(np.linalg.norm(features, ord=2, axis=1, keepdims=True), 1e-8)\ncluster_ids = self._clustering.fit_predict(features).astype(np.int64)\nreturn torch.from_numpy(cluster_ids)\ndef predict(self, features: torch.Tensor) -&gt; torch.Tensor:\nif self._clustering is None:\nraise ValueError(\"Clustering was not fitted. Call `fit_predict` first.\")\nfeatures = features.detach().cpu().numpy()\nif self.use_pca:\nfeatures = self._pca.transform(features)\nif self.use_l2_normalization:\nfeatures /= np.maximum(np.linalg.norm(features, ord=2, axis=1, keepdims=True), 1e-8)\ncluster_ids = self._clustering.predict(features).astype(np.int64)\nreturn torch.from_numpy(cluster_ids)\n</code></pre>"},{"location":"api/ocl/metrics/diagnosis/","title":"ocl.metrics.diagnosis","text":"<p>Metrics used for diagnosis.</p>"},{"location":"api/ocl/metrics/diagnosis/#ocl.metrics.diagnosis.TensorStatistic","title":"<code>TensorStatistic</code>","text":"<p>         Bases: <code>torchmetrics.Metric</code></p> <p>Metric that computes summary statistic of tensors for logging purposes.</p> <p>First dimension of tensor is assumed to be batch dimension. Other dimensions are reduced to a scalar by the chosen reduction approach (sum or mean).</p> Source code in <code>ocl/metrics/diagnosis.py</code> <pre><code>class TensorStatistic(torchmetrics.Metric):\n\"\"\"Metric that computes summary statistic of tensors for logging purposes.\n    First dimension of tensor is assumed to be batch dimension. Other dimensions are reduced to a\n    scalar by the chosen reduction approach (sum or mean).\n    \"\"\"\ndef __init__(self, reduction: str = \"mean\"):\nsuper().__init__()\nif reduction not in (\"sum\", \"mean\"):\nraise ValueError(f\"Unknown reduction {reduction}\")\nself.reduction = reduction\nself.add_state(\n\"values\", default=torch.tensor(0.0, dtype=torch.float64), dist_reduce_fx=\"sum\"\n)\nself.add_state(\"total\", default=torch.tensor(0), dist_reduce_fx=\"sum\")\ndef update(self, tensor: torch.Tensor):\ntensor = torch.atleast_2d(tensor).flatten(1, -1).to(dtype=torch.float64)\nif self.reduction == \"mean\":\ntensor = torch.mean(tensor, dim=1)\nelif self.reduction == \"sum\":\ntensor = torch.sum(tensor, dim=1)\nself.values += tensor.sum()\nself.total += len(tensor)\ndef compute(self) -&gt; torch.Tensor:\nreturn self.values / self.total\n</code></pre>"},{"location":"api/ocl/metrics/masks/","title":"ocl.metrics.masks","text":"<p>Metrics related to the evaluation of masks.</p>"},{"location":"api/ocl/metrics/masks/#ocl.metrics.masks.ARIMetric","title":"<code>ARIMetric</code>","text":"<p>         Bases: <code>torchmetrics.Metric</code></p> <p>Computes ARI metric.</p> Source code in <code>ocl/metrics/masks.py</code> <pre><code>class ARIMetric(torchmetrics.Metric):\n\"\"\"Computes ARI metric.\"\"\"\ndef __init__(\nself,\nforeground: bool = True,\nconvert_target_one_hot: bool = False,\nignore_overlaps: bool = False,\n):\nsuper().__init__()\nself.foreground = foreground\nself.convert_target_one_hot = convert_target_one_hot\nself.ignore_overlaps = ignore_overlaps\nself.add_state(\n\"values\", default=torch.tensor(0.0, dtype=torch.float64), dist_reduce_fx=\"sum\"\n)\nself.add_state(\"total\", default=torch.tensor(0), dist_reduce_fx=\"sum\")\ndef update(\nself, prediction: torch.Tensor, target: torch.Tensor, ignore: Optional[torch.Tensor] = None\n):\n\"\"\"Update this metric.\n        Args:\n            prediction: Predicted mask of shape (B, C, H, W) or (B, F, C, H, W), where C is the\n                number of classes.\n            target: Ground truth mask of shape (B, K, H, W) or (B, F, K, H, W), where K is the\n                number of classes.\n            ignore: Ignore mask of shape (B, 1, H, W) or (B, 1, K, H, W)\n        \"\"\"\nif prediction.ndim == 5:\n# Merge frames, height and width to single dimension.\nprediction = prediction.transpose(1, 2).flatten(-3, -1)\ntarget = target.transpose(1, 2).flatten(-3, -1)\nif ignore is not None:\nignore = ignore.to(torch.bool).transpose(1, 2).flatten(-3, -1)\nelif prediction.ndim == 4:\n# Merge height and width to single dimension.\nprediction = prediction.flatten(-2, -1)\ntarget = target.flatten(-2, -1)\nif ignore is not None:\nignore = ignore.to(torch.bool).flatten(-2, -1)\nelse:\nraise ValueError(f\"Incorrect input shape: f{prediction.shape}\")\nif self.ignore_overlaps:\noverlaps = (target &gt; 0).sum(1, keepdim=True) &gt; 1\nif ignore is None:\nignore = overlaps\nelse:\nignore = ignore | overlaps\nif ignore is not None:\nassert ignore.ndim == 3 and ignore.shape[1] == 1\nprediction = prediction.clone()\nprediction[ignore.expand_as(prediction)] = 0\ntarget = target.clone()\ntarget[ignore.expand_as(target)] = 0\n# Make channels / gt labels the last dimension.\nprediction = prediction.transpose(-2, -1)\ntarget = target.transpose(-2, -1)\nif self.convert_target_one_hot:\ntarget_oh = tensor_to_one_hot(target, dim=2)\n# For empty pixels (all values zero), one-hot assigns 1 to the first class, correct for\n# this (then it is technically not one-hot anymore).\ntarget_oh[:, :, 0][target.sum(dim=2) == 0] = 0\ntarget = target_oh\n# Should be either 0 (empty, padding) or 1 (single object).\nassert torch.all(target.sum(dim=-1) &lt; 2), \"Issues with target format, mask non-exclusive\"\nif self.foreground:\nari = fg_adjusted_rand_index(prediction, target)\nelse:\nari = adjusted_rand_index(prediction, target)\nself.values += ari.sum()\nself.total += len(ari)\ndef compute(self) -&gt; torch.Tensor:\nreturn self.values / self.total\n</code></pre>"},{"location":"api/ocl/metrics/masks/#ocl.metrics.masks.ARIMetric.update","title":"<code>update</code>","text":"<p>Update this metric.</p> <p>Parameters:</p> Name Type Description Default <code>prediction</code> <code>torch.Tensor</code> <p>Predicted mask of shape (B, C, H, W) or (B, F, C, H, W), where C is the number of classes.</p> required <code>target</code> <code>torch.Tensor</code> <p>Ground truth mask of shape (B, K, H, W) or (B, F, K, H, W), where K is the number of classes.</p> required <code>ignore</code> <code>Optional[torch.Tensor]</code> <p>Ignore mask of shape (B, 1, H, W) or (B, 1, K, H, W)</p> <code>None</code> Source code in <code>ocl/metrics/masks.py</code> <pre><code>def update(\nself, prediction: torch.Tensor, target: torch.Tensor, ignore: Optional[torch.Tensor] = None\n):\n\"\"\"Update this metric.\n    Args:\n        prediction: Predicted mask of shape (B, C, H, W) or (B, F, C, H, W), where C is the\n            number of classes.\n        target: Ground truth mask of shape (B, K, H, W) or (B, F, K, H, W), where K is the\n            number of classes.\n        ignore: Ignore mask of shape (B, 1, H, W) or (B, 1, K, H, W)\n    \"\"\"\nif prediction.ndim == 5:\n# Merge frames, height and width to single dimension.\nprediction = prediction.transpose(1, 2).flatten(-3, -1)\ntarget = target.transpose(1, 2).flatten(-3, -1)\nif ignore is not None:\nignore = ignore.to(torch.bool).transpose(1, 2).flatten(-3, -1)\nelif prediction.ndim == 4:\n# Merge height and width to single dimension.\nprediction = prediction.flatten(-2, -1)\ntarget = target.flatten(-2, -1)\nif ignore is not None:\nignore = ignore.to(torch.bool).flatten(-2, -1)\nelse:\nraise ValueError(f\"Incorrect input shape: f{prediction.shape}\")\nif self.ignore_overlaps:\noverlaps = (target &gt; 0).sum(1, keepdim=True) &gt; 1\nif ignore is None:\nignore = overlaps\nelse:\nignore = ignore | overlaps\nif ignore is not None:\nassert ignore.ndim == 3 and ignore.shape[1] == 1\nprediction = prediction.clone()\nprediction[ignore.expand_as(prediction)] = 0\ntarget = target.clone()\ntarget[ignore.expand_as(target)] = 0\n# Make channels / gt labels the last dimension.\nprediction = prediction.transpose(-2, -1)\ntarget = target.transpose(-2, -1)\nif self.convert_target_one_hot:\ntarget_oh = tensor_to_one_hot(target, dim=2)\n# For empty pixels (all values zero), one-hot assigns 1 to the first class, correct for\n# this (then it is technically not one-hot anymore).\ntarget_oh[:, :, 0][target.sum(dim=2) == 0] = 0\ntarget = target_oh\n# Should be either 0 (empty, padding) or 1 (single object).\nassert torch.all(target.sum(dim=-1) &lt; 2), \"Issues with target format, mask non-exclusive\"\nif self.foreground:\nari = fg_adjusted_rand_index(prediction, target)\nelse:\nari = adjusted_rand_index(prediction, target)\nself.values += ari.sum()\nself.total += len(ari)\n</code></pre>"},{"location":"api/ocl/metrics/masks/#ocl.metrics.masks.PatchARIMetric","title":"<code>PatchARIMetric</code>","text":"<p>         Bases: <code>ARIMetric</code></p> <p>Computes ARI metric assuming patch masks as input.</p> Source code in <code>ocl/metrics/masks.py</code> <pre><code>class PatchARIMetric(ARIMetric):\n\"\"\"Computes ARI metric assuming patch masks as input.\"\"\"\ndef __init__(\nself,\nforeground=True,\nresize_masks_mode: str = \"bilinear\",\n**kwargs,\n):\nsuper().__init__(foreground=foreground, **kwargs)\nself.resize_masks_mode = resize_masks_mode\ndef update(self, prediction: torch.Tensor, target: torch.Tensor):\n\"\"\"Update this metric.\n        Args:\n            prediction: Predicted mask of shape (B, C, P) or (B, F, C, P), where C is the\n                number of classes and P the number of patches.\n            target: Ground truth mask of shape (B, K, H, W) or (B, F, K, H, W), where K is the\n                number of classes.\n        \"\"\"\nh, w = target.shape[-2:]\nassert h == w\nprediction_resized = resize_patches_to_image(\nprediction, size=h, resize_mode=self.resize_masks_mode\n)\nreturn super().update(prediction=prediction_resized, target=target)\n</code></pre>"},{"location":"api/ocl/metrics/masks/#ocl.metrics.masks.PatchARIMetric.update","title":"<code>update</code>","text":"<p>Update this metric.</p> <p>Parameters:</p> Name Type Description Default <code>prediction</code> <code>torch.Tensor</code> <p>Predicted mask of shape (B, C, P) or (B, F, C, P), where C is the number of classes and P the number of patches.</p> required <code>target</code> <code>torch.Tensor</code> <p>Ground truth mask of shape (B, K, H, W) or (B, F, K, H, W), where K is the number of classes.</p> required Source code in <code>ocl/metrics/masks.py</code> <pre><code>def update(self, prediction: torch.Tensor, target: torch.Tensor):\n\"\"\"Update this metric.\n    Args:\n        prediction: Predicted mask of shape (B, C, P) or (B, F, C, P), where C is the\n            number of classes and P the number of patches.\n        target: Ground truth mask of shape (B, K, H, W) or (B, F, K, H, W), where K is the\n            number of classes.\n    \"\"\"\nh, w = target.shape[-2:]\nassert h == w\nprediction_resized = resize_patches_to_image(\nprediction, size=h, resize_mode=self.resize_masks_mode\n)\nreturn super().update(prediction=prediction_resized, target=target)\n</code></pre>"},{"location":"api/ocl/metrics/masks/#ocl.metrics.masks.UnsupervisedMaskIoUMetric","title":"<code>UnsupervisedMaskIoUMetric</code>","text":"<p>         Bases: <code>torchmetrics.Metric</code></p> <p>Computes IoU metric for segmentation masks when correspondences to ground truth are not known.</p> <p>Uses Hungarian matching to compute the assignment between predicted classes and ground truth classes.</p> <p>Parameters:</p> Name Type Description Default <code>use_threshold</code> <code>bool</code> <p>If <code>True</code>, convert predicted class probabilities to mask using a threshold. If <code>False</code>, class probabilities are turned into mask using a softmax instead.</p> <code>False</code> <code>threshold</code> <code>float</code> <p>Value to use for thresholding masks.</p> <code>0.5</code> <code>matching</code> <code>str</code> <p>Approach to match predicted to ground truth classes. For \"hungarian\", computes assignment that maximizes total IoU between all classes. For \"best_overlap\", uses the predicted class with maximum overlap for each ground truth class. Using \"best_overlap\" leads to the \"average best overlap\" metric.</p> <code>'hungarian'</code> <code>compute_discovery_fraction</code> <code>bool</code> <p>Instead of the IoU, compute the fraction of ground truth classes that were \"discovered\", meaning that they have an IoU greater than some threshold.</p> <code>False</code> <code>correct_localization</code> <code>bool</code> <p>Instead of the IoU, compute the fraction of images on which at least one ground truth class was correctly localised, meaning that they have an IoU greater than some threshold.</p> <code>False</code> <code>discovery_threshold</code> <code>float</code> <p>Minimum IoU to count a class as discovered/correctly localized.</p> <code>0.5</code> <code>ignore_background</code> <code>bool</code> <p>If true, assume class at index 0 of ground truth masks is background class that is removed before computing IoU.</p> <code>False</code> <code>ignore_overlaps</code> <code>bool</code> <p>If true, remove points where ground truth masks has overlappign classes from predictions and ground truth masks.</p> <code>False</code> Source code in <code>ocl/metrics/masks.py</code> <pre><code>class UnsupervisedMaskIoUMetric(torchmetrics.Metric):\n\"\"\"Computes IoU metric for segmentation masks when correspondences to ground truth are not known.\n    Uses Hungarian matching to compute the assignment between predicted classes and ground truth\n    classes.\n    Args:\n        use_threshold: If `True`, convert predicted class probabilities to mask using a threshold.\n            If `False`, class probabilities are turned into mask using a softmax instead.\n        threshold: Value to use for thresholding masks.\n        matching: Approach to match predicted to ground truth classes. For \"hungarian\", computes\n            assignment that maximizes total IoU between all classes. For \"best_overlap\", uses the\n            predicted class with maximum overlap for each ground truth class. Using \"best_overlap\"\n            leads to the \"average best overlap\" metric.\n        compute_discovery_fraction: Instead of the IoU, compute the fraction of ground truth classes\n            that were \"discovered\", meaning that they have an IoU greater than some threshold.\n        correct_localization: Instead of the IoU, compute the fraction of images on which at least\n            one ground truth class was correctly localised, meaning that they have an IoU\n            greater than some threshold.\n        discovery_threshold: Minimum IoU to count a class as discovered/correctly localized.\n        ignore_background: If true, assume class at index 0 of ground truth masks is background class\n            that is removed before computing IoU.\n        ignore_overlaps: If true, remove points where ground truth masks has overlappign classes from\n            predictions and ground truth masks.\n    \"\"\"\ndef __init__(\nself,\nuse_threshold: bool = False,\nthreshold: float = 0.5,\nmatching: str = \"hungarian\",\ncompute_discovery_fraction: bool = False,\ncorrect_localization: bool = False,\ndiscovery_threshold: float = 0.5,\nignore_background: bool = False,\nignore_overlaps: bool = False,\n):\nsuper().__init__()\nself.use_threshold = use_threshold\nself.threshold = threshold\nself.discovery_threshold = discovery_threshold\nself.compute_discovery_fraction = compute_discovery_fraction\nself.correct_localization = correct_localization\nif compute_discovery_fraction and correct_localization:\nraise ValueError(\n\"Only one of `compute_discovery_fraction` and `correct_localization` can be enabled.\"\n)\nmatchings = (\"hungarian\", \"best_overlap\")\nif matching not in matchings:\nraise ValueError(f\"Unknown matching type {matching}. Valid values are {matchings}.\")\nself.matching = matching\nself.ignore_background = ignore_background\nself.ignore_overlaps = ignore_overlaps\nself.add_state(\n\"values\", default=torch.tensor(0.0, dtype=torch.float64), dist_reduce_fx=\"sum\"\n)\nself.add_state(\"total\", default=torch.tensor(0), dist_reduce_fx=\"sum\")\ndef update(\nself, prediction: torch.Tensor, target: torch.Tensor, ignore: Optional[torch.Tensor] = None\n):\n\"\"\"Update this metric.\n        Args:\n            prediction: Predicted mask of shape (B, C, H, W) or (B, F, C, H, W), where C is the\n                number of classes. Assumes class probabilities as inputs.\n            target: Ground truth mask of shape (B, K, H, W) or (B, F, K, H, W), where K is the\n                number of classes.\n            ignore: Ignore mask of shape (B, 1, H, W) or (B, 1, K, H, W)\n        \"\"\"\nif prediction.ndim == 5:\n# Merge frames, height and width to single dimension.\npredictions = prediction.transpose(1, 2).flatten(-3, -1)\ntargets = target.transpose(1, 2).flatten(-3, -1)\nif ignore is not None:\nignore = ignore.to(torch.bool).transpose(1, 2).flatten(-3, -1)\nelif prediction.ndim == 4:\n# Merge height and width to single dimension.\npredictions = prediction.flatten(-2, -1)\ntargets = target.flatten(-2, -1)\nif ignore is not None:\nignore = ignore.to(torch.bool).flatten(-2, -1)\nelse:\nraise ValueError(f\"Incorrect input shape: f{prediction.shape}\")\nif self.use_threshold:\npredictions = predictions &gt; self.threshold\nelse:\nindices = torch.argmax(predictions, dim=1)\npredictions = torch.nn.functional.one_hot(indices, num_classes=predictions.shape[1])\npredictions = predictions.transpose(1, 2)\nif self.ignore_background:\ntargets = targets[:, 1:]\ntargets = targets &gt; 0  # Ensure masks are binary\nif self.ignore_overlaps:\noverlaps = targets.sum(1, keepdim=True) &gt; 1\nif ignore is None:\nignore = overlaps\nelse:\nignore = ignore | overlaps\nif ignore is not None:\nassert ignore.ndim == 3 and ignore.shape[1] == 1\npredictions[ignore.expand_as(predictions)] = 0\ntargets[ignore.expand_as(targets)] = 0\n# Should be either 0 (empty, padding) or 1 (single object).\nassert torch.all(targets.sum(dim=1) &lt; 2), \"Issues with target format, mask non-exclusive\"\nfor pred, target in zip(predictions, targets):\nnonzero_classes = torch.sum(target, dim=-1) &gt; 0\ntarget = target[nonzero_classes]  # Remove empty (e.g. padded) classes\nif len(target) == 0:\ncontinue  # Skip elements without any target mask\niou_per_class = unsupervised_mask_iou(\npred, target, matching=self.matching, reduction=\"none\"\n)\nif self.compute_discovery_fraction:\ndiscovered = iou_per_class &gt; self.discovery_threshold\nself.values += discovered.sum() / len(discovered)\nelif self.correct_localization:\ncorrectly_localized = torch.any(iou_per_class &gt; self.discovery_threshold)\nself.values += correctly_localized.sum()\nelse:\nself.values += iou_per_class.mean()\nself.total += 1\ndef compute(self) -&gt; torch.Tensor:\nif self.total == 0:\nreturn torch.zeros_like(self.values)\nelse:\nreturn self.values / self.total\n</code></pre>"},{"location":"api/ocl/metrics/masks/#ocl.metrics.masks.UnsupervisedMaskIoUMetric.update","title":"<code>update</code>","text":"<p>Update this metric.</p> <p>Parameters:</p> Name Type Description Default <code>prediction</code> <code>torch.Tensor</code> <p>Predicted mask of shape (B, C, H, W) or (B, F, C, H, W), where C is the number of classes. Assumes class probabilities as inputs.</p> required <code>target</code> <code>torch.Tensor</code> <p>Ground truth mask of shape (B, K, H, W) or (B, F, K, H, W), where K is the number of classes.</p> required <code>ignore</code> <code>Optional[torch.Tensor]</code> <p>Ignore mask of shape (B, 1, H, W) or (B, 1, K, H, W)</p> <code>None</code> Source code in <code>ocl/metrics/masks.py</code> <pre><code>def update(\nself, prediction: torch.Tensor, target: torch.Tensor, ignore: Optional[torch.Tensor] = None\n):\n\"\"\"Update this metric.\n    Args:\n        prediction: Predicted mask of shape (B, C, H, W) or (B, F, C, H, W), where C is the\n            number of classes. Assumes class probabilities as inputs.\n        target: Ground truth mask of shape (B, K, H, W) or (B, F, K, H, W), where K is the\n            number of classes.\n        ignore: Ignore mask of shape (B, 1, H, W) or (B, 1, K, H, W)\n    \"\"\"\nif prediction.ndim == 5:\n# Merge frames, height and width to single dimension.\npredictions = prediction.transpose(1, 2).flatten(-3, -1)\ntargets = target.transpose(1, 2).flatten(-3, -1)\nif ignore is not None:\nignore = ignore.to(torch.bool).transpose(1, 2).flatten(-3, -1)\nelif prediction.ndim == 4:\n# Merge height and width to single dimension.\npredictions = prediction.flatten(-2, -1)\ntargets = target.flatten(-2, -1)\nif ignore is not None:\nignore = ignore.to(torch.bool).flatten(-2, -1)\nelse:\nraise ValueError(f\"Incorrect input shape: f{prediction.shape}\")\nif self.use_threshold:\npredictions = predictions &gt; self.threshold\nelse:\nindices = torch.argmax(predictions, dim=1)\npredictions = torch.nn.functional.one_hot(indices, num_classes=predictions.shape[1])\npredictions = predictions.transpose(1, 2)\nif self.ignore_background:\ntargets = targets[:, 1:]\ntargets = targets &gt; 0  # Ensure masks are binary\nif self.ignore_overlaps:\noverlaps = targets.sum(1, keepdim=True) &gt; 1\nif ignore is None:\nignore = overlaps\nelse:\nignore = ignore | overlaps\nif ignore is not None:\nassert ignore.ndim == 3 and ignore.shape[1] == 1\npredictions[ignore.expand_as(predictions)] = 0\ntargets[ignore.expand_as(targets)] = 0\n# Should be either 0 (empty, padding) or 1 (single object).\nassert torch.all(targets.sum(dim=1) &lt; 2), \"Issues with target format, mask non-exclusive\"\nfor pred, target in zip(predictions, targets):\nnonzero_classes = torch.sum(target, dim=-1) &gt; 0\ntarget = target[nonzero_classes]  # Remove empty (e.g. padded) classes\nif len(target) == 0:\ncontinue  # Skip elements without any target mask\niou_per_class = unsupervised_mask_iou(\npred, target, matching=self.matching, reduction=\"none\"\n)\nif self.compute_discovery_fraction:\ndiscovered = iou_per_class &gt; self.discovery_threshold\nself.values += discovered.sum() / len(discovered)\nelif self.correct_localization:\ncorrectly_localized = torch.any(iou_per_class &gt; self.discovery_threshold)\nself.values += correctly_localized.sum()\nelse:\nself.values += iou_per_class.mean()\nself.total += 1\n</code></pre>"},{"location":"api/ocl/metrics/masks/#ocl.metrics.masks.unsupervised_mask_iou","title":"<code>unsupervised_mask_iou</code>","text":"<p>Compute intersection-over-union (IoU) between masks with unknown class correspondences.</p> <p>This metric is also known as Jaccard index. Note that this is a non-batched implementation.</p> <p>Parameters:</p> Name Type Description Default <code>pred_mask</code> <code>torch.Tensor</code> <p>Predicted mask of shape (C, N), where C is the number of predicted classes and N is the number of points. Masks are assumed to be binary.</p> required <code>true_mask</code> <code>torch.Tensor</code> <p>Ground truth mask of shape (K, N), where K is the number of ground truth classes and N is the number of points. Masks are assumed to be binary.</p> required <code>matching</code> <code>str</code> <p>How to match predicted classes to ground truth classes. For \"hungarian\", computes assignment that maximizes total IoU between all classes. For \"best_overlap\", uses the predicted class with maximum overlap for each ground truth class (each predicted class can be assigned to multiple ground truth classes). Empty ground truth classes are assigned IoU of zero.</p> <code>'hungarian'</code> <code>reduction</code> <code>str</code> <p>If \"mean\", return IoU averaged over classes. If \"none\", return per-class IoU.</p> <code>'mean'</code> <code>iou_empty</code> <code>float</code> <p>IoU for the case when a class does not occur, but was also not predicted.</p> <code>0.0</code> <p>Returns:</p> Type Description <code>torch.Tensor</code> <p>Mean IoU over classes if reduction is <code>mean</code>, tensor of shape (K,) containing per-class IoU</p> <code>torch.Tensor</code> <p>otherwise.</p> Source code in <code>ocl/metrics/masks.py</code> <pre><code>def unsupervised_mask_iou(\npred_mask: torch.Tensor,\ntrue_mask: torch.Tensor,\nmatching: str = \"hungarian\",\nreduction: str = \"mean\",\niou_empty: float = 0.0,\n) -&gt; torch.Tensor:\n\"\"\"Compute intersection-over-union (IoU) between masks with unknown class correspondences.\n    This metric is also known as Jaccard index. Note that this is a non-batched implementation.\n    Args:\n        pred_mask: Predicted mask of shape (C, N), where C is the number of predicted classes and\n            N is the number of points. Masks are assumed to be binary.\n        true_mask: Ground truth mask of shape (K, N), where K is the number of ground truth\n            classes and N is the number of points. Masks are assumed to be binary.\n        matching: How to match predicted classes to ground truth classes. For \"hungarian\", computes\n            assignment that maximizes total IoU between all classes. For \"best_overlap\", uses the\n            predicted class with maximum overlap for each ground truth class (each predicted class\n            can be assigned to multiple ground truth classes). Empty ground truth classes are\n            assigned IoU of zero.\n        reduction: If \"mean\", return IoU averaged over classes. If \"none\", return per-class IoU.\n        iou_empty: IoU for the case when a class does not occur, but was also not predicted.\n    Returns:\n        Mean IoU over classes if reduction is `mean`, tensor of shape (K,) containing per-class IoU\n        otherwise.\n    \"\"\"\nassert pred_mask.ndim == 2\nassert true_mask.ndim == 2\nn_gt_classes = len(true_mask)\npred_mask = pred_mask.unsqueeze(1).to(torch.bool)\ntrue_mask = true_mask.unsqueeze(0).to(torch.bool)\nintersection = torch.sum(pred_mask &amp; true_mask, dim=-1).to(torch.float64)\nunion = torch.sum(pred_mask | true_mask, dim=-1).to(torch.float64)\npairwise_iou = intersection / union\n# Remove NaN from divide-by-zero: class does not occur, and class was not predicted.\npairwise_iou[union == 0] = iou_empty\nif matching == \"hungarian\":\npred_idxs, true_idxs = scipy.optimize.linear_sum_assignment(\npairwise_iou.cpu(), maximize=True\n)\npred_idxs = torch.as_tensor(pred_idxs, dtype=torch.int64, device=pairwise_iou.device)\ntrue_idxs = torch.as_tensor(true_idxs, dtype=torch.int64, device=pairwise_iou.device)\nelif matching == \"best_overlap\":\nnon_empty_gt = torch.sum(true_mask.squeeze(0), dim=1) &gt; 0\npred_idxs = torch.argmax(pairwise_iou, dim=0)[non_empty_gt]\ntrue_idxs = torch.arange(pairwise_iou.shape[1])[non_empty_gt]\nelse:\nraise ValueError(f\"Unknown matching {matching}\")\nmatched_iou = pairwise_iou[pred_idxs, true_idxs]\niou = torch.zeros(n_gt_classes, dtype=torch.float64, device=pairwise_iou.device)\niou[true_idxs] = matched_iou\nif reduction == \"mean\":\nreturn iou.mean()\nelse:\nreturn iou\n</code></pre>"},{"location":"api/ocl/metrics/tracking/","title":"ocl.metrics.tracking","text":"<p>Metrics related to tracking.</p>"},{"location":"api/ocl/metrics/tracking/#ocl.metrics.tracking.MOTMetric","title":"<code>MOTMetric</code>","text":"<p>         Bases: <code>torchmetrics.Metric</code></p> <p>Multiple object tracking metric.</p> Source code in <code>ocl/metrics/tracking.py</code> <pre><code>class MOTMetric(torchmetrics.Metric):\n\"\"\"Multiple object tracking metric.\"\"\"\ndef __init__(\nself,\ntarget_is_mask: bool = True,\nuse_threshold: bool = True,\nthreshold: float = 0.5,\n):\n\"\"\"Initialize MOTMetric.\n        Args:\n            target_is_mask: Is the metrics evaluated on masks\n            use_threshold: Use threshold to binarize predicted mask\n            threshold: Threshold value\n        \"\"\"\nsuper().__init__()\nself.target_is_mask = target_is_mask\nself.use_threshold = use_threshold\nself.threshold = threshold\nself.reset_accumulator()\nself.accuracy = []\nself.add_state(\n\"values\", default=torch.tensor(0.0, dtype=torch.float64), dist_reduce_fx=\"sum\"\n)\nself.add_state(\"total\", default=torch.tensor(0), dist_reduce_fx=\"sum\")\ndef reset_accumulator(self):\nself.acc = mm.MOTAccumulator(auto_id=True)\ndef update(self, prediction: torch.Tensor, target: torch.Tensor):\n# Merge batch and frame dimensions\nB, F = prediction.shape[:2]\nprediction = prediction.flatten(0, 1)\ntarget = target.flatten(0, 1)\nn_pred_classes = prediction.shape[1]\nn_gt_classes = target.shape[1]\nif self.use_threshold:\nprediction = prediction &gt; self.threshold\nelse:\nindices = torch.argmax(prediction, dim=1)\nprediction = torch.nn.functional.one_hot(indices, num_classes=n_pred_classes)\nprediction = prediction.permute(0, 3, 1, 2)\npred_bboxes = masks_to_bboxes(prediction.flatten(0, 1)).unflatten(0, (B, F, n_pred_classes))\nif self.target_is_mask:\ntarget_bboxes = masks_to_bboxes(target.flatten(0, 1)).unflatten(0, (B, F, n_gt_classes))\nelse:\nassert target.shape[-1] == 4\n# Convert all-zero boxes added during padding to invalid boxes\ntarget[torch.all(target == 0.0, dim=-1)] = -1.0\ntarget_bboxes = target\nself.reset_accumulator()\nfor preds, targets in zip(pred_bboxes, target_bboxes):\n# seq evaluation\nself.reset_accumulator()\nfor pred, target, mask in zip(preds, targets, prediction):\nvalid_track_box = pred[:, 0] != -1.0\nvalid_target_box = target[:, 0] != -1.0\ntrack_id = valid_track_box.nonzero()[:, 0].detach().cpu().numpy()\ntarget_id = valid_target_box.nonzero()[:, 0].detach().cpu().numpy()\n# move background\nidx = track_id.tolist()\nfor id in idx:\nh, w = mask[id].shape\nthres = h * w * 0.25\nif pred[id][2] * pred[id][3] &gt;= thres:\nidx.remove(id)\ncur_obj_idx = np.array(idx)\nif valid_target_box.sum() == 0:\ncontinue  # Skip data points without any target bbox\npred = pred[cur_obj_idx].detach().cpu().numpy()\ntarget = target[valid_target_box].detach().cpu().numpy()\n# frame evaluation\nself.eval_frame(pred, target, cur_obj_idx, target_id)\nself.accuracy.append(self.acc)\nself.total += 1\ndef eval_frame(self, trk_tlwhs, tgt_tlwhs, trk_ids, tgt_ids):\n# get distance matrix\ntrk_tlwhs = np.copy(trk_tlwhs)\ntgt_tlwhs = np.copy(tgt_tlwhs)\ntrk_ids = np.copy(trk_ids)\ntgt_ids = np.copy(tgt_ids)\niou_distance = mm.distances.iou_matrix(tgt_tlwhs, trk_tlwhs, max_iou=0.5)\n# acc\nself.acc.update(tgt_ids, trk_ids, iou_distance)\ndef convert_motmetric_to_value(self, res):\ndp = res.replace(\" \", \";\").replace(\";;\", \";\").replace(\";;\", \";\").replace(\";;\", \";\")\ntmp = list(dp)\ntmp[0] = \"-\"\ndp = \"\".join(tmp)\nreturn io.StringIO(dp)\ndef compute(self) -&gt; torch.Tensor:\nif self.total == 0:\nreturn torch.zeros_like(self.values)\nelse:\nmetrics = mm.metrics.motchallenge_metrics\nmh = mm.metrics.create()\nsummary = mh.compute_many(\nself.accuracy, metrics=metrics, names=None, generate_overall=True\n)\nstrsummary = mm.io.render_summary(\nsummary, formatters=mh.formatters, namemap=mm.io.motchallenge_metric_names\n)\nres = self.convert_motmetric_to_value(strsummary)\ndf = pd.read_csv(res, sep=\";\", engine=\"python\")\nmota = df.iloc[-1][\"MOTA\"]\nself.values = torch.tensor(float(mota[:-1]), dtype=torch.float64).to(self.values.device)\nself.reset_accumulator()\nself.accuracy = []\nreturn self.values\n</code></pre>"},{"location":"api/ocl/metrics/tracking/#ocl.metrics.tracking.MOTMetric.__init__","title":"<code>__init__</code>","text":"<p>Initialize MOTMetric.</p> <p>Parameters:</p> Name Type Description Default <code>target_is_mask</code> <code>bool</code> <p>Is the metrics evaluated on masks</p> <code>True</code> <code>use_threshold</code> <code>bool</code> <p>Use threshold to binarize predicted mask</p> <code>True</code> <code>threshold</code> <code>float</code> <p>Threshold value</p> <code>0.5</code> Source code in <code>ocl/metrics/tracking.py</code> <pre><code>def __init__(\nself,\ntarget_is_mask: bool = True,\nuse_threshold: bool = True,\nthreshold: float = 0.5,\n):\n\"\"\"Initialize MOTMetric.\n    Args:\n        target_is_mask: Is the metrics evaluated on masks\n        use_threshold: Use threshold to binarize predicted mask\n        threshold: Threshold value\n    \"\"\"\nsuper().__init__()\nself.target_is_mask = target_is_mask\nself.use_threshold = use_threshold\nself.threshold = threshold\nself.reset_accumulator()\nself.accuracy = []\nself.add_state(\n\"values\", default=torch.tensor(0.0, dtype=torch.float64), dist_reduce_fx=\"sum\"\n)\nself.add_state(\"total\", default=torch.tensor(0), dist_reduce_fx=\"sum\")\n</code></pre>"},{"location":"api/ocl/metrics/utils/","title":"ocl.metrics.utils","text":"<p>Utility functions used in metrics computation.</p>"},{"location":"api/ocl/metrics/utils/#ocl.metrics.utils.tensor_to_one_hot","title":"<code>tensor_to_one_hot</code>","text":"<p>Convert tensor to one-hot encoding by using maximum across dimension as one-hot element.</p> Source code in <code>ocl/metrics/utils.py</code> <pre><code>def tensor_to_one_hot(tensor: torch.Tensor, dim: int) -&gt; torch.Tensor:\n\"\"\"Convert tensor to one-hot encoding by using maximum across dimension as one-hot element.\"\"\"\nassert 0 &lt;= dim\nmax_idxs = torch.argmax(tensor, dim=dim, keepdim=True)\nshape = [1] * dim + [-1] + [1] * (tensor.ndim - dim - 1)\none_hot = max_idxs == torch.arange(tensor.shape[dim], device=tensor.device).view(*shape)\nreturn one_hot.to(torch.long)\n</code></pre>"},{"location":"api/ocl/metrics/utils/#ocl.metrics.utils.adjusted_rand_index","title":"<code>adjusted_rand_index</code>","text":"<p>Computes adjusted Rand index (ARI), a clustering similarity score.</p> <p>This implementation ignores points with no cluster label in <code>true_mask</code> (i.e. those points for which <code>true_mask</code> is a zero vector). In the context of segmentation, that means this function can ignore points in an image corresponding to the background (i.e. not to an object).</p> <p>Implementation adapted from https://github.com/deepmind/multi_object_datasets and https://github.com/google-research/slot-attention-video/blob/main/savi/lib/metrics.py</p> <p>Parameters:</p> Name Type Description Default <code>pred_mask</code> <code>torch.Tensor</code> <p>Predicted cluster assignment encoded as categorical probabilities of shape (batch_size, n_points, n_pred_clusters).</p> required <code>true_mask</code> <code>torch.Tensor</code> <p>True cluster assignment encoded as one-hot of shape (batch_size, n_points, n_true_clusters).</p> required <p>Returns:</p> Type Description <code>torch.Tensor</code> <p>ARI scores of shape (batch_size,).</p> Source code in <code>ocl/metrics/utils.py</code> <pre><code>def adjusted_rand_index(pred_mask: torch.Tensor, true_mask: torch.Tensor) -&gt; torch.Tensor:\n\"\"\"Computes adjusted Rand index (ARI), a clustering similarity score.\n    This implementation ignores points with no cluster label in `true_mask` (i.e. those points for\n    which `true_mask` is a zero vector). In the context of segmentation, that means this function\n    can ignore points in an image corresponding to the background (i.e. not to an object).\n    Implementation adapted from https://github.com/deepmind/multi_object_datasets and\n    https://github.com/google-research/slot-attention-video/blob/main/savi/lib/metrics.py\n    Args:\n        pred_mask: Predicted cluster assignment encoded as categorical probabilities of shape\n            (batch_size, n_points, n_pred_clusters).\n        true_mask: True cluster assignment encoded as one-hot of shape (batch_size, n_points,\n            n_true_clusters).\n    Returns:\n        ARI scores of shape (batch_size,).\n    \"\"\"\nn_pred_clusters = pred_mask.shape[-1]\npred_cluster_ids = torch.argmax(pred_mask, axis=-1)\n# Convert true and predicted clusters to one-hot ('oh') representations. We use float64 here on\n# purpose, otherwise mixed precision training automatically casts to FP16 in some of the\n# operations below, which can create overflows.\ntrue_mask_oh = true_mask.to(torch.float64)  # already one-hot\npred_mask_oh = torch.nn.functional.one_hot(pred_cluster_ids, n_pred_clusters).to(torch.float64)\nn_ij = torch.einsum(\"bnc,bnk-&gt;bck\", true_mask_oh, pred_mask_oh)\na = torch.sum(n_ij, axis=-1)\nb = torch.sum(n_ij, axis=-2)\nn_fg_points = torch.sum(a, axis=1)\nrindex = torch.sum(n_ij * (n_ij - 1), axis=(1, 2))\naindex = torch.sum(a * (a - 1), axis=1)\nbindex = torch.sum(b * (b - 1), axis=1)\nexpected_rindex = aindex * bindex / torch.clamp(n_fg_points * (n_fg_points - 1), min=1)\nmax_rindex = (aindex + bindex) / 2\ndenominator = max_rindex - expected_rindex\nari = (rindex - expected_rindex) / denominator\n# There are two cases for which the denominator can be zero:\n# 1. If both true_mask and pred_mask assign all pixels to a single cluster.\n#    (max_rindex == expected_rindex == rindex == n_fg_points * (n_fg_points-1))\n# 2. If both true_mask and pred_mask assign max 1 point to each cluster.\n#    (max_rindex == expected_rindex == rindex == 0)\n# In both cases, we want the ARI score to be 1.0:\nreturn torch.where(denominator &gt; 0, ari, torch.ones_like(ari))\n</code></pre>"},{"location":"api/ocl/metrics/utils/#ocl.metrics.utils.fg_adjusted_rand_index","title":"<code>fg_adjusted_rand_index</code>","text":"<p>Compute adjusted random index using only foreground groups (FG-ARI).</p> <p>Parameters:</p> Name Type Description Default <code>pred_mask</code> <code>torch.Tensor</code> <p>Predicted cluster assignment encoded as categorical probabilities of shape (batch_size, n_points, n_pred_clusters).</p> required <code>true_mask</code> <code>torch.Tensor</code> <p>True cluster assignment encoded as one-hot of shape (batch_size, n_points, n_true_clusters).</p> required <code>bg_dim</code> <code>int</code> <p>Index of background class in true mask.</p> <code>0</code> <p>Returns:</p> Type Description <code>torch.Tensor</code> <p>ARI scores of shape (batch_size,).</p> Source code in <code>ocl/metrics/utils.py</code> <pre><code>def fg_adjusted_rand_index(\npred_mask: torch.Tensor, true_mask: torch.Tensor, bg_dim: int = 0\n) -&gt; torch.Tensor:\n\"\"\"Compute adjusted random index using only foreground groups (FG-ARI).\n    Args:\n        pred_mask: Predicted cluster assignment encoded as categorical probabilities of shape\n            (batch_size, n_points, n_pred_clusters).\n        true_mask: True cluster assignment encoded as one-hot of shape (batch_size, n_points,\n            n_true_clusters).\n        bg_dim: Index of background class in true mask.\n    Returns:\n        ARI scores of shape (batch_size,).\n    \"\"\"\nn_true_clusters = true_mask.shape[-1]\nassert 0 &lt;= bg_dim &lt; n_true_clusters\nif bg_dim == 0:\ntrue_mask_only_fg = true_mask[..., 1:]\nelif bg_dim == n_true_clusters - 1:\ntrue_mask_only_fg = true_mask[..., :-1]\nelse:\ntrue_mask_only_fg = torch.cat(\n(true_mask[..., :bg_dim], true_mask[..., bg_dim + 1 :]), dim=-1\n)\nreturn adjusted_rand_index(pred_mask, true_mask_only_fg)\n</code></pre>"},{"location":"api/ocl/metrics/utils/#ocl.metrics.utils.masks_to_bboxes","title":"<code>masks_to_bboxes</code>","text":"<p>Compute bounding boxes around the provided masks.</p> <p>Adapted from DETR: https://github.com/facebookresearch/detr/blob/main/util/box_ops.py</p> <p>Parameters:</p> Name Type Description Default <code>masks</code> <code>torch.Tensor</code> <p>Tensor of shape (N, H, W), where N is the number of masks, H and W are the spatial dimensions.</p> required <code>empty_value</code> <code>float</code> <p>Value bounding boxes should contain for empty masks.</p> <code>-1.0</code> <p>Returns:</p> Type Description <code>torch.Tensor</code> <p>Tensor of shape (N, 4), containing bounding boxes in (x1, y1, x2, y2) format, where (x1, y1)</p> <code>torch.Tensor</code> <p>is the coordinate of top-left corner and (x2, y2) is the coordinate of the bottom-right</p> <code>torch.Tensor</code> <p>corner (inclusive) in pixel coordinates. If mask is empty, all coordinates contain</p> <code>torch.Tensor</code> <p><code>empty_value</code> instead.</p> Source code in <code>ocl/metrics/utils.py</code> <pre><code>def masks_to_bboxes(masks: torch.Tensor, empty_value: float = -1.0) -&gt; torch.Tensor:\n\"\"\"Compute bounding boxes around the provided masks.\n    Adapted from DETR: https://github.com/facebookresearch/detr/blob/main/util/box_ops.py\n    Args:\n        masks: Tensor of shape (N, H, W), where N is the number of masks, H and W are the spatial\n            dimensions.\n        empty_value: Value bounding boxes should contain for empty masks.\n    Returns:\n        Tensor of shape (N, 4), containing bounding boxes in (x1, y1, x2, y2) format, where (x1, y1)\n        is the coordinate of top-left corner and (x2, y2) is the coordinate of the bottom-right\n        corner (inclusive) in pixel coordinates. If mask is empty, all coordinates contain\n        `empty_value` instead.\n    \"\"\"\nmasks = masks.bool()\nif masks.numel() == 0:\nreturn torch.zeros((0, 4), device=masks.device)\nlarge_value = 1e8\ninv_mask = ~masks\nh, w = masks.shape[-2:]\ny = torch.arange(0, h, dtype=torch.float, device=masks.device)\nx = torch.arange(0, w, dtype=torch.float, device=masks.device)\ny, x = torch.meshgrid(y, x, indexing=\"ij\")\nx_mask = masks * x.unsqueeze(0)\nx_max = x_mask.flatten(1).max(-1)[0]\nx_min = x_mask.masked_fill(inv_mask, large_value).flatten(1).min(-1)[0]\ny_mask = masks * y.unsqueeze(0)\ny_max = y_mask.flatten(1).max(-1)[0]\ny_min = y_mask.masked_fill(inv_mask, large_value).flatten(1).min(-1)[0]\nbboxes = torch.stack((x_min, y_min, x_max, y_max), dim=1)\nbboxes[x_min == large_value] = empty_value\nreturn bboxes\n</code></pre>"},{"location":"api/ocl/models/","title":"ocl.models","text":"<p>Models defined in code.</p>"},{"location":"api/ocl/models/#ocl.models.SAVi","title":"<code>SAVi</code>","text":"<p>         Bases: <code>nn.Module</code></p> <p>Code based implementation of SAVi model.</p> Source code in <code>ocl/models/savi.py</code> <pre><code>class SAVi(nn.Module):\n\"\"\"Code based implementation of SAVi model.\"\"\"\ndef __init__(\nself,\nconditioning: nn.Module,\nfeature_extractor: nn.Module,\nperceptual_grouping: nn.Module,\ndecoder: nn.Module,\ntransition_model: nn.Module,\ninput_path: str = \"input.image\",\n):\nsuper().__init__()\nself.conditioning = conditioning\nself.feature_extractor = feature_extractor\nself.perceptual_grouping = perceptual_grouping\nself.decoder = decoder\nself.transition_model = transition_model\nself.input_path = input_path\ndef forward(self, inputs: Dict[str, Any]):\noutput = inputs\nvideo = get_tree_element(inputs, self.input_path)\nbatch_size = video.shape[0]\nfeatures = self.feature_extractor(video=video)\noutput[\"feature_extractor\"] = features\nconditioning = self.conditioning(batch_size=batch_size)\noutput[\"initial_conditioning\"] = conditioning\n# Loop over time.\nperceptual_grouping_outputs = []\ndecoder_outputs = []\ntransition_model_outputs = []\nfor frame_features in features:\nperceptual_grouping_output = self.perceptual_grouping(\nfeature=frame_features, conditioning=conditioning\n)\nslots = perceptual_grouping_output.objects\ndecoder_output = self.decoder(object_features=slots)\nconditioning = self.transition_model(slots)\n# Store outputs.\nperceptual_grouping_outputs.append(slots)\ndecoder_outputs.append(decoder_output)\ntransition_model_outputs.append(conditioning)\n# Stack all recurrent outputs.\nstacking_fn = partial(torch.stack, dim=1)\noutput[\"perceptual_grouping\"] = reduce_tree(perceptual_grouping_outputs, stacking_fn)\noutput[\"decoder\"] = reduce_tree(decoder_outputs, stacking_fn)\noutput[\"transition_model\"] = reduce_tree(transition_model_outputs, stacking_fn)\nreturn output\n</code></pre>"},{"location":"api/ocl/models/savi/","title":"ocl.models.savi","text":""},{"location":"api/ocl/models/savi/#ocl.models.savi.SAVi","title":"<code>SAVi</code>","text":"<p>         Bases: <code>nn.Module</code></p> <p>Code based implementation of SAVi model.</p> Source code in <code>ocl/models/savi.py</code> <pre><code>class SAVi(nn.Module):\n\"\"\"Code based implementation of SAVi model.\"\"\"\ndef __init__(\nself,\nconditioning: nn.Module,\nfeature_extractor: nn.Module,\nperceptual_grouping: nn.Module,\ndecoder: nn.Module,\ntransition_model: nn.Module,\ninput_path: str = \"input.image\",\n):\nsuper().__init__()\nself.conditioning = conditioning\nself.feature_extractor = feature_extractor\nself.perceptual_grouping = perceptual_grouping\nself.decoder = decoder\nself.transition_model = transition_model\nself.input_path = input_path\ndef forward(self, inputs: Dict[str, Any]):\noutput = inputs\nvideo = get_tree_element(inputs, self.input_path)\nbatch_size = video.shape[0]\nfeatures = self.feature_extractor(video=video)\noutput[\"feature_extractor\"] = features\nconditioning = self.conditioning(batch_size=batch_size)\noutput[\"initial_conditioning\"] = conditioning\n# Loop over time.\nperceptual_grouping_outputs = []\ndecoder_outputs = []\ntransition_model_outputs = []\nfor frame_features in features:\nperceptual_grouping_output = self.perceptual_grouping(\nfeature=frame_features, conditioning=conditioning\n)\nslots = perceptual_grouping_output.objects\ndecoder_output = self.decoder(object_features=slots)\nconditioning = self.transition_model(slots)\n# Store outputs.\nperceptual_grouping_outputs.append(slots)\ndecoder_outputs.append(decoder_output)\ntransition_model_outputs.append(conditioning)\n# Stack all recurrent outputs.\nstacking_fn = partial(torch.stack, dim=1)\noutput[\"perceptual_grouping\"] = reduce_tree(perceptual_grouping_outputs, stacking_fn)\noutput[\"decoder\"] = reduce_tree(decoder_outputs, stacking_fn)\noutput[\"transition_model\"] = reduce_tree(transition_model_outputs, stacking_fn)\nreturn output\n</code></pre>"},{"location":"api/ocl/neural_networks/","title":"ocl.neural_networks","text":""},{"location":"api/ocl/neural_networks/#ocl.neural_networks.build_two_layer_mlp","title":"<code>build_two_layer_mlp</code>","text":"<p>Build a two layer MLP, with optional initial layer norm.</p> <p>Separate class as this type of construction is used very often for slot attention and transformers.</p> Source code in <code>ocl/neural_networks/convenience.py</code> <pre><code>def build_two_layer_mlp(\ninput_dim, output_dim, hidden_dim, initial_layer_norm: bool = False, residual: bool = False\n):\n\"\"\"Build a two layer MLP, with optional initial layer norm.\n    Separate class as this type of construction is used very often for slot attention and\n    transformers.\n    \"\"\"\nreturn build_mlp(\ninput_dim, output_dim, [hidden_dim], initial_layer_norm=initial_layer_norm, residual=residual\n)\n</code></pre>"},{"location":"api/ocl/neural_networks/convenience/","title":"ocl.neural_networks.convenience","text":"<p>Convenience functions for the construction neural networks using config.</p>"},{"location":"api/ocl/neural_networks/convenience/#ocl.neural_networks.convenience.build_two_layer_mlp","title":"<code>build_two_layer_mlp</code>","text":"<p>Build a two layer MLP, with optional initial layer norm.</p> <p>Separate class as this type of construction is used very often for slot attention and transformers.</p> Source code in <code>ocl/neural_networks/convenience.py</code> <pre><code>def build_two_layer_mlp(\ninput_dim, output_dim, hidden_dim, initial_layer_norm: bool = False, residual: bool = False\n):\n\"\"\"Build a two layer MLP, with optional initial layer norm.\n    Separate class as this type of construction is used very often for slot attention and\n    transformers.\n    \"\"\"\nreturn build_mlp(\ninput_dim, output_dim, [hidden_dim], initial_layer_norm=initial_layer_norm, residual=residual\n)\n</code></pre>"},{"location":"api/ocl/neural_networks/extensions/","title":"ocl.neural_networks.extensions","text":"<p>Extensions of existing layers to implement additional functionality.</p>"},{"location":"api/ocl/neural_networks/extensions/#ocl.neural_networks.extensions.TransformerDecoderWithAttention","title":"<code>TransformerDecoderWithAttention</code>","text":"<p>         Bases: <code>nn.TransformerDecoder</code></p> <p>Modified nn.TransformerDecoder class that returns attention weights over memory.</p> Source code in <code>ocl/neural_networks/extensions.py</code> <pre><code>class TransformerDecoderWithAttention(nn.TransformerDecoder):\n\"\"\"Modified nn.TransformerDecoder class that returns attention weights over memory.\"\"\"\ndef __init__(\nself,\ndecoder_layer,\nnum_layers,\nnorm=None,\nreturn_attention_weights=False,\nattention_weight_type: Union[int, str] = \"mean\",\n):\nsuper(TransformerDecoderWithAttention, self).__init__(decoder_layer, num_layers, norm)\nif return_attention_weights:\nself.attention_hooks = []\nfor layer in self.layers:\nself.attention_hooks.append(self._prepare_layer(layer))\nelse:\nself.attention_hooks = None\nif isinstance(attention_weight_type, int):\nif attention_weight_type &gt;= num_layers or attention_weight_type &lt; -num_layers:\nraise ValueError(\nf\"Index {attention_weight_type} exceeds number of layers {num_layers}\"\n)\nelif attention_weight_type != \"mean\":\nraise ValueError(\"`weights` needs to be a number or 'mean'.\")\nself.weights = attention_weight_type\ndef _prepare_layer(self, layer):\nassert isinstance(layer, nn.TransformerDecoderLayer)\ndef _mha_block(self, x, mem, attn_mask, key_padding_mask, is_causal):\nx = self.multihead_attn(\nx,\nmem,\nmem,\nattn_mask=attn_mask,\nkey_padding_mask=key_padding_mask,\nis_causal=is_causal,\nneed_weights=True,\n)[0]\nreturn self.dropout2(x)\n# Patch _mha_block method to compute attention weights\nlayer._mha_block = _mha_block.__get__(layer, nn.TransformerDecoderLayer)\nclass AttentionHook:\ndef __init__(self):\nself._attention = None\ndef pop(self) -&gt; torch.Tensor:\nassert self._attention is not None, \"Forward was not called yet!\"\nattention = self._attention\nself._attention = None\nreturn attention\ndef __call__(self, module, inp, outp):\nself._attention = outp[1]\nhook = AttentionHook()\nlayer.multihead_attn.register_forward_hook(hook)\nreturn hook\ndef forward(\nself,\ntgt: torch.Tensor,\nmemory: torch.Tensor,\ntgt_mask: Optional[torch.Tensor] = None,\nmemory_mask: Optional[torch.Tensor] = None,\ntgt_key_padding_mask: Optional[torch.Tensor] = None,\nmemory_key_padding_mask: Optional[torch.Tensor] = None,\n) -&gt; torch.Tensor:\noutput = tgt\nfor mod in self.layers:\noutput = mod(\noutput,\nmemory,\ntgt_mask=tgt_mask,\nmemory_mask=memory_mask,\ntgt_key_padding_mask=tgt_key_padding_mask,\nmemory_key_padding_mask=memory_key_padding_mask,\n)\nif self.norm is not None:\noutput = self.norm(output)\nif self.attention_hooks is not None:\nattentions = []\nfor hook in self.attention_hooks:\nattentions.append(hook.pop())\nif self.weights == \"mean\":\nattentions = torch.stack(attentions, dim=-1)\n# Take mean over all layers\nattention = attentions.mean(dim=-1)\nelse:\nattention = attentions[self.weights]\nreturn output, attention.transpose(1, 2)\nelse:\nreturn output\n</code></pre>"},{"location":"api/ocl/neural_networks/feature_pyramid_networks/","title":"ocl.neural_networks.feature_pyramid_networks","text":""},{"location":"api/ocl/neural_networks/positional_embedding/","title":"ocl.neural_networks.positional_embedding","text":"<p>Implementation of different positional embeddings.</p>"},{"location":"api/ocl/neural_networks/positional_embedding/#ocl.neural_networks.positional_embedding.SoftPositionEmbed","title":"<code>SoftPositionEmbed</code>","text":"<p>         Bases: <code>nn.Module</code></p> <p>Embeding of positions using convex combination of learnable tensors.</p> <p>This assumes that the input positions are between 0 and 1.</p> Source code in <code>ocl/neural_networks/positional_embedding.py</code> <pre><code>class SoftPositionEmbed(nn.Module):\n\"\"\"Embeding of positions using convex combination of learnable tensors.\n    This assumes that the input positions are between 0 and 1.\n    \"\"\"\ndef __init__(\nself, n_spatial_dims: int, feature_dim: int, cnn_channel_order=False, savi_style=False\n):\n\"\"\"__init__.\n        Args:\n            n_spatial_dims (int): Number of spatial dimensions.\n            feature_dim (int): Dimensionality of the input features.\n            cnn_channel_order (bool): Assume features are in CNN channel order (i.e. C x H x W).\n            savi_style (bool): Use savi style positional encoding, where positions are normalized\n                between -1 and 1 and a single dense layer is used for embedding.\n        \"\"\"\nsuper().__init__()\nself.savi_style = savi_style\nn_features = n_spatial_dims if savi_style else 2 * n_spatial_dims\nself.dense = nn.Linear(in_features=n_features, out_features=feature_dim)\nself.cnn_channel_order = cnn_channel_order\ndef forward(self, inputs: torch.Tensor, positions: torch.Tensor):\nif self.savi_style:\n# Rescale positional encoding to -1 to 1\npositions = (positions - 0.5) * 2\nelse:\npositions = torch.cat([positions, 1 - positions], axis=-1)\nemb_proj = self.dense(positions)\nif self.cnn_channel_order:\nemb_proj = emb_proj.permute(*range(inputs.ndim - 3), -1, -3, -2)\nreturn inputs + emb_proj\n</code></pre>"},{"location":"api/ocl/neural_networks/positional_embedding/#ocl.neural_networks.positional_embedding.SoftPositionEmbed.__init__","title":"<code>__init__</code>","text":"<p>init.</p> <p>Parameters:</p> Name Type Description Default <code>n_spatial_dims</code> <code>int</code> <p>Number of spatial dimensions.</p> required <code>feature_dim</code> <code>int</code> <p>Dimensionality of the input features.</p> required <code>cnn_channel_order</code> <code>bool</code> <p>Assume features are in CNN channel order (i.e. C x H x W).</p> <code>False</code> <code>savi_style</code> <code>bool</code> <p>Use savi style positional encoding, where positions are normalized between -1 and 1 and a single dense layer is used for embedding.</p> <code>False</code> Source code in <code>ocl/neural_networks/positional_embedding.py</code> <pre><code>def __init__(\nself, n_spatial_dims: int, feature_dim: int, cnn_channel_order=False, savi_style=False\n):\n\"\"\"__init__.\n    Args:\n        n_spatial_dims (int): Number of spatial dimensions.\n        feature_dim (int): Dimensionality of the input features.\n        cnn_channel_order (bool): Assume features are in CNN channel order (i.e. C x H x W).\n        savi_style (bool): Use savi style positional encoding, where positions are normalized\n            between -1 and 1 and a single dense layer is used for embedding.\n    \"\"\"\nsuper().__init__()\nself.savi_style = savi_style\nn_features = n_spatial_dims if savi_style else 2 * n_spatial_dims\nself.dense = nn.Linear(in_features=n_features, out_features=feature_dim)\nself.cnn_channel_order = cnn_channel_order\n</code></pre>"},{"location":"api/ocl/neural_networks/positional_embedding/#ocl.neural_networks.positional_embedding.LearnedAdditivePositionalEmbed","title":"<code>LearnedAdditivePositionalEmbed</code>","text":"<p>         Bases: <code>nn.Module</code></p> <p>Add positional encoding as in SLATE.</p> Source code in <code>ocl/neural_networks/positional_embedding.py</code> <pre><code>class LearnedAdditivePositionalEmbed(nn.Module):\n\"\"\"Add positional encoding as in SLATE.\"\"\"\ndef __init__(self, max_len, d_model, dropout=0.0):\nsuper().__init__()\nself.dropout = nn.Dropout(dropout)\nself.pe = nn.Parameter(torch.zeros(1, max_len, d_model), requires_grad=True)\nnn.init.trunc_normal_(self.pe)\ndef forward(self, input):\nT = input.shape[1]\nreturn self.dropout(input + self.pe[:, :T])\n</code></pre>"},{"location":"api/ocl/neural_networks/positional_embedding/#ocl.neural_networks.positional_embedding.DummyPositionEmbed","title":"<code>DummyPositionEmbed</code>","text":"<p>         Bases: <code>nn.Module</code></p> <p>Embedding that just passes through inputs without adding any positional embeddings.</p> Source code in <code>ocl/neural_networks/positional_embedding.py</code> <pre><code>class DummyPositionEmbed(nn.Module):\n\"\"\"Embedding that just passes through inputs without adding any positional embeddings.\"\"\"\ndef __init__(self):\nsuper().__init__()\ndef forward(self, inputs: torch.Tensor, positions: torch.Tensor):\nreturn inputs\n</code></pre>"},{"location":"api/ocl/neural_networks/slate/","title":"ocl.neural_networks.slate","text":"<p>Neural networks used for the implemenation of SLATE.</p>"},{"location":"api/ocl/neural_networks/wrappers/","title":"ocl.neural_networks.wrappers","text":"<p>Wrapper modules with allow the introduction or residuals or the combination of other modules.</p>"},{"location":"api/ocl/neural_networks/wrappers/#ocl.neural_networks.wrappers.Sequential","title":"<code>Sequential</code>","text":"<p>         Bases: <code>nn.Module</code></p> <p>Extended sequential module that supports multiple inputs and outputs to layers.</p> <p>This allows a stack of layers where for example the first layer takes two inputs and only has a single output or where a layer has multiple outputs and the downstream layer takes multiple inputs.</p> Source code in <code>ocl/neural_networks/wrappers.py</code> <pre><code>class Sequential(nn.Module):\n\"\"\"Extended sequential module that supports multiple inputs and outputs to layers.\n    This allows a stack of layers where for example the first layer takes two inputs and only has\n    a single output or where a layer has multiple outputs and the downstream layer takes multiple\n    inputs.\n    \"\"\"\ndef __init__(self, *layers):\nsuper().__init__()\nself.layers = nn.ModuleList(layers)\ndef forward(self, *inputs):\noutputs = inputs\nfor layer in self.layers:\nif isinstance(outputs, (tuple, list)):\noutputs = layer(*outputs)\nelse:\noutputs = layer(outputs)\nreturn outputs\n</code></pre>"},{"location":"api/ocl/utils/","title":"ocl.utils","text":""},{"location":"api/ocl/utils/#ocl.utils.JoinWindows","title":"<code>JoinWindows</code>","text":"<p>         Bases: <code>torch.nn.Module</code></p> <p>Join individual windows to single output.</p> Source code in <code>ocl/utils/windows.py</code> <pre><code>class JoinWindows(torch.nn.Module):\n\"\"\"Join individual windows to single output.\"\"\"\ndef __init__(self, n_windows: int, size):\nsuper().__init__()\nself.n_windows = n_windows\nself.size = size\ndef forward(self, masks: torch.Tensor, keys: str) -&gt; torch.Tensor:\nassert len(masks) == self.n_windows\nkeys_split = [key.split(\"_\") for key in keys]\npad_left = [int(elems[1]) for elems in keys_split]\npad_top = [int(elems[2]) for elems in keys_split]\ntarget_height, target_width = self.size\nn_masks = masks.shape[0] * masks.shape[1]\nheight, width = masks.shape[2], masks.shape[3]\nfull_mask = torch.zeros(n_masks, *self.size).to(masks)\nx = 0\ny = 0\nfor idx, mask in enumerate(masks):\nelems = masks.shape[1]\nx_start = 0 if pad_left[idx] &gt;= 0 else -pad_left[idx]\nx_end = min(width, target_width - pad_left[idx])\ny_start = 0 if pad_top[idx] &gt;= 0 else -pad_top[idx]\ny_end = min(height, target_height - pad_top[idx])\ncropped = mask[:, y_start:y_end, x_start:x_end]\nfull_mask[\nidx * elems : (idx + 1) * elems, y : y + cropped.shape[-2], x : x + cropped.shape[-1]\n] = cropped\nx += cropped.shape[-1]\nif x &gt; target_width:\ny += cropped.shape[-2]\nx = 0\nassert torch.all(torch.abs(torch.sum(full_mask, axis=0) - 1) &lt;= 1e-2)\nreturn full_mask.unsqueeze(0)\n</code></pre>"},{"location":"api/ocl/utils/bboxes/","title":"ocl.utils.bboxes","text":"<p>Utilities for handling bboxes.</p>"},{"location":"api/ocl/utils/dataset_patches/","title":"ocl.utils.dataset_patches","text":"<p>Patches <code>torchdata</code> for behavior to be consistent with webdatasets.</p>"},{"location":"api/ocl/utils/dataset_patches/#ocl.utils.dataset_patches.ChainedGenerator","title":"<code>ChainedGenerator</code>","text":"<p>         Bases: <code>IterDataPipe</code></p> <p>Simple interface to allow chaining via a generator function.</p> <p>This mirrors functionality from the webdatasets package.</p> Source code in <code>ocl/utils/dataset_patches.py</code> <pre><code>@torchdata.datapipes.functional_datapipe(\"then\")\nclass ChainedGenerator(IterDataPipe):\n\"\"\"Simple interface to allow chaining via a generator function.\n    This mirrors functionality from the webdatasets package.\n    \"\"\"\ndef __init__(self, source_datapipe: IterDataPipe, generator):\nself.source_datapipe = source_datapipe\nself.generator = generator\ndef __iter__(self):\nyield from self.generator(self.source_datapipe)\n</code></pre>"},{"location":"api/ocl/utils/dataset_patches/#ocl.utils.dataset_patches.patched_pathsplit","title":"<code>patched_pathsplit</code>","text":"<p>Split a path into a WebDataset prefix and suffix.</p> <p>The version of pathsplit in torchdata behaves differently from WebDatasets by keeping \".\" in the suffix. This is patched here, by excluding the separating dot from the regex match.</p> <p>The prefix is used for grouping files into samples, the suffix is used as key in the output dictionary. The suffix consists of all components after the first \".\" in the filename.</p> <p>In torchdata, the prefix consists of the .tar file path followed by the file name inside the archive.</p> <p>Any backslash in the prefix is replaced by a forward slash to make Windows prefixes consistent with POSIX paths.</p> Source code in <code>ocl/utils/dataset_patches.py</code> <pre><code>def patched_pathsplit(p):\n\"\"\"Split a path into a WebDataset prefix and suffix.\n    The version of pathsplit in torchdata behaves\n    differently from WebDatasets by keeping \".\" in the\n    suffix. This is patched here, by excluding the\n    separating dot from the regex match.\n    The prefix is used for grouping files into samples,\n    the suffix is used as key in the output dictionary.\n    The suffix consists of all components after the first\n    \".\" in the filename.\n    In torchdata, the prefix consists of the .tar file\n    path followed by the file name inside the archive.\n    Any backslash in the prefix is replaced by a forward\n    slash to make Windows prefixes consistent with POSIX\n    paths.\n    \"\"\"\n# convert Windows pathnames to UNIX pathnames, otherwise\n# we get an inconsistent mix of the Windows path to the tar\n# file followed by the POSIX path inside that tar file\np = p.replace(\"\\\\\", \"/\")\nif \".\" not in p:\nreturn p, \"\"\n# we need to use a regular expression because os.path is\n# platform specific, but tar files always contain POSIX paths\n# Patched here, exclude the extension dot from the regex expression.\nmatch = re.search(r\"^.*/(.*?)\\.([^/]*)$\", p)\npass\nif not match:\nreturn p, \"\"\nprefix, suffix = match.groups()\nreturn prefix, suffix\n</code></pre>"},{"location":"api/ocl/utils/logging/","title":"ocl.utils.logging","text":""},{"location":"api/ocl/utils/logging/#ocl.utils.logging.ExtendedMLflowExperiment","title":"<code>ExtendedMLflowExperiment</code>","text":"<p>MLflow experiment made to mimic tensorboard experiments.</p> Source code in <code>ocl/utils/logging.py</code> <pre><code>class ExtendedMLflowExperiment:\n\"\"\"MLflow experiment made to mimic tensorboard experiments.\"\"\"\ndef __init__(self, mlflow_client: MlflowClient, run_id: str):\nself._mlflow_client = mlflow_client\nself._run_id = run_id\nself._tempdir = TemporaryDirectory()\ndef _get_tmp_prefix_for_step(self, step: int):\nreturn os.path.join(self._tempdir.name, f\"{step:07d}\")\ndef add_video(self, vid_tensor, fps: int, tag: str, global_step: int):\npath = tag  # TF paths are typically split using \"/\"\nfilename = write_video_tensor(\nself._get_tmp_prefix_for_step(global_step), prepare_video_tensor(vid_tensor), fps\n)\nself._mlflow_client.log_artifact(self._run_id, filename, path)\nos.remove(filename)\ndef add_image(self, img_tensor: torch.Tensor, dataformats: str, tag: str, global_step: int):\npath = tag\nfilename = write_image_tensor(\nself._get_tmp_prefix_for_step(global_step),\nprepare_image_tensor(img_tensor, dataformats=dataformats),\n)\nself._mlflow_client.log_artifact(self._run_id, filename, path)\nos.remove(filename)\ndef add_images(self, img_tensor, dataformats: str, tag: str, global_step: int):\n# Internally works by having an additional N dimension in `dataformats`.\nself.add_image(img_tensor, dataformats, tag, global_step)\ndef add_figure(self, figure, close: bool, tag: str, global_step: int):\nif isinstance(figure, list):\nself.add_image(\nfigure_to_image(figure, close),\ndataformats=\"NCHW\",\ntag=tag,\nglobal_step=global_step,\n)\nelse:\nself.add_image(\nfigure_to_image(figure, close),\ndataformats=\"CHW\",\ntag=tag,\nglobal_step=global_step,\n)\ndef __getattr__(self, name):\n\"\"\"Fallback to mlflow client for missing attributes.\n        Fallback to make the experiment object still behave like the regular MLflow client.  While\n        this is suboptimal, it does allow us to save a lot of handcrafted code by relying on\n        inheritance and pytorch lightings implementation of the MLflow logger.\n        \"\"\"\nreturn getattr(self._mlflow_client, name)\n</code></pre>"},{"location":"api/ocl/utils/logging/#ocl.utils.logging.ExtendedMLflowExperiment.__getattr__","title":"<code>__getattr__</code>","text":"<p>Fallback to mlflow client for missing attributes.</p> <p>Fallback to make the experiment object still behave like the regular MLflow client.  While this is suboptimal, it does allow us to save a lot of handcrafted code by relying on inheritance and pytorch lightings implementation of the MLflow logger.</p> Source code in <code>ocl/utils/logging.py</code> <pre><code>def __getattr__(self, name):\n\"\"\"Fallback to mlflow client for missing attributes.\n    Fallback to make the experiment object still behave like the regular MLflow client.  While\n    this is suboptimal, it does allow us to save a lot of handcrafted code by relying on\n    inheritance and pytorch lightings implementation of the MLflow logger.\n    \"\"\"\nreturn getattr(self._mlflow_client, name)\n</code></pre>"},{"location":"api/ocl/utils/masking/","title":"ocl.utils.masking","text":"<p>Utilities related to masking.</p>"},{"location":"api/ocl/utils/masking/#ocl.utils.masking.CreateSlotMask","title":"<code>CreateSlotMask</code>","text":"<p>         Bases: <code>nn.Module</code></p> <p>Module intended to create a mask that marks empty slots.</p> <p>Module takes a tensor holding the number of slots per batch entry, and returns a binary mask of shape (batch_size, max_slots) where entries exceeding the number of slots are masked out.</p> Source code in <code>ocl/utils/masking.py</code> <pre><code>class CreateSlotMask(nn.Module):\n\"\"\"Module intended to create a mask that marks empty slots.\n    Module takes a tensor holding the number of slots per batch entry, and returns a binary mask of\n    shape (batch_size, max_slots) where entries exceeding the number of slots are masked out.\n    \"\"\"\ndef __init__(self, max_slots: int):\nsuper().__init__()\nself.max_slots = max_slots\ndef forward(self, n_slots: torch.Tensor) -&gt; torch.Tensor:\n(batch_size,) = n_slots.shape\n# Create mask of shape B x K where the first n_slots entries per-row are false, the rest true\nindices = torch.arange(self.max_slots, device=n_slots.device)\nmasks = indices.unsqueeze(0).expand(batch_size, -1) &gt;= n_slots.unsqueeze(1)\nreturn masks\n</code></pre>"},{"location":"api/ocl/utils/masking/#ocl.utils.masking.CreateRandomMaskPatterns","title":"<code>CreateRandomMaskPatterns</code>","text":"<p>         Bases: <code>nn.Module</code></p> <p>Create random masks.</p> <p>Useful for showcasing behavior of metrics.</p> Source code in <code>ocl/utils/masking.py</code> <pre><code>class CreateRandomMaskPatterns(nn.Module):\n\"\"\"Create random masks.\n    Useful for showcasing behavior of metrics.\n    \"\"\"\ndef __init__(self, pattern: str, n_slots: Optional[int] = None, n_cols: int = 2):\nsuper().__init__()\nif pattern not in (\"random\", \"blocks\"):\nraise ValueError(f\"Unknown pattern {pattern}\")\nself.pattern = pattern\nself.n_slots = n_slots\nself.n_cols = n_cols\ndef forward(self, masks: torch.Tensor) -&gt; torch.Tensor:\nif self.pattern == \"random\":\nrand_mask = torch.rand_like(masks)\nreturn rand_mask / rand_mask.sum(1, keepdim=True)\nelif self.pattern == \"blocks\":\nn_slots = masks.shape[1] if self.n_slots is None else self.n_slots\nheight, width = masks.shape[-2:]\nnew_masks = torch.zeros(\nlen(masks), n_slots, height, width, device=masks.device, dtype=masks.dtype\n)\nblocks_per_col = int(n_slots // self.n_cols)\nremainder = n_slots - (blocks_per_col * self.n_cols)\nslot = 0\nfor col in range(self.n_cols):\nrows = blocks_per_col if col &lt; self.n_cols - 1 else blocks_per_col + remainder\nfor row in range(rows):\nblock_width = math.ceil(width / self.n_cols)\nblock_height = math.ceil(height / rows)\nx = col * block_width\ny = row * block_height\nnew_masks[:, slot, y : y + block_height, x : x + block_width] = 1\nslot += 1\nassert torch.allclose(new_masks.sum(1), torch.ones_like(masks[:, 0]))\nreturn new_masks\n</code></pre>"},{"location":"api/ocl/utils/resizing/","title":"ocl.utils.resizing","text":"<p>Utilities related to resizing of tensors.</p>"},{"location":"api/ocl/utils/resizing/#ocl.utils.resizing.Resize","title":"<code>Resize</code>","text":"<p>         Bases: <code>nn.Module</code></p> <p>Module resizing tensors.</p> Source code in <code>ocl/utils/resizing.py</code> <pre><code>class Resize(nn.Module):\n\"\"\"Module resizing tensors.\"\"\"\nMODES = {\"nearest\", \"linear\", \"bilinear\", \"bicubic\", \"trilinear\", \"area\", \"nearest-exact\"}\ndef __init__(\nself,\nsize: Optional[Union[int, Tuple[int, int]]] = None,\nresize_mode: str = \"bilinear\",\npatch_mode: bool = False,\nchannels_last: bool = False,\n):\nsuper().__init__()\nself.size = size\nif resize_mode not in Resize.MODES:\nraise ValueError(f\"`mode` must be one of {Resize.MODES}\")\nself.resize_mode = resize_mode\nself.patch_mode = patch_mode\nself.channels_last = channels_last\nself.expected_dims = 3 if patch_mode else 4\ndef forward(\nself, input: torch.Tensor, size_tensor: Optional[torch.Tensor] = None\n) -&gt; torch.Tensor:\n\"\"\"Resize tensor.\n        Args:\n            input: Tensor to resize. If `patch_mode=False`, assumed to be of shape (..., C, H, W).\n                If `patch_mode=True`, assumed to be of shape (..., C, P), where P is the number of\n                patches. Patches are assumed to be viewable as a perfect square image. If\n                `channels_last=True`, channel dimension is assumed to be the last dimension instead.\n            size_tensor: Tensor which size to resize to. If tensor has &lt;=2 dimensions and the last\n                dimension of this tensor has length 2, the two entries are taken as height and width.\n                Otherwise, the size of the last two dimensions of this tensor are used as height\n                and width.\n        Returns: Tensor of shape (..., C, H, W), where height and width are either specified by\n            `size` or `size_tensor`.\n        \"\"\"\ndims_to_flatten = input.ndim - self.expected_dims\nif dims_to_flatten &gt; 0:\nflattened_dims = input.shape[: dims_to_flatten + 1]\ninput = input.flatten(0, dims_to_flatten)\nelif dims_to_flatten &lt; 0:\nraise ValueError(\nf\"Tensor needs at least {self.expected_dims} dimensions, but only has {input.ndim}\"\n)\nif self.patch_mode:\nif self.channels_last:\ninput = input.transpose(-2, -1)\nn_channels, n_patches = input.shape[-2:]\npatch_size_float = math.sqrt(n_patches)\npatch_size = int(math.sqrt(n_patches))\nif patch_size_float != patch_size:\nraise ValueError(\nf\"The number of patches needs to be a perfect square, but is {n_patches}.\"\n)\ninput = input.view(-1, n_channels, patch_size, patch_size)\nelse:\nif self.channels_last:\ninput = input.permute(0, 3, 1, 2)\nif self.size is None:\nif size_tensor is None:\nraise ValueError(\"`size` is `None` but no `size_tensor` was passed.\")\nif size_tensor.ndim &lt;= 2 and size_tensor.shape[-1] == 2:\nheight, width = size_tensor.unbind(-1)\nheight = torch.atleast_1d(height)[0].squeeze().detach().cpu()\nwidth = torch.atleast_1d(width)[0].squeeze().detach().cpu()\nsize = (int(height), int(width))\nelse:\nsize = size_tensor.shape[-2:]\nelse:\nsize = self.size\ninput = torch.nn.functional.interpolate(\ninput,\nsize=size,\nmode=self.resize_mode,\n)\nif dims_to_flatten &gt; 0:\ninput = input.unflatten(0, flattened_dims)\nreturn input\n</code></pre>"},{"location":"api/ocl/utils/resizing/#ocl.utils.resizing.Resize.forward","title":"<code>forward</code>","text":"<p>Resize tensor.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>torch.Tensor</code> <p>Tensor to resize. If <code>patch_mode=False</code>, assumed to be of shape (..., C, H, W). If <code>patch_mode=True</code>, assumed to be of shape (..., C, P), where P is the number of patches. Patches are assumed to be viewable as a perfect square image. If <code>channels_last=True</code>, channel dimension is assumed to be the last dimension instead.</p> required <code>size_tensor</code> <code>Optional[torch.Tensor]</code> <p>Tensor which size to resize to. If tensor has &lt;=2 dimensions and the last dimension of this tensor has length 2, the two entries are taken as height and width. Otherwise, the size of the last two dimensions of this tensor are used as height and width.</p> <code>None</code> <p>Tensor of shape (..., C, H, W), where height and width are either specified by</p> Type Description <code>torch.Tensor</code> <p><code>size</code> or <code>size_tensor</code>.</p> Source code in <code>ocl/utils/resizing.py</code> <pre><code>def forward(\nself, input: torch.Tensor, size_tensor: Optional[torch.Tensor] = None\n) -&gt; torch.Tensor:\n\"\"\"Resize tensor.\n    Args:\n        input: Tensor to resize. If `patch_mode=False`, assumed to be of shape (..., C, H, W).\n            If `patch_mode=True`, assumed to be of shape (..., C, P), where P is the number of\n            patches. Patches are assumed to be viewable as a perfect square image. If\n            `channels_last=True`, channel dimension is assumed to be the last dimension instead.\n        size_tensor: Tensor which size to resize to. If tensor has &lt;=2 dimensions and the last\n            dimension of this tensor has length 2, the two entries are taken as height and width.\n            Otherwise, the size of the last two dimensions of this tensor are used as height\n            and width.\n    Returns: Tensor of shape (..., C, H, W), where height and width are either specified by\n        `size` or `size_tensor`.\n    \"\"\"\ndims_to_flatten = input.ndim - self.expected_dims\nif dims_to_flatten &gt; 0:\nflattened_dims = input.shape[: dims_to_flatten + 1]\ninput = input.flatten(0, dims_to_flatten)\nelif dims_to_flatten &lt; 0:\nraise ValueError(\nf\"Tensor needs at least {self.expected_dims} dimensions, but only has {input.ndim}\"\n)\nif self.patch_mode:\nif self.channels_last:\ninput = input.transpose(-2, -1)\nn_channels, n_patches = input.shape[-2:]\npatch_size_float = math.sqrt(n_patches)\npatch_size = int(math.sqrt(n_patches))\nif patch_size_float != patch_size:\nraise ValueError(\nf\"The number of patches needs to be a perfect square, but is {n_patches}.\"\n)\ninput = input.view(-1, n_channels, patch_size, patch_size)\nelse:\nif self.channels_last:\ninput = input.permute(0, 3, 1, 2)\nif self.size is None:\nif size_tensor is None:\nraise ValueError(\"`size` is `None` but no `size_tensor` was passed.\")\nif size_tensor.ndim &lt;= 2 and size_tensor.shape[-1] == 2:\nheight, width = size_tensor.unbind(-1)\nheight = torch.atleast_1d(height)[0].squeeze().detach().cpu()\nwidth = torch.atleast_1d(width)[0].squeeze().detach().cpu()\nsize = (int(height), int(width))\nelse:\nsize = size_tensor.shape[-2:]\nelse:\nsize = self.size\ninput = torch.nn.functional.interpolate(\ninput,\nsize=size,\nmode=self.resize_mode,\n)\nif dims_to_flatten &gt; 0:\ninput = input.unflatten(0, flattened_dims)\nreturn input\n</code></pre>"},{"location":"api/ocl/utils/resizing/#ocl.utils.resizing.resize_patches_to_image","title":"<code>resize_patches_to_image</code>","text":"<p>Convert and resize a tensor of patches to image shape.</p> <p>This method requires that the patches can be converted to a square image.</p> <p>Parameters:</p> Name Type Description Default <code>patches</code> <code>torch.Tensor</code> <p>Patches to be converted of shape (..., C, P), where C is the number of channels and P the number of patches.</p> required <code>size</code> <code>Optional[int]</code> <p>Image size to resize to.</p> <code>None</code> <code>scale_factor</code> <code>Optional[float]</code> <p>Scale factor by which to resize the patches. Can be specified alternatively to <code>size</code>.</p> <code>None</code> <code>resize_mode</code> <code>str</code> <p>Method to resize with. Valid options are \"nearest\", \"nearest-exact\", \"bilinear\", \"bicubic\".</p> <code>'bilinear'</code> <p>Returns:</p> Type Description <code>torch.Tensor</code> <p>Tensor of shape (..., C, S, S) where S is the image size.</p> Source code in <code>ocl/utils/resizing.py</code> <pre><code>def resize_patches_to_image(\npatches: torch.Tensor,\nsize: Optional[int] = None,\nscale_factor: Optional[float] = None,\nresize_mode: str = \"bilinear\",\n) -&gt; torch.Tensor:\n\"\"\"Convert and resize a tensor of patches to image shape.\n    This method requires that the patches can be converted to a square image.\n    Args:\n        patches: Patches to be converted of shape (..., C, P), where C is the number of channels and\n            P the number of patches.\n        size: Image size to resize to.\n        scale_factor: Scale factor by which to resize the patches. Can be specified alternatively to\n            `size`.\n        resize_mode: Method to resize with. Valid options are \"nearest\", \"nearest-exact\", \"bilinear\",\n            \"bicubic\".\n    Returns:\n        Tensor of shape (..., C, S, S) where S is the image size.\n    \"\"\"\nhas_size = size is None\nhas_scale = scale_factor is None\nif has_size == has_scale:\nraise ValueError(\"Exactly one of `size` or `scale_factor` must be specified.\")\nn_channels = patches.shape[-2]\nn_patches = patches.shape[-1]\npatch_size_float = math.sqrt(n_patches)\npatch_size = int(math.sqrt(n_patches))\nif patch_size_float != patch_size:\nraise ValueError(\"The number of patches needs to be a perfect square.\")\nimage = torch.nn.functional.interpolate(\npatches.view(-1, n_channels, patch_size, patch_size),\nsize=size,\nscale_factor=scale_factor,\nmode=resize_mode,\n)\nreturn image.view(*patches.shape[:-1], image.shape[-2], image.shape[-1])\n</code></pre>"},{"location":"api/ocl/utils/routing/","title":"ocl.utils.routing","text":"<p>Utility function related to routing of information.</p> <p>These utility functions allow dynamical routing between modules and allow the specification of complex models using config alone.</p>"},{"location":"api/ocl/utils/routing/#ocl.utils.routing.RoutableMixin","title":"<code>RoutableMixin</code>","text":"<p>Mixin class that allows to connect any element of a (nested) dict with a module input.</p> Source code in <code>ocl/utils/routing.py</code> <pre><code>class RoutableMixin:\n\"\"\"Mixin class that allows to connect any element of a (nested) dict with a module input.\"\"\"\ndef __init__(self, input_mapping: Mapping[str, Optional[str]]):\nself.input_mapping = {\nkey: value.split(\".\") for key, value in input_mapping.items() if value is not None\n}\ndef _route(method, filter_parameters=True):\n\"\"\"Pass arguments to a function based on the mapping defined in `self.input_mapping`.\n        This method supports both filtering for parameters that match the arguments of the wrapped\n        method and passing all arguments defined in `input_mapping`.  If a non-optional argument is\n        missing this will raise an exception.  Additional arguments can also be passed to the method\n        to override entries in the input dict.  Non-keyword arguments are always directly passed to\n        the method.\n        Args:\n            method: The method to pass the arguments to.\n            filter_parameters: Only pass arguments to wrapped method that match the methods\n                signature.  This is practical if different methods require different types of input.\n        \"\"\"\n# Run inspection here to reduce compute time when calling method.\nsignature = inspect.signature(method)\nvalid_parameters = list(signature.parameters)  # Returns the parameter names.\nvalid_parameters = valid_parameters[1:]  # Discard \"self\".\n# Keep track of default parameters. For these we should not fail if they are not in\n# the input dict.\nwith_defaults = [\nname\nfor name, param in signature.parameters.items()\nif param.default is not inspect.Parameter.empty\n]\n@functools.wraps(method)\ndef method_with_routing(self, *args, inputs=None, **kwargs):\nif not inputs:\ninputs = {}\nif self.input_mapping:\nif not inputs:  # Empty dict.\ninputs = kwargs\nrouted_inputs = {}\nfor input_field, input_path in self.input_mapping.items():\nif filter_parameters and input_field not in valid_parameters:\n# Skip parameters that are not the function signature.\ncontinue\nif input_field in kwargs.keys():\n# Skip parameters that are directly provided as kwargs.\ncontinue\ntry:\nelement = tree_utils.get_tree_element(inputs, input_path)\nrouted_inputs[input_field] = element\nexcept ValueError as e:\nif input_field in with_defaults:\ncontinue\nelse:\nraise e\n# Support for additional parameters passed via keyword arguments.\n# TODO(hornmax): This is not ideal as it mixes routing args from the input dict\n# and explicitly passed kwargs and thus could lead to collisions.\nfor name, element in kwargs.items():\nif filter_parameters and name not in valid_parameters:\ncontinue\nelse:\nrouted_inputs[name] = element\nreturn method(self, *args, **routed_inputs)\nelse:\nreturn method(self, *args, **kwargs)\nreturn method_with_routing\n# This is needed in order to allow the decorator to be used in child classes. The documentation\n# looks a bit hacky but I didn't find an alternative approach on how to do it.\nroute = staticmethod(functools.partial(_route, filter_parameters=True))\nroute.__doc__ = (\n\"\"\"Route input arguments according to input_mapping and filter non-matching arguments.\"\"\"\n)\nroute_unfiltered = staticmethod(functools.partial(_route, filter_parameters=False))\nroute_unfiltered.__doc__ = \"\"\"Route all input arguments according to input_mapping.\"\"\"\n</code></pre>"},{"location":"api/ocl/utils/routing/#ocl.utils.routing.DataRouter","title":"<code>DataRouter</code>","text":"<p>         Bases: <code>nn.Module</code>, <code>RoutableMixin</code></p> <p>Data router for modules that don't support the RoutableMixin.</p> <p>This allows the usage of modules without RoutableMixin support in the dynamic information flow pattern of the code.</p> Source code in <code>ocl/utils/routing.py</code> <pre><code>class DataRouter(nn.Module, RoutableMixin):\n\"\"\"Data router for modules that don't support the RoutableMixin.\n    This allows the usage of modules without RoutableMixin support in the dynamic information flow\n    pattern of the code.\n    \"\"\"\ndef __init__(self, module: nn.Module, input_mapping: Mapping[str, str]):\nnn.Module.__init__(self)\nRoutableMixin.__init__(self, input_mapping)\nself.module = module\nself._cached_valid_parameters = None\n@RoutableMixin.route_unfiltered\ndef forward(self, *args, **kwargs):\n# We need to filter parameters at runtime as we cannot know them prior to initialization.\nif not self._cached_valid_parameters:\ntry:\nsignature = inspect.signature(self.module.forward)\nexcept AttributeError:\nif callable(self.module):\nsignature = inspect.signature(self.module.__call__)\nelse:\nsignature = inspect.signature(self.module)\nself._cached_valid_parameters = list(signature.parameters)\nkwargs = {\nname: param for name, param in kwargs.items() if name in self._cached_valid_parameters\n}\nreturn self.module(*args, **kwargs)\n</code></pre>"},{"location":"api/ocl/utils/routing/#ocl.utils.routing.Combined","title":"<code>Combined</code>","text":"<p>         Bases: <code>nn.ModuleDict</code></p> <p>Module to combine multiple modules and store their outputs.</p> <p>A combined module groups together multiple model components and allows them to access any information that was returned in processing steps prior to their own application.</p> <p>It functions similarly to <code>nn.ModuleDict</code> yet for modules of type <code>RoutableMixin</code> and additionally implements a forward routine which will return a dict of the outputs of the submodules.</p> Source code in <code>ocl/utils/routing.py</code> <pre><code>class Combined(nn.ModuleDict):\n\"\"\"Module to combine multiple modules and store their outputs.\n    A combined module groups together multiple model components and allows them to access any\n    information that was returned in processing steps prior to their own application.\n    It functions similarly to `nn.ModuleDict` yet for modules of type `RoutableMixin` and\n    additionally implements a forward routine which will return a dict of the outputs of the\n    submodules.\n    \"\"\"\ndef __init__(self, **modules: Dict[str, Union[RoutableMixin, Combined, Recurrent]]):\nsuper().__init__(modules)\ndef forward(self, inputs: Dict[str, Any]):\n# The combined module does not know where it is positioned and thus also does not know in\n# which sub-path results should be written. As we want different modules of a combined\n# module to be able access previous outputs using their global path in the dictionary, we\n# need to somehow keep track of the nesting level and then directly write results into the\n# input dict at the right path.  The prefix variable keeps track of the nesting level.\nprefix: List[str]\nif \"prefix\" in inputs.keys():\nprefix = inputs[\"prefix\"]\nelse:\nprefix = []\ninputs[\"prefix\"] = prefix\noutputs = tree_utils.get_tree_element(inputs, prefix)\nfor name, module in self.items():\n# Update prefix state such that nested calls of combined return dict in the correct\n# location.\nprefix.append(name)\noutputs[name] = {}\n# If module is a Combined module, it will return the same dict as set above. If not the\n# dict will be overwritten with the output of the module.\noutputs[name] = module(inputs=inputs)\n# Remove last component of prefix after execution.\nprefix.pop()\nreturn outputs\n</code></pre>"},{"location":"api/ocl/utils/routing/#ocl.utils.routing.Recurrent","title":"<code>Recurrent</code>","text":"<p>         Bases: <code>nn.Module</code></p> <p>Module to apply another module in a recurrent fashion over a axis.</p> <p>This module takes a set of input tensors and applies a module recurrent over them.  The output of the previous iteration is kept in the <code>previous_output</code> key of input dict and thus can be accessed using data routing. After applying the module to the input slices, the outputs are stacked along the same axis as the inputs where split.</p> Source code in <code>ocl/utils/routing.py</code> <pre><code>class Recurrent(nn.Module):\n\"\"\"Module to apply another module in a recurrent fashion over a axis.\n    This module takes a set of input tensors and applies a module recurrent over them.  The output\n    of the previous iteration is kept in the `previous_output` key of input dict and thus can be\n    accessed using data routing. After applying the module to the input slices, the outputs are\n    stacked along the same axis as the inputs where split.\n    \"\"\"\ndef __init__(\nself,\nmodule: nn.Module,\ninputs_to_split: List[str],\ninitial_input_mapping: Dict[str, str],\nsplit_axis: int = 1,\nchunk_size: int = 1,\n):\n\"\"\"Initialize recurrent module.\n        Args:\n            module: The module that should be applied recurrently along input tensors.\n            inputs_to_split: List of paths that should be split for recurrent application.\n            initial_input_mapping: Mapping that constructs the first `previous_output` element.  If\n                `previous_output` should just be a tensor, use a mapping of the format\n                `{\"\": \"input_path\"}`.\n            split_axis: Axis along which to split the tensors defined by inputs_to_split.\n            chunk_size: The size of each slice, when set to 1, the slice dimension is squeezed prior\n                to passing to the module.\n        \"\"\"\nsuper().__init__()\nself.module = module\nself.inputs_to_split = [path.split(\".\") for path in inputs_to_split]\nself.initial_input_mapping = {\noutput: input.split(\".\") for output, input in initial_input_mapping.items()\n}\nself.split_axis = split_axis\nself.chunk_size = chunk_size\ndef _build_initial_dict(self, inputs):\n# This allows us to bing the initial input and previous_output into a similar format.\noutput_dict = {}\nfor output_path, input_path in self.initial_input_mapping.items():\nsource = tree_utils.get_tree_element(inputs, input_path)\nif output_path == \"\":\n# Just the object itself, no dict nesting.\nreturn source\noutput_path = output_path.split(\".\")\ncur_search = output_dict\nfor path_part in output_path[:-1]:\n# Iterate along path and create nodes that do not exist yet.\ntry:\n# Get element prior to last.\ncur_search = tree_utils.get_tree_element(cur_search, [path_part])\nexcept ValueError:\n# Element does not yet exist.\ncur_search[path_part] = {}\ncur_search = cur_search[path_part]\ncur_search[output_path[-1]] = source\nreturn output_dict\ndef forward(self, inputs: Dict[str, Any]) -&gt; Dict[str, Any]:\n# TODO: Come up with a better way of handling the initial input without putting restrictions\n# on modules being run recurrently.\noutputs = [self._build_initial_dict(inputs)]\nfor split_dict in tree_utils.split_tree(\ninputs, self.inputs_to_split, self.split_axis, self.chunk_size\n):\nsplit_dict[\"previous_output\"] = outputs[-1]\noutputs.append(self.module(inputs=split_dict))\n# TODO: When chunk size is larger than 1 then this should be cat and not stack. Otherwise an\n# additional axis would be added. Evtl. this should be configurable.\nstack_fn = functools.partial(torch.stack, dim=self.split_axis)\n# Ignore initial input.\nreturn tree_utils.reduce_tree(outputs[1:], stack_fn)\n</code></pre>"},{"location":"api/ocl/utils/routing/#ocl.utils.routing.Recurrent.__init__","title":"<code>__init__</code>","text":"<p>Initialize recurrent module.</p> <p>Parameters:</p> Name Type Description Default <code>module</code> <code>nn.Module</code> <p>The module that should be applied recurrently along input tensors.</p> required <code>inputs_to_split</code> <code>List[str]</code> <p>List of paths that should be split for recurrent application.</p> required <code>initial_input_mapping</code> <code>Dict[str, str]</code> <p>Mapping that constructs the first <code>previous_output</code> element.  If <code>previous_output</code> should just be a tensor, use a mapping of the format <code>{\"\": \"input_path\"}</code>.</p> required <code>split_axis</code> <code>int</code> <p>Axis along which to split the tensors defined by inputs_to_split.</p> <code>1</code> <code>chunk_size</code> <code>int</code> <p>The size of each slice, when set to 1, the slice dimension is squeezed prior to passing to the module.</p> <code>1</code> Source code in <code>ocl/utils/routing.py</code> <pre><code>def __init__(\nself,\nmodule: nn.Module,\ninputs_to_split: List[str],\ninitial_input_mapping: Dict[str, str],\nsplit_axis: int = 1,\nchunk_size: int = 1,\n):\n\"\"\"Initialize recurrent module.\n    Args:\n        module: The module that should be applied recurrently along input tensors.\n        inputs_to_split: List of paths that should be split for recurrent application.\n        initial_input_mapping: Mapping that constructs the first `previous_output` element.  If\n            `previous_output` should just be a tensor, use a mapping of the format\n            `{\"\": \"input_path\"}`.\n        split_axis: Axis along which to split the tensors defined by inputs_to_split.\n        chunk_size: The size of each slice, when set to 1, the slice dimension is squeezed prior\n            to passing to the module.\n    \"\"\"\nsuper().__init__()\nself.module = module\nself.inputs_to_split = [path.split(\".\") for path in inputs_to_split]\nself.initial_input_mapping = {\noutput: input.split(\".\") for output, input in initial_input_mapping.items()\n}\nself.split_axis = split_axis\nself.chunk_size = chunk_size\n</code></pre>"},{"location":"api/ocl/utils/trees/","title":"ocl.utils.trees","text":"<p>Utilities for working with our own version of PyTrees which focus on torch tensors.</p> <p>PyTrees are any nested structure of dictionaries, lists, tuples, namedtuples or dataclasses.</p>"},{"location":"api/ocl/utils/trees/#ocl.utils.trees.is_tensor_or_module","title":"<code>is_tensor_or_module</code>","text":"<p>Check if input is a torch.Tensor or a torch.nn.Module.</p> Source code in <code>ocl/utils/trees.py</code> <pre><code>def is_tensor_or_module(t: Any):\n\"\"\"Check if input is a torch.Tensor or a torch.nn.Module.\"\"\"\nreturn isinstance(t, (torch.Tensor, torch.nn.Module))\n</code></pre>"},{"location":"api/ocl/utils/trees/#ocl.utils.trees.is_namedtuple","title":"<code>is_namedtuple</code>","text":"<p>Check if input is a named tuple.</p> Source code in <code>ocl/utils/trees.py</code> <pre><code>def is_namedtuple(obj) -&gt; bool:\n\"\"\"Check if input is a named tuple.\"\"\"\nreturn isinstance(obj, tuple) and hasattr(obj, \"_asdict\") and hasattr(obj, \"_fields\")\n</code></pre>"},{"location":"api/ocl/utils/trees/#ocl.utils.trees.get_tree_element","title":"<code>get_tree_element</code>","text":"<p>Get element of a tree.</p> Source code in <code>ocl/utils/trees.py</code> <pre><code>def get_tree_element(d: Tree, path: List[str]) -&gt; Any:\n\"\"\"Get element of a tree.\"\"\"\nnext_element = d\nfor next_element_name in path:\nif isinstance(next_element, abc.Mapping) and next_element_name in next_element:\nnext_element = next_element[next_element_name]\nelif hasattr(next_element, next_element_name):\nnext_element = getattr(next_element, next_element_name)\nelif isinstance(next_element, (list, tuple)) and next_element_name.isnumeric():\nnext_element = next_element[int(next_element_name)]\nelse:\ntry:\nnext_element = getattr(next_element, next_element_name)\nexcept AttributeError:\nmsg = f\"Trying to access path {'.'.join(path)}, \"\nif isinstance(next_element, abc.Mapping):\nmsg += f\"but element {next_element_name} is not among keys {next_element.keys()}\"\nelif isinstance(next_element, (list, tuple)):\nmsg += f\"but cannot index into list with {next_element_name}\"\nelse:\nmsg += (\nf\"but element {next_element_name} cannot be used to access attribute of \"\nf\"object of type {type(next_element)}\"\n)\nraise ValueError(msg)\nreturn next_element\n</code></pre>"},{"location":"api/ocl/utils/trees/#ocl.utils.trees.walk_tree_with_paths","title":"<code>walk_tree_with_paths</code>","text":"<p>Walk over all tensors + modules and their paths in a nested structure.</p> <p>This could lead to an infinite loop.</p> Source code in <code>ocl/utils/trees.py</code> <pre><code>def walk_tree_with_paths(\nnext_element, path=None, instance_check=is_tensor_or_module\n) -&gt; Generator[Tuple[List[str], Any], None, None]:\n\"\"\"Walk over all tensors + modules and their paths in a nested structure.\n    This could lead to an infinite loop.\n    \"\"\"\nif path is None:\npath = []\nif instance_check(next_element):\nyield path, next_element\nelif isinstance(next_element, str):\n# Special handling for strings, as even a single element slice is a sequence. This leads to\n# infinite nesting.\npass\nelif isinstance(next_element, torch.nn.Module):\nfor key, value in next_element.named_children():\nyield from walk_tree_with_paths(\nvalue, path=_build_walk_path(path, key), instance_check=instance_check\n)\nelif isinstance(next_element, (dict, Mapping)):\nfor key, value in next_element.items():\nyield from walk_tree_with_paths(\nvalue, path=_build_walk_path(path, key), instance_check=instance_check\n)\nelif dataclasses.is_dataclass(next_element):\nfor field in dataclasses.fields(next_element):\nyield from walk_tree_with_paths(\ngetattr(next_element, field.name),\npath=_build_walk_path(path, field.name),\ninstance_check=instance_check,\n)\nelif is_namedtuple(next_element):\nfor field_name in next_element._fields:\nyield from walk_tree_with_paths(\ngetattr(next_element, field_name),\npath=_build_walk_path(path, field_name),\ninstance_check=instance_check,\n)\nelif isinstance(next_element, (List, Sequence, tuple)):\nfor index, el in enumerate(next_element):\nyield from walk_tree_with_paths(\nel, path=_build_walk_path(path, index), instance_check=instance_check\n)\n</code></pre>"},{"location":"api/ocl/utils/trees/#ocl.utils.trees.reduce_tree","title":"<code>reduce_tree</code>","text":"<p>Apply reduction function to a list of nested dicts.</p> <p>This only considers tensors at the moment, for other data types are simply copied from the first element.</p> Source code in <code>ocl/utils/trees.py</code> <pre><code>def reduce_tree(outputs: List[Dict[str, Any]], fn: Callable[[List[torch.Tensor]], torch.Tensor]):\n\"\"\"Apply reduction function to a list of nested dicts.\n    This only considers tensors at the moment, for other data types are simply copied from the first\n    element.\n    \"\"\"\nid_to_reduced_tensor = {}\nfor path, tensor in walk_tree_with_paths(outputs[0]):\nstacked_tensor = fn([tensor] + [get_tree_element(output, path) for output in outputs[1:]])\nid_to_reduced_tensor[id(tensor)] = stacked_tensor\n# Replace all tensors with their stacked versions.\nreturn copy.deepcopy(outputs[0], memo=id_to_reduced_tensor)\n</code></pre>"},{"location":"api/ocl/utils/trees/#ocl.utils.trees.map_tree","title":"<code>map_tree</code>","text":"<p>Apply a function to each element of a tree.</p> <p>This only considers tensors at the moment, for other data types are simply copied from the first element.</p> Source code in <code>ocl/utils/trees.py</code> <pre><code>def map_tree(d: Tree, fn: Callable[[torch.Tensor], torch.Tensor]):\n\"\"\"Apply a function to each element of a tree.\n    This only considers tensors at the moment, for other data types are simply copied from the first\n    element.\n    \"\"\"\nid_to_mapped_tensor = {}\nfor _, tensor in walk_tree_with_paths(d):\nmapped_tensor = fn(tensor)\nid_to_mapped_tensor[id(tensor)] = mapped_tensor\n# Replace all tensors with their stacked versions.\nreturn copy.deepcopy(d, memo=id_to_mapped_tensor)\n</code></pre>"},{"location":"api/ocl/utils/windows/","title":"ocl.utils.windows","text":"<p>Utility functions related to windows of inputs.</p>"},{"location":"api/ocl/utils/windows/#ocl.utils.windows.JoinWindows","title":"<code>JoinWindows</code>","text":"<p>         Bases: <code>torch.nn.Module</code></p> <p>Join individual windows to single output.</p> Source code in <code>ocl/utils/windows.py</code> <pre><code>class JoinWindows(torch.nn.Module):\n\"\"\"Join individual windows to single output.\"\"\"\ndef __init__(self, n_windows: int, size):\nsuper().__init__()\nself.n_windows = n_windows\nself.size = size\ndef forward(self, masks: torch.Tensor, keys: str) -&gt; torch.Tensor:\nassert len(masks) == self.n_windows\nkeys_split = [key.split(\"_\") for key in keys]\npad_left = [int(elems[1]) for elems in keys_split]\npad_top = [int(elems[2]) for elems in keys_split]\ntarget_height, target_width = self.size\nn_masks = masks.shape[0] * masks.shape[1]\nheight, width = masks.shape[2], masks.shape[3]\nfull_mask = torch.zeros(n_masks, *self.size).to(masks)\nx = 0\ny = 0\nfor idx, mask in enumerate(masks):\nelems = masks.shape[1]\nx_start = 0 if pad_left[idx] &gt;= 0 else -pad_left[idx]\nx_end = min(width, target_width - pad_left[idx])\ny_start = 0 if pad_top[idx] &gt;= 0 else -pad_top[idx]\ny_end = min(height, target_height - pad_top[idx])\ncropped = mask[:, y_start:y_end, x_start:x_end]\nfull_mask[\nidx * elems : (idx + 1) * elems, y : y + cropped.shape[-2], x : x + cropped.shape[-1]\n] = cropped\nx += cropped.shape[-1]\nif x &gt; target_width:\ny += cropped.shape[-2]\nx = 0\nassert torch.all(torch.abs(torch.sum(full_mask, axis=0) - 1) &lt;= 1e-2)\nreturn full_mask.unsqueeze(0)\n</code></pre>"},{"location":"configs/","title":"Configuration","text":"<p>OCLF command line tools are configured using configuration files written in yaml via the hydra configuration framework. The base configuration is defined in training_config, datasets and experiments are defined in  the corresponding subfolders.</p>"},{"location":"configs/#faqs","title":"FAQs","text":"Why are some elements of the configuration files defined as dictionaries? <p>Hydra does not support merging of lists from multiple configurations.  We thus instead rely on a workaround of using dictionaries which are later converted to lists.  Examples of this are <code>trainer.callbacks</code> which is initialized using a <code>${oc.dict.values:experiment.callbacks}</code> and thus derives callbacks from the dictionariy <code>experiment.callbacks</code> and the <code>train_transforms</code> and <code>eval_transforms</code> arguments of ocl.datasets.WebdatasetDataModule.</p> Can I add configurations in a separate location to those in OCLF? <p>Yes, this is possible with the hydra using the <code>--config-dir</code> command line argument. See here for further information.</p>"},{"location":"configs/SUMMARY/","title":"SUMMARY","text":"<ul> <li>configs</li> <li>training_config</li> <li>cluster<ul> <li>mlflow_logging</li> <li>slurm</li> </ul> </li> <li>dataset<ul> <li>cater</li> <li>cater6</li> <li>cater6_image</li> <li>cater_image</li> <li>clevr</li> <li>clevr6</li> <li>clevrer</li> <li>coco</li> <li>coco2014_20k</li> <li>coco_nocrowd</li> <li>dummy_dataset</li> <li>movi_c</li> <li>movi_c_image</li> <li>movi_e</li> <li>movi_e_image</li> <li>voc2012</li> <li>voc2012_trainaug</li> <li>voc2012_trainval</li> </ul> </li> <li>experiment<ul> <li>projects<ul> <li>bridging<ul> <li>dinosaur<ul> <li>_base_feature_recon</li> <li>_metrics_clevr_patch</li> <li>_metrics_coco</li> <li>_preprocessing_coco_dino_feature_recon_ccrop</li> <li>_preprocessing_coco_dino_feature_recon_origres</li> <li>_preprocessing_coco_dino_feature_recon_randcrop</li> <li>_preprocessing_coco_imagenet_feature_recon</li> <li>_preprocessing_movi_dino_feature_recon</li> <li>_preprocessing_voc2012_segm_dino_feature_recon</li> <li>coco_feat_rec_dino_base16</li> <li>coco_feat_rec_dino_base16_auto</li> <li>coco_feat_rec_dino_small16_auto</li> <li>coco_feat_rec_in_base16_auto</li> <li>coco_feat_rec_in_small16_auto</li> <li>coco_feat_rec_resnet50</li> <li>movi_c_feat_rec</li> <li>movi_c_feat_rec_auto</li> <li>movi_e_feat_rec</li> <li>movi_e_feat_rec_auto</li> <li>voc2012_trainaug_feat_rec_dino_base16_auto</li> </ul> </li> <li>experiment_image_feature_rec<ul> <li>_base</li> <li>_metrics_coco_image</li> <li>_preprocessing_coco_dino_feature_recon</li> <li>_preprocessing_coco_dino_image_recon</li> <li>_preprocessing_coco_resnet_feature_recon</li> <li>coco_feat_rec_dino_base16_fine</li> <li>coco_feat_rec_dino_base16_frozen</li> <li>coco_feat_rec_dino_base16_resnet_scratch</li> <li>coco_feat_rec_dino_base16_scratch</li> <li>coco_image_rec_base16_fine</li> <li>coco_image_rec_base16_frozen</li> <li>coco_image_rec_base16_scratch</li> </ul> </li> <li>slot_attention<ul> <li>_base_large</li> <li>_metrics_clevr</li> <li>_metrics_coco</li> <li>_preprocessing_coco</li> <li>_preprocessing_movi</li> <li>_preprocessing_voc2012_trainaug</li> <li>coco</li> <li>movi_c</li> <li>movi_e</li> <li>voc2012_trainaug</li> </ul> </li> </ul> </li> </ul> </li> <li>SAVi<ul> <li>cater</li> <li>clevrer</li> <li>clevrer_highres</li> </ul> </li> <li>SAVi_code<ul> <li>cater</li> <li>cater_savi_with_predictor</li> </ul> </li> <li>_output_path</li> <li>clip<ul> <li>coco_finetuning</li> </ul> </li> <li>examples<ul> <li>hp_scheduling</li> <li>parameter_groups</li> <li>slot_masking</li> </ul> </li> <li>occluded_slot_attention<ul> <li>clevr6</li> </ul> </li> <li>slate<ul> <li>_base</li> <li>_base_large</li> <li>_clevr_preprocessing</li> <li>_metrics_clevr_patch</li> <li>_metrics_masks</li> <li>_movi_preprocessing</li> <li>_preprocessing_coco</li> <li>_preprocessing_voc2012_trainaug</li> <li>clevr6</li> <li>coco</li> <li>movi_c</li> <li>movi_e</li> <li>voc2012_trainaug</li> </ul> </li> <li>slot_attention<ul> <li>_base</li> <li>_base_large</li> <li>_base_optical_flow</li> <li>_metrics_clevr</li> <li>_metrics_coco</li> <li>_preprocessing_cater</li> <li>_preprocessing_clevr</li> <li>_preprocessing_coco</li> <li>_preprocessing_movi</li> <li>_preprocessing_voc2012_trainaug</li> <li>cater10</li> <li>cater6</li> <li>clevr10</li> <li>clevr10_lds</li> <li>clevr6</li> <li>coco</li> <li>movi_c</li> <li>movi_c_optical_flow</li> <li>movi_e</li> <li>voc2012_trainaug</li> </ul> </li> </ul> </li> <li>evaluation_config</li> <li>evaluation_clustering_config</li> <li>evaluation<ul> <li>projects<ul> <li>bridging<ul> <li>_base_clustering</li> <li>_base_metrics</li> <li>_classes_coco</li> <li>_classes_coco_coarse</li> <li>_classes_voc</li> <li>_metrics_add_ignore_mask</li> <li>_metrics_discovery_bboxes</li> <li>_metrics_discovery_masks</li> <li>_metrics_discovery_movi</li> <li>_metrics_segmentation</li> <li>_preprocessing_coco</li> <li>_preprocessing_coco_27</li> <li>_preprocessing_coco_things</li> <li>_preprocessing_coco_thingsandstuff</li> <li>_preprocessing_movi_c</li> <li>_preprocessing_movi_e</li> <li>_preprocessing_to_ccrop_320_masks</li> <li>_preprocessing_to_ccrop_image</li> <li>_preprocessing_voc2012</li> <li>_preprocessing_voc2012_trainaug</li> <li>clustering_coco</li> <li>clustering_coco_20k</li> <li>clustering_coco_27</li> <li>clustering_coco_cls</li> <li>clustering_coco_thingsandstuff</li> <li>clustering_voc2012</li> <li>metrics_coco</li> <li>metrics_coco_20k</li> <li>metrics_coco_ccrop</li> <li>metrics_coco_ccrop_random</li> <li>metrics_coco_ccrop_sa</li> <li>metrics_movi_c</li> <li>metrics_movi_c_sa</li> <li>metrics_movi_e</li> <li>metrics_movi_e_sa</li> <li>metrics_movi_random</li> <li>metrics_voc2012_bboxes</li> <li>metrics_voc2012_masks</li> <li>metrics_voc2012_masks_ccrop</li> <li>metrics_voc2012_masks_ccrop_random</li> <li>metrics_voc2012_masks_ccrop_sa</li> <li>outputs_coco_ccrop</li> <li>outputs_coco_ccrop_sa</li> <li>outputs_coco_ccrop_slots</li> <li>outputs_coco_ccrop_slots_sa</li> <li>outputs_movi</li> <li>outputs_movi_e</li> <li>outputs_movi_e_sa</li> <li>outputs_movi_sa</li> <li>outputs_voc2012_bbox</li> <li>outputs_voc2012_ccrop</li> </ul> </li> </ul> </li> <li>eval</li> <li>slate<ul> <li>_base_metrics</li> <li>_metrics_discovery_masks</li> <li>_metrics_masks</li> <li>_movi_preprocessing</li> <li>_preprocessing_coco</li> <li>metrics_coco</li> <li>metrics_movi_c</li> <li>metrics_movi_e</li> <li>metrics_pascal</li> </ul> </li> </ul> </li> </ul>"},{"location":"configs/evaluation_clustering_config/","title":"Evaluation clustering config","text":"<p>Configuration to evaluate a trained model for object discovery by clustering object representations.</p> <p>Given a set of images, each with a set of ground truth masks and a set of object masks and representations, we perform the following steps:</p> <ol> <li>Assign each object a cluster id by clustering the corresponding representations over all      images.</li> <li>Merge object masks with the same cluster id on the same image to form a semantic mask.</li> <li>Compute IoU between masks of predicted clusters and ground truth classes over all images.</li> <li>Assign clusters to classes based on the IoU and a matching strategy.</li> </ol> <p>In contrast to most other configurations, the clustering evaluation configuration is defined programmatically in python.  The definition can be found below:</p> ocl/cli/eval_cluster_metrics.py:EvaluationClusteringConfig<pre><code>@dataclasses.dataclass\nclass EvaluationClusteringConfig:\n\"\"\"Configuration for evaluation.\"\"\"\n# Path to training configuration file or configuration dir. If dir, train_config_name\n# needs to be set as well.\ntrain_config_path: str\n# Number of classes. Note that on COCO, this should be one larger than the maximum class ID that\n# can appear, which does not correspond to the real number of classes.\nn_classes: int\n# Clustering methods to get cluster ID per object by clustering representations\n# This only supports clustering metrics.\nclusterings: Optional[Dict[str, Any]] = None\n# Paths for model outputs to get cluster ID per object\nmodel_clusterings: Optional[Dict[str, str]] = None\ntrain_config_overrides: Optional[List[str]] = None\ntrain_config_name: Optional[str] = None\ncheckpoint_path: Optional[str] = None\noutput_dir: Optional[str] = None\nreport_filename: str = \"clustering_metrics.json\"\nbatch_size: int = 25\nclass_discovery_threshold: float = 0.02\nuse_mask_threshold: bool = False\nmask_threshold: float = 0.5\nignore_background: bool = False\nuse_unmatched_as_background: bool = False\nuse_ignore_masks: bool = False\nn_min_mask_pixels: int = 1  # Minimum number of pixels a mask must occupy to be considered valid\nn_min_max_mask_values: float = 1e-4  # Mask must have at least one value above threshold\n# Type of representation to use for clustering.\nrepresentation_type: RepresentationType = RepresentationType.SLOTS\n# Setting this allows to add modules to the model that are executed during evaluation\nmodules: Optional[Dict[str, Any]] = None\n# Setting this allows to evaluate on a different dataset than the model was trained on\ndataset: Optional[Any] = None\n# Path to slot representations\nslots_path: str = \"perceptual_grouping.objects\"\n# Path to feature representations\nfeatures_path: str = \"feature_extractor.features\"\n# Path to slot masks, image shaped\nmasks_path: str = \"object_decoder.masks_as_image\"\n# Path to slot masks, but flattened to match the size of features\nmasks_flat_path: str = \"object_decoder.masks\"\n# Path to reference masks\ntarget_masks_path: str = \"input.segmentation_mask\"\n# Path to ignore masks\nignore_masks_path: str = \"input.ignore_mask\"\n# Path under which representation to cluster is stored\ncluster_representation_path: str = \"representation\"\n# Path under which empty slot mask is stored\nempty_slots_path: str = \"empty_slots\"\nclass_name_by_category_id: Optional[Dict[int, str]] = None\n</code></pre>"},{"location":"configs/evaluation_config/","title":"Evaluation config","text":"<p>In contrast to most other configurations, the evaluation configuration is defined programmatically in python.  The definition can be found below:</p> ocl/cli/eval.py:EvaluationConfig<pre><code>@dataclasses.dataclass\nclass EvaluationConfig:\n\"\"\"Configuration for evaluation.\"\"\"\n# Path to training configuration file or configuration dir. If dir, train_config_name\n# needs to be set as well.\ntrain_config_path: str\ntrain_config_overrides: Optional[List[str]] = None\ntrain_config_name: Optional[str] = None\ncheckpoint_path: Optional[str] = None\noutput_dir: Optional[str] = None\nreport_filename: str = \"metrics.json\"\n# Setting this allows to add modules to the model that are executed during evaluation\nmodules: Optional[Dict[str, Any]] = None\n# Setting this allows to evaluate on a different dataset than the model was trained on\ndataset: Optional[Any] = None\n# Setting this allows to evaluate on different metrics than the model was trained on\nevaluation_metrics: Optional[Dict[str, Any]] = None\nsave_outputs: bool = False\nskip_metrics: bool = False\noutputs_dirname: str = \"outputs\"\noutputs_to_store: Optional[List[str]] = None\nn_samples_to_store: Optional[int] = None\neval_train: bool = False\neval_val: bool = True\neval_test: bool = False\neval_batch_size: Optional[int] = None\n</code></pre>"},{"location":"configs/training_config/","title":"Training config","text":"<p>In contrast to most other configurations, the training configuration is defined programmatically in python.  The definition can be found below:</p>"},{"location":"configs/training_config/#ocl.cli.train.TrainingConfig","title":"<code>ocl.cli.train.TrainingConfig</code>  <code>dataclass</code>","text":"<p>Configuration of a training run.</p> <p>For losses, metrics and visualizations it can be of use to utilize the routed module as these are simply provided with a dictionary of all model inputs and outputs.</p> <p>Attributes:</p> Name Type Description <code>dataset</code> <code>Any</code> <p>The pytorch lightning datamodule that will be used for training</p> <code>models</code> <code>Any</code> <p>Either a dictionary of torch.nn.Modules which will be interpreted as a Combined model or a torch.nn.Module itself that accepts a dictionary as input.</p> <code>optimizers</code> <code>Dict[str, Any]</code> <p>Dictionary of functools.partial wrapped optimizers or OptimizationWrapper instances</p> <code>losses</code> <code>Dict[str, Any]</code> <p>Dict of callables that return scalar values which will be summed to compute a total loss.  Typically should contain routed versions of callables.</p> <code>visualizations</code> <code>Dict[str, Any]</code> <p>Dictionary of visualizations.  Typically should contain routed versions of visualizations.</p> <code>trainer</code> <code>TrainerConf</code> <p>Pytorch lightning trainer</p> <code>training_vis_frequency</code> <code>Optional[int]</code> <p>Number of optimization steps between generation and storage of visualizations.</p> <code>training_metrics</code> <code>Optional[Dict[str, Any]]</code> <p>Dictionary of torchmetrics that should be used to log training progress. Typically should contain routed versions of torchmetrics.</p> <code>evaluation_metrics</code> <code>Optional[Dict[str, Any]]</code> <p>Dictionary of torchmetrics that should be used to log progress on evaluation splits of the data.  Typically should contain routed versions of Torchmetrics.</p> <code>load_checkpoint</code> <code>Optional[str]</code> <p>Path to checkpoint file that should be loaded prior to starting training.</p> <code>seed</code> <code>Optional[int]</code> <p>Seed used to ensure reproducability.</p> <code>experiment</code> <code>Dict[str, Any]</code> <p>Dictionary with arbitrary additional information.  Useful when building configurations as it can be used as central point for a single parameter that might influence multiple model components.</p> Source code in <code>ocl/cli/train.py</code> <pre><code>@dataclasses.dataclass\nclass TrainingConfig:\n\"\"\"Configuration of a training run.\n    For losses, metrics and visualizations it can be of use to utilize the\n    [routed][] module as these are simply provided with a dictionary of all\n    model inputs and outputs.\n    Attributes:\n        dataset: The pytorch lightning datamodule that will be used for training\n        models: Either a dictionary of [torch.nn.Module][]s which will be interpreted\n            as a [Combined][ocl.utils.routing.Combined] model or a [torch.nn.Module][] itself\n            that accepts a dictionary as input.\n        optimizers: Dictionary of [functools.partial][] wrapped optimizers or\n            [OptimizationWrapper][ocl.optimization.OptimizationWrapper] instances\n        losses: Dict of callables that return scalar values which will be summed to\n            compute a total loss.  Typically should contain [routed][] versions of callables.\n        visualizations: Dictionary of [visualizations][ocl.visualizations].  Typically\n            should contain [routed][] versions of visualizations.\n        trainer: Pytorch lightning trainer\n        training_vis_frequency: Number of optimization steps between generation and\n            storage of visualizations.\n        training_metrics: Dictionary of torchmetrics that should be used to log training progress.\n            Typically should contain [routed][] versions of torchmetrics.\n        evaluation_metrics: Dictionary of torchmetrics that should be used to log progress on\n            evaluation splits of the data.  Typically should contain [routed][] versions of\n            Torchmetrics.\n        load_checkpoint: Path to checkpoint file that should be loaded prior to starting training.\n        seed: Seed used to ensure reproducability.\n        experiment: Dictionary with arbitrary additional information.  Useful when building\n            configurations as it can be used as central point for a single parameter that might\n            influence multiple model components.\n    \"\"\"\ndataset: Any\nmodels: Any  # When provided with dict wrap in `utils.Combined`, otherwise interpret as model.\noptimizers: Dict[str, Any]\nlosses: Dict[str, Any]\nvisualizations: Dict[str, Any] = dataclasses.field(default_factory=dict)\ntrainer: TrainerConf = dataclasses.field(default_factory=lambda: TrainerConf())\ntraining_vis_frequency: Optional[int] = None\ntraining_metrics: Optional[Dict[str, Any]] = None\nevaluation_metrics: Optional[Dict[str, Any]] = None\nload_checkpoint: Optional[str] = None\nseed: Optional[int] = None\nexperiment: Dict[str, Any] = dataclasses.field(default_factory=lambda: {\"callbacks\": {}})\n</code></pre>"},{"location":"configs/cluster/","title":"Cluster configurations","text":"<p>This folder contains configurations that might be useful for running experiments on clusters or using alternative pytorch lightning loggers.</p>"},{"location":"configs/cluster/mlflow_logging/","title":"configs/cluster/mlflow_logging.yaml","text":"<pre><code># @package _global_\n# Configuration of mlflow logger. This is basic example of the usage.\ntrainer:\n# We rely on hydras dir management\ndefault_root_dir: .\nlogger:\n_target_: ocl.utils.logging.ExtendedMLFlowLogger\n# Override if you want to have a different structure\nexperiment_name: ${slice:${hydra:runtime.choices.experiment},\"/\",0}\nrun_name: ${slice:${hydra:runtime.choices.experiment},\"/\",\"1:\"}_${now:%Y-%m-%d_%H-%M-%S}\nexperiment:\ncallbacks:\ncheckpoint:\n_target_: pytorch_lightning.callbacks.model_checkpoint.ModelCheckpoint\nlog_hydra_config:\n_target_: ocl.utils.logging.LogHydraConfigCallback\nhydra_output_subdir: ${hydra:output_subdir}\n# Add this in order to track parameters from the hydra config as hyperparameters\nadditional_paths:\nlog_model_summary:\n_target_: ocl.utils.logging.LogModelSummaryCallback\nhydra:\njob:\nchdir: true\n</code></pre>"},{"location":"configs/cluster/slurm/","title":"configs/cluster/slurm.yaml","text":"<pre><code># @package _global_\ndefaults:\n- override /hydra/launcher: submitit_slurm\ntrainer:\nstrategy:\n_target_: pytorch_lightning.plugins.training_type.ddp.DDPPlugin\nfind_unused_parameters: true\nplugins:\n- _target_: pytorch_lightning.plugins.environments.SLURMEnvironment\nauto_requeue: false\nhydra:\nlauncher:\ntimeout_min: 0\ncpus_per_task: 8\ntasks_per_node: 8\ngres: gpu:8\n</code></pre>"},{"location":"configs/dataset/","title":"Dataset configurations","text":"<p>A dataset is expected to be a pytorch lightning <code>LightningDataModule</code>, where the constructor at least accepts the following parameters which are used in the experiment config tests.</p> <ul> <li><code>batch_size: int</code></li> <li><code>num_workers: int</code></li> <li><code>shuffle_buffer_size: int</code></li> </ul> <p>Check out the datasets section of the api for some examples on how a dataset can be implemented.</p>"},{"location":"configs/dataset/#composition-of-datasets","title":"Composition of datasets","text":"<p>Dataset configurations can be composed, such that it is straight forward to create derived versions of datasets for example by sampling images from a video or by filtering out some instances.  This is possible as transforms are stored in dictionaries and thus can be composed in hydra.</p> <p>Check out configs/dataset/clevr6.yaml and configs/dataset/movi_c_image.yaml for some examples.</p>"},{"location":"configs/dataset/cater/","title":"configs/dataset/cater.yaml","text":"<pre><code># Video dataset CATER based on https://github.com/deepmind/multi_object_datasets .\n_target_: ocl.datasets.WebdatasetDataModule\ntrain_shards: ${oc.env:DATASET_PREFIX}/cater_with_masks/train/shard-{000000..000152}.tar\ntrain_size: 35427\nval_shards: ${oc.env:DATASET_PREFIX}/cater_with_masks/val/shard-{000000..000016}.tar\nval_size: 3937\ntest_shards: ${oc.env:DATASET_PREFIX}/cater_with_masks/test/shard-{000000..000073}.tar\ntest_size: 17100\n</code></pre>"},{"location":"configs/dataset/cater6/","title":"configs/dataset/cater6.yaml","text":"<pre><code># Values derived from running `bin/compute_dataset_size.py`\ndefaults:\n- cater  # (1)!\ntrain_size: 11768\nval_size: 1311\ntest_size: 5793\ntrain_transforms:\n01_cater6_subset:\n_target_: ocl.transforms.Filter\npredicate: \"${lambda_fn:'lambda object_positions: (object_positions != float(\\\"\\\ninf\\\")).all((0, 2)).sum().item() &lt; 7'}\"\nfields:\n- object_positions\neval_transforms:\n01_cater6_subset:\n_target_: ocl.transforms.Filter\npredicate: \"${lambda_fn:'lambda object_positions: (object_positions != float(\\\"\\\ninf\\\")).all((0, 2)).sum().item() &lt; 7'}\"\nfields:\n- object_positions\n</code></pre> <ol> <li>/dataset/cater</li> </ol>"},{"location":"configs/dataset/cater6_image/","title":"configs/dataset/cater6_image.yaml","text":"<pre><code># Image dataset containing subsampled frames from CATER6 dataset.\n# This dataset was constructed in order to mimic CLEVR6 as closely as possible, yet with higher\n# complexity due to the change of camera perspective and potentially flying objects.\ndefaults:\n- cater6  # (1)!\ntrain_size: 35304\nval_size: 3933\n# Test split is much larger than clevr6 due to the predefined splits of cater.\ntest_size: 17379\ntrain_transforms:\n02_sample_frames:\n_target_: ocl.transforms.SampleSlices\nfields:\n- image\nn_slices_per_input: 3\ndim: 0\nseed: 457834752\nshuffle_buffer_size: 1000\neval_transforms:\n02_sample_frames:\n_target_: ocl.transforms.SampleSlices\nfields:\n- image\n- mask\n- camera_matrix\n- object_positions\nn_slices_per_input: 3\ndim: 0\nseed: 457834753\nshuffle_buffer_size: 1\n</code></pre> <ol> <li>/dataset/cater6</li> </ol>"},{"location":"configs/dataset/cater_image/","title":"configs/dataset/cater_image.yaml","text":"<pre><code># Image dataset containing subsampled frames from CATER dataset.\ndefaults:\n- cater  # (1)!\ntrain_size: 106281\nval_size: 11811\ntest_size: 51300\ntrain_transforms:\n02_sample_frames:\n_target_: ocl.transforms.SampleSlices\nfields:\n- image\nn_slices_per_input: 3\ndim: 0\nseed: 457834752\nshuffle_buffer_size: 1000\neval_transforms:\n02_sample_frames:\n_target_: ocl.transforms.SampleSlices\nfields:\n- image\n- mask\n- camera_matrix\n- object_positions\nn_slices_per_input: 3\ndim: 0\nseed: 457834753\nshuffle_buffer_size: 1\n</code></pre> <ol> <li>/dataset/cater</li> </ol>"},{"location":"configs/dataset/clevr/","title":"configs/dataset/clevr.yaml","text":"<pre><code># Image dataset CLEVR based on https://github.com/deepmind/multi_object_datasets .\n_target_: ocl.datasets.WebdatasetDataModule\ntrain_shards: ${oc.env:DATASET_PREFIX}/clevr_with_masks/train/shard-{000000..000114}.tar\ntrain_size: 70000\nval_shards: ${oc.env:DATASET_PREFIX}/clevr_with_masks/val/shard-{000000..000024}.tar\nval_size: 15000\ntest_shards: ${oc.env:DATASET_PREFIX}/clevr_with_masks/test/shard-{000000..000024}.tar\ntest_size: 15000\n</code></pre>"},{"location":"configs/dataset/clevr6/","title":"configs/dataset/clevr6.yaml","text":"<pre><code># Image dataset containing instances from CLEVR with at most 6 objects in each scene.\ndefaults:\n- clevr  # (1)!\ntrain_size: 26240\nval_size: 5553\ntest_size: 5600\ntrain_transforms:\n01_clevr6_subset:\n_target_: ocl.transforms.Filter\npredicate: \"${lambda_fn:'lambda visibility: visibility.sum().item() &lt; 7'}\"\nfields:\n- visibility\neval_transforms:\n01_clevr6_subset:\n_target_: ocl.transforms.Filter\npredicate: \"${lambda_fn:'lambda visibility: visibility.sum().item() &lt; 7'}\"\nfields:\n- visibility\n</code></pre> <ol> <li>/dataset/clevr</li> </ol>"},{"location":"configs/dataset/clevrer/","title":"configs/dataset/clevrer.yaml","text":"<pre><code># Video dataset CLEVRER\n_target_: ocl.datasets.WebdatasetDataModule\ntrain_shards: ${oc.env:DATASET_PREFIX}/clevrer/train/shard-{000000..000129}.tar\ntrain_size: 10000\nval_shards: ${oc.env:DATASET_PREFIX}/clevrer/validation/shard-{000050..000065}.tar\nval_size: 1163\ntest_shards: ${oc.env:DATASET_PREFIX}/clevrer/validation/shard-{000000..000050}.tar\ntest_size: 3915\n</code></pre>"},{"location":"configs/dataset/coco/","title":"configs/dataset/coco.yaml","text":"<pre><code># The coco2017 dataset with instance, stuff and caption annotations.\n_target_: ocl.datasets.WebdatasetDataModule\ntrain_shards: ${oc.env:DATASET_PREFIX}/coco2017/train/shard-{000000..000412}.tar\ntrain_size: 118287\nval_shards: ${oc.env:DATASET_PREFIX}/coco2017/val/shard-{000000..000017}.tar\nval_size: 5000\ntest_shards: ${oc.env:DATASET_PREFIX}/coco2017/test/shard-{000000..000126}.tar\ntest_size: 40670\nuse_autopadding: true\n</code></pre>"},{"location":"configs/dataset/coco2014_20k/","title":"configs/dataset/coco2014_20k.yaml","text":"<pre><code># The COCO20k dataset with instance and caption annotations.\n#\n# COCO 20k is a subset of COCO 2014. It is a random subset of both the training\n# and validation sets, filtered to not contain any images only having crowd instances\n# and no annotations labeled as crowd. It is commonly used to evaluate object\n# discovery methods.\n#\n# This configuration defines COCO20k as a validation set.\n_target_: ocl.datasets.WebdatasetDataModule\nval_shards: ${oc.env:DATASET_PREFIX}/coco2014/20k/shard-{000000..000064}.tar\nval_size: 19820\nuse_autopadding: true\n</code></pre>"},{"location":"configs/dataset/coco_nocrowd/","title":"configs/dataset/coco_nocrowd.yaml","text":"<pre><code># The coco2017 dataset with instance, stuff and caption annotations.\n# Validation dataset does not contain any crowd annotations.\n_target_: ocl.datasets.WebdatasetDataModule\ntrain_shards: ${oc.env:DATASET_PREFIX}/coco2017/train/shard-{000000..000412}.tar\ntrain_size: 118287\nval_shards: ${oc.env:DATASET_PREFIX}/coco2017/val_nocrowd/shard-{000000..000017}.tar\nval_size: 5000\ntest_shards: ${oc.env:DATASET_PREFIX}/coco2017/test/shard-{000000..000126}.tar\ntest_size: 40670\nuse_autopadding: true\n</code></pre>"},{"location":"configs/dataset/dummy_dataset/","title":"configs/dataset/dummy_dataset.yaml","text":"<pre><code># Define the parameters here, otherwise hydras override syntax gets in the way\n# (i.e. we would need ot use +override in some test contexts).\n_target_: ocl.datasets.DummyDataModule\nbatch_size:\neval_batch_size:\ntrain_size:\nval_size:\ntest_size:\n</code></pre>"},{"location":"configs/dataset/movi_c/","title":"configs/dataset/movi_c.yaml","text":"<pre><code>_target_: ocl.datasets.WebdatasetDataModule\ntrain_shards: ${oc.env:DATASET_PREFIX}/movi_c/train/shard-{000000..000298}.tar\ntrain_size: 9737\nval_shards: ${oc.env:DATASET_PREFIX}/movi_c/val/shard-{000000..000007}.tar\nval_size: 250\ntest_shards: ${oc.env:DATASET_PREFIX}/movi_c/val/shard-{000000..000007}.tar\ntest_size: 250\nuse_autopadding: true\neval_transforms:\n00_1_rename_fields:\n_target_: ocl.transforms.Map\ntransform:\n_target_: ocl.preprocessing.RenameFields\nmapping:\nvideo: image\nsegmentations: mask\nfields: [video, segmentations]\n# While this could easily be a batch transform,\n# but many preprocessing steps assume different naming.\n# Thus apply rename prior to their application.\nbatch_transform: false\n00_2_adapt_mask_format:\n_target_: ocl.transforms.SimpleTransform\ntransforms:\nmask:\n_target_: ocl.preprocessing.IntegerToOneHotMask\noutput_axis: -4\nmax_instances: 10\nignore_typical_background: false\nbatch_transform: false\ntrain_transforms:\n00_1_rename_fields:\n_target_: ocl.transforms.Map\ntransform:\n_target_: ocl.preprocessing.RenameFields\nmapping:\nvideo: image\nfields: [video]\n# While this could easily be a batch transform,\n# but many preprocessing steps assume different naming.\n# Thus apply rename prior to their application.\nbatch_transform: false\n</code></pre>"},{"location":"configs/dataset/movi_c_image/","title":"configs/dataset/movi_c_image.yaml","text":"<pre><code># Image dataset containing subsampled frames from MOVI_C dataset.\ndefaults:\n- movi_c # (1)!\n- _self_\n# Values derived from running `bin/compute_dataset_size.py`.\ntrain_size: 87633\nval_size: 6000\ntest_size: 6000\ntrain_transforms:\n02_sample_frames:\n_target_: ocl.transforms.SampleSlices\nfields:\n- image\nn_slices_per_input: 9\ndim: 0\nseed: 457834752\nshuffle_buffer_size: 1000\neval_transforms:\n02_sample_frames:\n_target_: ocl.transforms.SampleSlices\nfields:\n- image\n- mask\nn_slices_per_input: -1\ndim: 0\nseed: 457834753\nshuffle_buffer_size: 1\n</code></pre> <ol> <li>/dataset/movi_c</li> </ol>"},{"location":"configs/dataset/movi_e/","title":"configs/dataset/movi_e.yaml","text":"<pre><code>_target_: ocl.datasets.WebdatasetDataModule\ntrain_shards: ${oc.env:DATASET_PREFIX}/movi_e/train/shard-{000000..000679}.tar\ntrain_size: 9749\nval_shards: ${oc.env:DATASET_PREFIX}/movi_e/val/shard-{000000..000017}.tar\nval_size: 250\ntest_shards: ${oc.env:DATASET_PREFIX}/movi_e/val/shard-{000000..000017}.tar\ntest_size: 250\nuse_autopadding: true\neval_transforms:\n00_1_rename_fields:\n_target_: ocl.transforms.Map\ntransform:\n_target_: ocl.preprocessing.RenameFields\nmapping:\nvideo: image\nsegmentations: mask\nfields: [video, segmentations]\n# While this could easily be a batch transform,\n# but many preprocessing steps assume different naming.\n# Thus apply rename prior to their application.\nbatch_transform: false\n00_2_adapt_mask_format:\n_target_: ocl.transforms.SimpleTransform\ntransforms:\nmask:\n_target_: ocl.preprocessing.IntegerToOneHotMask\noutput_axis: -4\nmax_instances: 23\nignore_typical_background: false\nbatch_transform: false\ntrain_transforms:\n00_1_rename_fields:\n_target_: ocl.transforms.Map\ntransform:\n_target_: ocl.preprocessing.RenameFields\nmapping:\nvideo: image\nfields: [video]\n# While this could easily be a batch transform,\n# but many preprocessing steps assume different naming.\n# Thus apply rename prior to their application.\nbatch_transform: false\n</code></pre>"},{"location":"configs/dataset/movi_e_image/","title":"configs/dataset/movi_e_image.yaml","text":"<pre><code># Image dataset containing subsampled frames from MOVI_E dataset.\ndefaults:\n- movi_e # (1)!\n- _self_\n# Values derived from running `bin/compute_dataset_size.py`.\ntrain_size: 87741\nval_size: 6000\ntest_size: 6000\ntrain_transforms:\n02_sample_frames:\n_target_: ocl.transforms.SampleSlices\nfields:\n- image\nn_slices_per_input: 9\ndim: 0\nseed: 457834752\nshuffle_buffer_size: 1000\neval_transforms:\n02_sample_frames:\n_target_: ocl.transforms.SampleSlices\nfields:\n- image\n- mask\nn_slices_per_input: -1\ndim: 0\nseed: 457834753\nshuffle_buffer_size: 1\n</code></pre> <ol> <li>/dataset/movi_e</li> </ol>"},{"location":"configs/dataset/voc2012/","title":"configs/dataset/voc2012.yaml","text":"<pre><code># The PASCAL VOC 2012 dataset. Does not contain segmentation annotations.\n_target_: ocl.datasets.WebdatasetDataModule\ntrain_shards: ${oc.env:DATASET_PREFIX}/voc2012_detection/train/shard-{000000..000021}.tar\ntrain_size: 5717\nval_shards: ${oc.env:DATASET_PREFIX}/voc2012_detection/val/shard-{000000..000022}.tar\nval_size: 5823\ntest_shards: ${oc.env:DATASET_PREFIX}/voc2012_detection/test/shard-{000000..000041}.tar\ntest_size: 10991\nuse_autopadding: true\n</code></pre>"},{"location":"configs/dataset/voc2012_trainaug/","title":"configs/dataset/voc2012_trainaug.yaml","text":"<pre><code># The PASCAL VOC 2012 dataset in the trainaug variant with instance segmentation masks.\n_target_: ocl.datasets.WebdatasetDataModule\ntrain_shards: ${oc.env:DATASET_PREFIX}/voc2012/trainaug/shard-{000000..000040}.tar\ntrain_size: 10582\nval_shards: ${oc.env:DATASET_PREFIX}/voc2012/val/shard-{000000..000011}.tar\nval_size: 1449\ntest_shards:\ntest_size:\nuse_autopadding: true\n</code></pre>"},{"location":"configs/dataset/voc2012_trainval/","title":"configs/dataset/voc2012_trainval.yaml","text":"<pre><code># The PASCAL VOC 2012 dataset, using joint train+val splits for training and validation.\n# This setting is often used in the unsupervised case.\n_target_: ocl.datasets.WebdatasetDataModule\ntrain_shards:\n- ${oc.env:DATASET_PREFIX}/voc2012_detection/train/shard-000000.tar\n- ${oc.env:DATASET_PREFIX}/voc2012_detection/train/shard-000001.tar\n- ${oc.env:DATASET_PREFIX}/voc2012_detection/train/shard-000002.tar\n- ${oc.env:DATASET_PREFIX}/voc2012_detection/train/shard-000003.tar\n- ${oc.env:DATASET_PREFIX}/voc2012_detection/train/shard-000004.tar\n- ${oc.env:DATASET_PREFIX}/voc2012_detection/train/shard-000005.tar\n- ${oc.env:DATASET_PREFIX}/voc2012_detection/train/shard-000006.tar\n- ${oc.env:DATASET_PREFIX}/voc2012_detection/train/shard-000007.tar\n- ${oc.env:DATASET_PREFIX}/voc2012_detection/train/shard-000008.tar\n- ${oc.env:DATASET_PREFIX}/voc2012_detection/train/shard-000009.tar\n- ${oc.env:DATASET_PREFIX}/voc2012_detection/train/shard-000010.tar\n- ${oc.env:DATASET_PREFIX}/voc2012_detection/train/shard-000011.tar\n- ${oc.env:DATASET_PREFIX}/voc2012_detection/train/shard-000012.tar\n- ${oc.env:DATASET_PREFIX}/voc2012_detection/train/shard-000013.tar\n- ${oc.env:DATASET_PREFIX}/voc2012_detection/train/shard-000014.tar\n- ${oc.env:DATASET_PREFIX}/voc2012_detection/train/shard-000015.tar\n- ${oc.env:DATASET_PREFIX}/voc2012_detection/train/shard-000016.tar\n- ${oc.env:DATASET_PREFIX}/voc2012_detection/train/shard-000017.tar\n- ${oc.env:DATASET_PREFIX}/voc2012_detection/train/shard-000018.tar\n- ${oc.env:DATASET_PREFIX}/voc2012_detection/train/shard-000019.tar\n- ${oc.env:DATASET_PREFIX}/voc2012_detection/train/shard-000020.tar\n- ${oc.env:DATASET_PREFIX}/voc2012_detection/train/shard-000021.tar\n- ${oc.env:DATASET_PREFIX}/voc2012_detection/val/shard-000000.tar\n- ${oc.env:DATASET_PREFIX}/voc2012_detection/val/shard-000001.tar\n- ${oc.env:DATASET_PREFIX}/voc2012_detection/val/shard-000002.tar\n- ${oc.env:DATASET_PREFIX}/voc2012_detection/val/shard-000003.tar\n- ${oc.env:DATASET_PREFIX}/voc2012_detection/val/shard-000004.tar\n- ${oc.env:DATASET_PREFIX}/voc2012_detection/val/shard-000005.tar\n- ${oc.env:DATASET_PREFIX}/voc2012_detection/val/shard-000006.tar\n- ${oc.env:DATASET_PREFIX}/voc2012_detection/val/shard-000007.tar\n- ${oc.env:DATASET_PREFIX}/voc2012_detection/val/shard-000008.tar\n- ${oc.env:DATASET_PREFIX}/voc2012_detection/val/shard-000009.tar\n- ${oc.env:DATASET_PREFIX}/voc2012_detection/val/shard-000010.tar\n- ${oc.env:DATASET_PREFIX}/voc2012_detection/val/shard-000011.tar\n- ${oc.env:DATASET_PREFIX}/voc2012_detection/val/shard-000012.tar\n- ${oc.env:DATASET_PREFIX}/voc2012_detection/val/shard-000013.tar\n- ${oc.env:DATASET_PREFIX}/voc2012_detection/val/shard-000014.tar\n- ${oc.env:DATASET_PREFIX}/voc2012_detection/val/shard-000015.tar\n- ${oc.env:DATASET_PREFIX}/voc2012_detection/val/shard-000016.tar\n- ${oc.env:DATASET_PREFIX}/voc2012_detection/val/shard-000017.tar\n- ${oc.env:DATASET_PREFIX}/voc2012_detection/val/shard-000018.tar\n- ${oc.env:DATASET_PREFIX}/voc2012_detection/val/shard-000019.tar\n- ${oc.env:DATASET_PREFIX}/voc2012_detection/val/shard-000020.tar\n- ${oc.env:DATASET_PREFIX}/voc2012_detection/val/shard-000021.tar\n- ${oc.env:DATASET_PREFIX}/voc2012_detection/val/shard-000022.tar\ntrain_size: 11540\nval_shards:\n- ${oc.env:DATASET_PREFIX}/voc2012_detection/train/shard-000000.tar\n- ${oc.env:DATASET_PREFIX}/voc2012_detection/train/shard-000001.tar\n- ${oc.env:DATASET_PREFIX}/voc2012_detection/train/shard-000002.tar\n- ${oc.env:DATASET_PREFIX}/voc2012_detection/train/shard-000003.tar\n- ${oc.env:DATASET_PREFIX}/voc2012_detection/train/shard-000004.tar\n- ${oc.env:DATASET_PREFIX}/voc2012_detection/train/shard-000005.tar\n- ${oc.env:DATASET_PREFIX}/voc2012_detection/train/shard-000006.tar\n- ${oc.env:DATASET_PREFIX}/voc2012_detection/train/shard-000007.tar\n- ${oc.env:DATASET_PREFIX}/voc2012_detection/train/shard-000008.tar\n- ${oc.env:DATASET_PREFIX}/voc2012_detection/train/shard-000009.tar\n- ${oc.env:DATASET_PREFIX}/voc2012_detection/train/shard-000010.tar\n- ${oc.env:DATASET_PREFIX}/voc2012_detection/train/shard-000011.tar\n- ${oc.env:DATASET_PREFIX}/voc2012_detection/train/shard-000012.tar\n- ${oc.env:DATASET_PREFIX}/voc2012_detection/train/shard-000013.tar\n- ${oc.env:DATASET_PREFIX}/voc2012_detection/train/shard-000014.tar\n- ${oc.env:DATASET_PREFIX}/voc2012_detection/train/shard-000015.tar\n- ${oc.env:DATASET_PREFIX}/voc2012_detection/train/shard-000016.tar\n- ${oc.env:DATASET_PREFIX}/voc2012_detection/train/shard-000017.tar\n- ${oc.env:DATASET_PREFIX}/voc2012_detection/train/shard-000018.tar\n- ${oc.env:DATASET_PREFIX}/voc2012_detection/train/shard-000019.tar\n- ${oc.env:DATASET_PREFIX}/voc2012_detection/train/shard-000020.tar\n- ${oc.env:DATASET_PREFIX}/voc2012_detection/train/shard-000021.tar\n- ${oc.env:DATASET_PREFIX}/voc2012_detection/val/shard-000000.tar\n- ${oc.env:DATASET_PREFIX}/voc2012_detection/val/shard-000001.tar\n- ${oc.env:DATASET_PREFIX}/voc2012_detection/val/shard-000002.tar\n- ${oc.env:DATASET_PREFIX}/voc2012_detection/val/shard-000003.tar\n- ${oc.env:DATASET_PREFIX}/voc2012_detection/val/shard-000004.tar\n- ${oc.env:DATASET_PREFIX}/voc2012_detection/val/shard-000005.tar\n- ${oc.env:DATASET_PREFIX}/voc2012_detection/val/shard-000006.tar\n- ${oc.env:DATASET_PREFIX}/voc2012_detection/val/shard-000007.tar\n- ${oc.env:DATASET_PREFIX}/voc2012_detection/val/shard-000008.tar\n- ${oc.env:DATASET_PREFIX}/voc2012_detection/val/shard-000009.tar\n- ${oc.env:DATASET_PREFIX}/voc2012_detection/val/shard-000010.tar\n- ${oc.env:DATASET_PREFIX}/voc2012_detection/val/shard-000011.tar\n- ${oc.env:DATASET_PREFIX}/voc2012_detection/val/shard-000012.tar\n- ${oc.env:DATASET_PREFIX}/voc2012_detection/val/shard-000013.tar\n- ${oc.env:DATASET_PREFIX}/voc2012_detection/val/shard-000014.tar\n- ${oc.env:DATASET_PREFIX}/voc2012_detection/val/shard-000015.tar\n- ${oc.env:DATASET_PREFIX}/voc2012_detection/val/shard-000016.tar\n- ${oc.env:DATASET_PREFIX}/voc2012_detection/val/shard-000017.tar\n- ${oc.env:DATASET_PREFIX}/voc2012_detection/val/shard-000018.tar\n- ${oc.env:DATASET_PREFIX}/voc2012_detection/val/shard-000019.tar\n- ${oc.env:DATASET_PREFIX}/voc2012_detection/val/shard-000020.tar\n- ${oc.env:DATASET_PREFIX}/voc2012_detection/val/shard-000021.tar\n- ${oc.env:DATASET_PREFIX}/voc2012_detection/val/shard-000022.tar\nval_size: 11540\ntest_shards: ${oc.env:DATASET_PREFIX}/voc2012_detection/test/shard-{000000..000041}.tar\ntest_size: 10991\nuse_autopadding: true\n</code></pre>"},{"location":"configs/evaluation/eval/","title":"configs/evaluation/eval.yaml","text":"<pre><code># @package _global_\n# Config file used with eval\ndefaults:\n- /evaluation_config # (1)!\n- _self_\n# Setting this allows to add modules to the model that are executed during evaluation\nmodules:\n# Setting this allows to evaluate on a different dataset than the model was trained on\ndataset:\n# Setting this allows to evaluate on different metrics than the model was trained on\nevaluation_metrics:\n</code></pre> <ol> <li>/evaluation_config</li> </ol>"},{"location":"configs/evaluation/projects/bridging/","title":"Evaluation configs of Bridging the Gap to Real-World Object-Centric Learning","text":""},{"location":"configs/evaluation/projects/bridging/_base_clustering/","title":"configs/evaluation/projects/bridging/_base_clustering.yaml","text":"<pre><code># @package _global_\nhydra:\nrun:\ndir: \"${eval_lambda:'lambda a, b: a if a is not None else b',${output_dir},./outputs/evaluation/clustering/${now:%Y-%m-%d_%H-%M-%S}}\"\nmodules:\nmasks_resized:\n_target_: routed.ocl.utils.resizing.Resize\ninput_path: object_decoder.masks\nsize_tensor_path: input.segmentation_mask\npatch_mode: true\nchannels_last: false\nmasks_path: masks_resized\nmodel_clusterings:\n# Unused here\n</code></pre>"},{"location":"configs/evaluation/projects/bridging/_base_metrics/","title":"configs/evaluation/projects/bridging/_base_metrics.yaml","text":"<pre><code># @package _global_\nhydra:\nrun:\ndir: \"${eval_lambda:'lambda a, b: a if a is not None else b',${output_dir},./outputs/evaluation/metrics/${now:%Y-%m-%d_%H-%M-%S}}\"\nmodules:\nmasks_resized:\n_target_: routed.ocl.utils.resizing.Resize\ninput_path: object_decoder.masks\nsize_tensor_path: input.segmentation_mask\npatch_mode: true\nchannels_last: false\n</code></pre>"},{"location":"configs/evaluation/projects/bridging/_classes_coco/","title":"configs/evaluation/projects/bridging/_classes_coco.yaml","text":"<pre><code># @package _global_\nclass_name_by_category_id:\n0: unused\n1: person\n2: bicycle\n3: car\n4: motorcycle\n5: airplane\n6: bus\n7: train\n8: truck\n9: boat\n10: traffic light\n11: fire hydrant\n13: stop sign\n14: parking meter\n15: bench\n16: bird\n17: cat\n18: dog\n19: horse\n20: sheep\n21: cow\n22: elephant\n23: bear\n24: zebra\n25: giraffe\n27: backpack\n28: umbrella\n31: handbag\n32: tie\n33: suitcase\n34: frisbee\n35: skis\n36: snowboard\n37: sports ball\n38: kite\n39: baseball bat\n40: baseball glove\n41: skateboard\n42: surfboard\n43: tennis racket\n44: bottle\n46: wine glass\n47: cup\n48: fork\n49: knife\n50: spoon\n51: bowl\n52: banana\n53: apple\n54: sandwich\n55: orange\n56: broccoli\n57: carrot\n58: hot dog\n59: pizza\n60: donut\n61: cake\n62: chair\n63: couch\n64: potted plant\n65: bed\n67: dining table\n70: toilet\n72: tv\n73: laptop\n74: mouse\n75: remote\n76: keyboard\n77: cell phone\n78: microwave\n79: oven\n80: toaster\n81: sink\n82: refrigerator\n84: book\n85: clock\n86: vase\n87: scissors\n88: teddy bear\n89: hair drier\n90: toothbrush\n92: banner\n93: blanket\n94: branch\n95: bridge\n96: building-other\n97: bush\n98: cabinet\n99: cage\n100: cardboard\n101: carpet\n102: ceiling-other\n103: ceiling-tile\n104: cloth\n105: clothes\n106: clouds\n107: counter\n108: cupboard\n109: curtain\n110: desk-stuff\n111: dirt\n112: door-stuff\n113: fence\n114: floor-marble\n115: floor-other\n116: floor-stone\n117: floor-tile\n118: floor-wood\n119: flower\n120: fog\n121: food-other\n122: fruit\n123: furniture-other\n124: grass\n125: gravel\n126: ground-other\n127: hill\n128: house\n129: leaves\n130: light\n131: mat\n132: metal\n133: mirror-stuff\n134: moss\n135: mountain\n136: mud\n137: napkin\n138: net\n139: paper\n140: pavement\n141: pillow\n142: plant-other\n143: plastic\n144: platform\n145: playingfield\n146: railing\n147: railroad\n148: river\n149: road\n150: rock\n151: roof\n152: rug\n153: salad\n154: sand\n155: sea\n156: shelf\n157: sky-other\n158: skyscraper\n159: snow\n160: solid-other\n161: stairs\n162: stone\n163: straw\n164: structural-other\n165: table\n166: tent\n167: textile-other\n168: towel\n169: tree\n170: vegetable\n171: wall-brick\n172: wall-concrete\n173: wall-other\n174: wall-panel\n175: wall-stone\n176: wall-tile\n177: wall-wood\n178: water-other\n179: waterdrops\n180: window-blind\n181: window-other\n182: wood\n183: other\n</code></pre>"},{"location":"configs/evaluation/projects/bridging/_classes_coco_coarse/","title":"configs/evaluation/projects/bridging/_classes_coco_coarse.yaml","text":"<pre><code># @package _global_\nclass_name_by_category_id:\n0: unlabeled\n1: person\n2: vehicle\n3: outdoor\n4: animal\n5: accessory\n6: sports\n7: kitchen\n8: food\n9: furniture\n10: electronic\n11: appliance\n12: indoor\n13: textile\n14: plant\n15: building\n16: furniture-stuff\n17: structural\n18: raw-material\n19: floor\n20: ceiling\n21: sky\n22: ground\n23: water\n24: food-stuff\n25: wall\n26: window\n27: solid\n28: other\n</code></pre>"},{"location":"configs/evaluation/projects/bridging/_classes_voc/","title":"configs/evaluation/projects/bridging/_classes_voc.yaml","text":"<pre><code># @package _global_\nclass_name_by_category_id:\n0: background\n1: aeroplane\n2: bicycle\n3: bird\n4: boat\n5: bottle\n6: bus\n7: car\n8: cat\n9: chair\n10: cow\n11: diningtable\n12: dog\n13: horse\n14: motorbike\n15: person\n16: pottedplant\n17: sheep\n18: sofa\n19: train\n20: tvmonitor\n</code></pre>"},{"location":"configs/evaluation/projects/bridging/_metrics_add_ignore_mask/","title":"configs/evaluation/projects/bridging/_metrics_add_ignore_mask.yaml","text":"<pre><code># @package _global_\n# Add ignore mask to mask metrics\nevaluation_metrics:\ninstance_mask_ari:\nignore_path: ignore_mask\ninstance_mask_unsup_iou:\nignore_path: ignore_mask\ninstance_mask_abo:\nignore_path: ignore_mask\ninstance_mask_corloc:\nignore_path: ignore_mask\ninstance_mask_recall:\nignore_path: ignore_mask\nsegmentation_mask_ari:\nignore_path: ignore_mask\nsegmentation_mask_unsup_iou:\nignore_path: ignore_mask\nsegmentation_mask_abo:\nignore_path: ignore_mask\n</code></pre>"},{"location":"configs/evaluation/projects/bridging/_metrics_discovery_bboxes/","title":"configs/evaluation/projects/bridging/_metrics_discovery_bboxes.yaml","text":"<pre><code># @package _global_\n# Metrics for object discovery on COCO/VOC with bounding boxes\nevaluation_metrics:\ninstance_bbox_corloc:\n_target_: routed.ocl.metrics.BboxCorLocMetric\nprediction_path: masks_resized\ntarget_path: input.instance_bbox\ninstance_bbox_recall:\n_target_: routed.ocl.metrics.BboxRecallMetric\nprediction_path: masks_resized\ntarget_path: input.instance_bbox\n</code></pre>"},{"location":"configs/evaluation/projects/bridging/_metrics_discovery_masks/","title":"configs/evaluation/projects/bridging/_metrics_discovery_masks.yaml","text":"<pre><code># @package _global_\n# Metrics for object discovery on COCO/VOC with instance masks.\nevaluation_metrics:\ninstance_mask_ari:\n_target_: routed.ocl.metrics.ARIMetric\nprediction_path: masks_resized\ntarget_path: input.instance_mask\nforeground: false\nconvert_target_one_hot: true\nignore_overlaps: true\ninstance_mask_unsup_iou:\n_target_: routed.ocl.metrics.UnsupervisedMaskIoUMetric\nprediction_path: masks_resized\ntarget_path: input.instance_mask\nignore_overlaps: true\ninstance_mask_abo:\n_target_: routed.ocl.metrics.AverageBestOverlapMetric\nprediction_path: masks_resized\ntarget_path: input.instance_mask\nignore_overlaps: true\ninstance_mask_corloc:\n_target_: routed.ocl.metrics.MaskCorLocMetric\nprediction_path: masks_resized\ntarget_path: input.instance_mask\nignore_overlaps: true\ninstance_mask_recall:\n_target_: routed.ocl.metrics.BestOverlapObjectRecoveryMetric\nprediction_path: masks_resized\ntarget_path: input.instance_mask\nignore_overlaps: true\n</code></pre>"},{"location":"configs/evaluation/projects/bridging/_metrics_discovery_movi/","title":"configs/evaluation/projects/bridging/_metrics_discovery_movi.yaml","text":"<pre><code># @package _global_\n# Metrics for MOVi datasets\nevaluation_metrics:\nari:\n_target_: routed.ocl.metrics.ARIMetric\nprediction_path: masks_resized\ntarget_path: input.mask\nforeground: true\nabo:\n_target_: routed.ocl.metrics.AverageBestOverlapMetric\nprediction_path: masks_resized\ntarget_path: input.mask\nignore_background: true\n</code></pre>"},{"location":"configs/evaluation/projects/bridging/_metrics_segmentation/","title":"configs/evaluation/projects/bridging/_metrics_segmentation.yaml","text":"<pre><code># @package _global_\n# Metrics for segmentation on COCO/VOC.\nevaluation_metrics:\nsegmentation_mask_ari:\n_target_: routed.ocl.metrics.ARIMetric\nprediction_path: masks_resized\ntarget_path: input.segmentation_mask\nforeground: false\nconvert_target_one_hot: true\nsegmentation_mask_unsup_iou:\n_target_: routed.ocl.metrics.UnsupervisedMaskIoUMetric\nprediction_path: masks_resized\ntarget_path: input.segmentation_mask\nsegmentation_mask_abo:\n_target_: routed.ocl.metrics.AverageBestOverlapMetric\nprediction_path: masks_resized\ntarget_path: input.segmentation_mask\n</code></pre>"},{"location":"configs/evaluation/projects/bridging/_preprocessing_coco/","title":"configs/evaluation/projects/bridging/_preprocessing_coco.yaml","text":"<pre><code># @package _global_\n# Preprocessing for evaluating on COCO with 80 things classes.\n#\n# Masks are in original image resolution.\ndataset:\neval_transforms:\n03a_preprocessing:\n_target_: ocl.transforms.Map\ntransform:\n_target_: torchvision.transforms.Compose\ntransforms:\n- _target_: ocl.preprocessing.InstanceMasksToDenseMasks\n- _target_: ocl.preprocessing.AddSegmentationMaskFromInstanceMask\n- _target_: ocl.preprocessing.AddEmptyMasks\nmask_keys:\n- instance_mask\n- segmentation_mask\n- _target_: ocl.preprocessing.CanonicalizeBboxes\nformat: xywh\n- _target_: ocl.preprocessing.AddEmptyBboxes\n- _target_: ocl.preprocessing.DropEntries\nkeys:\n- instance_category\nfields:\n- image\n- instance_mask\n- instance_category\n- instance_bbox\nbatch_transform: false\n03b_preprocessing:\n_target_: ocl.transforms.SimpleTransform\ntransforms:\nimage:\n_target_: torchvision.transforms.Compose\ntransforms:\n- _target_: torchvision.transforms.ToTensor\n- _target_: torchvision.transforms.Resize\n_convert_: all\nsize: [224, 224]\ninterpolation: ${torchvision_interpolation_mode:BICUBIC}\n- \"${lambda_fn:'lambda image: image.clamp(0.0, 1.0)'}\"\n- _target_: torchvision.transforms.Normalize\nmean: [0.485, 0.456, 0.406]\nstd: [0.229, 0.224, 0.225]\ninstance_mask:\n_target_: torchvision.transforms.Compose\ntransforms:\n- _target_: ocl.preprocessing.DenseMaskToTensor\nsegmentation_mask:\n_target_: torchvision.transforms.Compose\ntransforms:\n- _target_: ocl.preprocessing.DenseMaskToTensor\nbatch_transform: false\n</code></pre>"},{"location":"configs/evaluation/projects/bridging/_preprocessing_coco_27/","title":"configs/evaluation/projects/bridging/_preprocessing_coco_27.yaml","text":"<pre><code># @package _global_\n# Preprocessing for evaluating semantic segmentation on COCO-Stuff-164k 27 coarse classes.\n#\n# Segmentation mask is resized to 320 pixels shorter side and center cropped to 320x320.\ndataset:\neval_transforms:\n03a_preprocessing:\n_target_: ocl.transforms.Map\ntransform:\n_target_: torchvision.transforms.Compose\ntransforms:\n- _target_: ocl.preprocessing.ConvertCocoStuff164kMasks\noutput_key: segmentation_mask\nstuffthings_key: stuffthings_mask\nignore_key: ignore_mask\n- _target_: ocl.preprocessing.DropEntries\nkeys:\n- stuffthings_mask\nfields:\n- stuffthings_mask\nbatch_transform: false\n03b_preprocessing:\n_target_: ocl.transforms.SimpleTransform\ntransforms:\nimage:\n_target_: torchvision.transforms.Compose\ntransforms:\n- _target_: torchvision.transforms.ToTensor\n- _target_: torchvision.transforms.Resize\nsize: 224\ninterpolation: ${torchvision_interpolation_mode:BICUBIC}\n- \"${lambda_fn:'lambda image: image.clamp(0.0, 1.0)'}\"\n- _target_: torchvision.transforms.CenterCrop\nsize: 224\n- _target_: torchvision.transforms.Normalize\nmean: [0.485, 0.456, 0.406]\nstd: [0.229, 0.224, 0.225]\nsegmentation_mask:\n_target_: torchvision.transforms.Compose\ntransforms:\n- _target_: ocl.preprocessing.ConvertToCocoSuperclasses\n- _target_: ocl.preprocessing.DenseMaskToTensor\n- _target_: ocl.preprocessing.ResizeNearestExact\nsize: 320\n- _target_: torchvision.transforms.CenterCrop\nsize: 320\nignore_mask:\n_target_: torchvision.transforms.Compose\ntransforms:\n- _target_: ocl.preprocessing.DenseMaskToTensor\n- _target_: ocl.preprocessing.ResizeNearestExact\nsize: 320\n- _target_: torchvision.transforms.CenterCrop\nsize: 320\nbatch_transform: false\n</code></pre>"},{"location":"configs/evaluation/projects/bridging/_preprocessing_coco_things/","title":"configs/evaluation/projects/bridging/_preprocessing_coco_things.yaml","text":"<pre><code># @package _global_\n# Preprocessing for evaluating semantic segmentation on COCO-Stuff-164k only 90 things classes.\n#\n# Stuff classes are set to background.\n#\n# Masks are in original image resolution.\ndataset:\neval_transforms:\n03a_preprocessing:\n_target_: ocl.transforms.Map\ntransform:\n_target_: torchvision.transforms.Compose\ntransforms:\n- _target_: ocl.preprocessing.ConvertCocoStuff164kMasks\noutput_key: segmentation_mask\nstuffthings_key: stuffthings_mask\nignore_key: ignore_mask\ndrop_stuff: true\n- _target_: ocl.preprocessing.DropEntries\nkeys:\n- stuffthings_mask\nfields:\n- stuffthings_mask\nbatch_transform: false\n03b_preprocessing:\n_target_: ocl.transforms.SimpleTransform\ntransforms:\nimage:\n_target_: torchvision.transforms.Compose\ntransforms:\n- _target_: torchvision.transforms.ToTensor\n- _target_: torchvision.transforms.Resize\nsize: 224\ninterpolation: ${torchvision_interpolation_mode:BICUBIC}\n- \"${lambda_fn:'lambda image: image.clamp(0.0, 1.0)'}\"\n- _target_: torchvision.transforms.CenterCrop\nsize: 224\n- _target_: torchvision.transforms.Normalize\nmean: [0.485, 0.456, 0.406]\nstd: [0.229, 0.224, 0.225]\nsegmentation_mask:\n_target_: torchvision.transforms.Compose\ntransforms:\n- _target_: ocl.preprocessing.DenseMaskToTensor\nignore_mask:\n_target_: torchvision.transforms.Compose\ntransforms:\n- _target_: ocl.preprocessing.DenseMaskToTensor\nbatch_transform: false\n</code></pre>"},{"location":"configs/evaluation/projects/bridging/_preprocessing_coco_thingsandstuff/","title":"configs/evaluation/projects/bridging/_preprocessing_coco_thingsandstuff.yaml","text":"<pre><code># @package _global_\n# Preprocessing for evaluating semantic segmentation on COCO-Stuff-164k 172 thingsandstuff classes.\n#\n# Segmentation mask is resized to 320 pixels shorter side and center cropped to 320x320.\ndataset:\neval_transforms:\n03a_preprocessing:\n_target_: ocl.transforms.Map\ntransform:\n_target_: torchvision.transforms.Compose\ntransforms:\n- _target_: ocl.preprocessing.ConvertCocoStuff164kMasks\noutput_key: segmentation_mask\nstuffthings_key: stuffthings_mask\nignore_key: ignore_mask\n- _target_: ocl.preprocessing.DropEntries\nkeys:\n- stuffthings_mask\nfields:\n- stuffthings_mask\nbatch_transform: false\n03b_preprocessing:\n_target_: ocl.transforms.SimpleTransform\ntransforms:\nimage:\n_target_: torchvision.transforms.Compose\ntransforms:\n- _target_: torchvision.transforms.ToTensor\n- _target_: torchvision.transforms.Resize\nsize: 224\ninterpolation: ${torchvision_interpolation_mode:BICUBIC}\n- \"${lambda_fn:'lambda image: image.clamp(0.0, 1.0)'}\"\n- _target_: torchvision.transforms.CenterCrop\nsize: 224\n- _target_: torchvision.transforms.Normalize\nmean: [0.485, 0.456, 0.406]\nstd: [0.229, 0.224, 0.225]\nsegmentation_mask:\n_target_: torchvision.transforms.Compose\ntransforms:\n- _target_: ocl.preprocessing.DenseMaskToTensor\n- _target_: ocl.preprocessing.ResizeNearestExact\nsize: 320\n- _target_: torchvision.transforms.CenterCrop\nsize: 320\nignore_mask:\n_target_: torchvision.transforms.Compose\ntransforms:\n- _target_: ocl.preprocessing.DenseMaskToTensor\n- _target_: ocl.preprocessing.ResizeNearestExact\nsize: 320\n- _target_: torchvision.transforms.CenterCrop\nsize: 320\nbatch_transform: false\n</code></pre>"},{"location":"configs/evaluation/projects/bridging/_preprocessing_movi_c/","title":"configs/evaluation/projects/bridging/_preprocessing_movi_c.yaml","text":"<pre><code># @package _global_\n# Preprocessing for evaluating on MOVi-C\ndataset:\neval_transforms:\n03_preprocessing:\n_target_: ocl.transforms.SimpleTransform\ntransforms:\nimage:\n_target_: torchvision.transforms.Compose\ntransforms:\n- _target_: torchvision.transforms.ToTensor\n- _target_: torchvision.transforms.Resize\nsize: 224\ninterpolation: ${torchvision_interpolation_mode:BICUBIC}\n- \"${lambda_fn:'lambda image: image.clamp(0.0, 1.0)'}\" # Bicubic interpolation can get out of range\n- _target_: torchvision.transforms.Normalize\nmean: [0.485, 0.456, 0.406]\nstd: [0.229, 0.224, 0.225]\nmask:\n_target_: torchvision.transforms.Compose\ntransforms:\n- _target_: ocl.preprocessing.MultiMaskToTensor\n- _target_: ocl.preprocessing.CheckFormat\nshape: [11, 128, 128]\none_hot: true\nbatch_transform: false\n</code></pre>"},{"location":"configs/evaluation/projects/bridging/_preprocessing_movi_e/","title":"configs/evaluation/projects/bridging/_preprocessing_movi_e.yaml","text":"<pre><code># @package _global_\n# Preprocessing for evaluating on MOVi-E\ndataset:\neval_transforms:\n03_preprocessing:\n_target_: ocl.transforms.SimpleTransform\ntransforms:\nimage:\n_target_: torchvision.transforms.Compose\ntransforms:\n- _target_: torchvision.transforms.ToTensor\n- _target_: torchvision.transforms.Resize\nsize: 224\ninterpolation: ${torchvision_interpolation_mode:BICUBIC}\n- \"${lambda_fn:'lambda image: image.clamp(0.0, 1.0)'}\" # Bicubic interpolation can get out of range\n- _target_: torchvision.transforms.Normalize\nmean: [0.485, 0.456, 0.406]\nstd: [0.229, 0.224, 0.225]\nmask:\n_target_: torchvision.transforms.Compose\ntransforms:\n- _target_: ocl.preprocessing.MultiMaskToTensor\n- _target_: ocl.preprocessing.CheckFormat\nshape: [24, 128, 128]\none_hot: true\nbatch_transform: false\n</code></pre>"},{"location":"configs/evaluation/projects/bridging/_preprocessing_to_ccrop_320_masks/","title":"configs/evaluation/projects/bridging/_preprocessing_to_ccrop_320_masks.yaml","text":"<pre><code># @package _global_\n# Change eval masks to center crop at 320x320 resolution.\nplugins:\n03b_preprocessing:\nevaluation_transforms:\ninstance_mask:\n_target_: torchvision.transforms.Compose\ntransforms:\n- _target_: ocl.preprocessing.DenseMaskToTensor\n- _target_: ocl.preprocessing.ResizeNearestExact\nsize: 320\n- _target_: torchvision.transforms.CenterCrop\nsize: 320\nsegmentation_mask:\n_target_: torchvision.transforms.Compose\ntransforms:\n- _target_: ocl.preprocessing.DenseMaskToTensor\n- _target_: ocl.preprocessing.ResizeNearestExact\nsize: 320\n- _target_: torchvision.transforms.CenterCrop\nsize: 320\n</code></pre>"},{"location":"configs/evaluation/projects/bridging/_preprocessing_to_ccrop_image/","title":"configs/evaluation/projects/bridging/_preprocessing_to_ccrop_image.yaml","text":"<pre><code># @package _global_\n# Change eval image to center crop at 320x320 resolution.\nplugins:\n03b_preprocessing:\nevaluation_transforms:\nimage:\n_target_: torchvision.transforms.Compose\ntransforms:\n- _target_: torchvision.transforms.ToTensor\n- _target_: torchvision.transforms.Resize\nsize: 224\ninterpolation: ${torchvision_interpolation_mode:BICUBIC}\n- \"${lambda_fn:'lambda image: image.clamp(0.0, 1.0)'}\"\n- _target_: torchvision.transforms.CenterCrop\nsize: 224\n- _target_: torchvision.transforms.Normalize\nmean: [0.485, 0.456, 0.406]\nstd: [0.229, 0.224, 0.225]\n</code></pre>"},{"location":"configs/evaluation/projects/bridging/_preprocessing_voc2012/","title":"configs/evaluation/projects/bridging/_preprocessing_voc2012.yaml","text":"<pre><code># @package _global_\ndataset:\neval_transforms:\n02a_format_consistency:\n_target_: ocl.transforms.Map\ntransform:\n_target_: ocl.preprocessing.RenameFields\nmapping:\nobjects.bbox: instance_bbox\nfields:\n- objects.bbox\nbatch_transform: false\n03a_preprocessing:\n_target_: ocl.transforms.Map\ntransform:\n_target_: torchvision.transforms.Compose\ntransforms:\n- _target_: ocl.preprocessing.AddImageSize\n- _target_: ocl.preprocessing.CanonicalizeBboxes\nformat: yxyx\n- _target_: ocl.preprocessing.RescaleBboxes\n- _target_: ocl.preprocessing.AddEmptyBboxes\nfields:\n- image\nbatch_transform: false\n03b_preprocessing:\n_target_: ocl.transforms.SimpleTransform\ntransforms:\nimage:\n_target_: torchvision.transforms.Compose\ntransforms:\n- _target_: torchvision.transforms.ToTensor\n- _target_: torchvision.transforms.Resize\n_convert_: all\nsize: [224, 224]\ninterpolation: ${torchvision_interpolation_mode:BICUBIC}\n- \"${lambda_fn:'lambda image: image.clamp(0.0, 1.0)'}\"\n- _target_: torchvision.transforms.Normalize\nmean: [0.485, 0.456, 0.406]\nstd: [0.229, 0.224, 0.225]\nbatch_transform: false\n</code></pre>"},{"location":"configs/evaluation/projects/bridging/_preprocessing_voc2012_trainaug/","title":"configs/evaluation/projects/bridging/_preprocessing_voc2012_trainaug.yaml","text":"<pre><code># @package _global_\ndataset:\neval_transforms:\n02a_format_consistency:\n_target_: ocl.transforms.SimpleTransform\ntransforms:\n# Convert to one-hot encoding.\nsegmentation-instance:\n_target_: ocl.preprocessing.IntegerToOneHotMask\nbatch_transform: false\n02b_format_consistency:\n_target_: ocl.transforms.Map\ntransform:\n_target_: torchvision.transforms.Compose\ntransforms:\n# Create segmentation mask.\n- _target_: ocl.preprocessing.VOCInstanceMasksToDenseMasks\ninstance_mask_key: segmentation-instance\nclass_mask_key: segmentation-class\nclasses_key: instance_category\n- _target_: ocl.preprocessing.RenameFields\nmapping:\nsegmentation-instance: instance_mask\nobjects.bbox: instance_bbox\nfields:\n- segmentation-instance\n- segmentation-class\n- objects.bbox\n- image\nbatch_transform: false\n03a_preprocessing:\n_target_: ocl.transforms.Map\ntransform:\n_target_: torchvision.transforms.Compose\ntransforms:\n- _target_: ocl.preprocessing.AddSegmentationMaskFromInstanceMask\n- _target_: ocl.preprocessing.AddEmptyMasks\nmask_keys:\n- instance_mask\n- segmentation_mask\n- ignore_mask\n- _target_: ocl.preprocessing.CanonicalizeBboxes\nformat: yxyx\n- _target_: ocl.preprocessing.RescaleBboxes\n- _target_: ocl.preprocessing.AddEmptyBboxes\nfields:\n- image\n- instance_mask\n- instance_category\nbatch_transform: false\n03b_preprocessing:\n_target_: ocl.transforms.SimpleTransform\ntransforms:\nimage:\n_target_: torchvision.transforms.Compose\ntransforms:\n- _target_: torchvision.transforms.ToTensor\n- _target_: torchvision.transforms.Resize\n_convert_: all\nsize: [224, 224]\ninterpolation: ${torchvision_interpolation_mode:BICUBIC}\n- \"${lambda_fn:'lambda image: image.clamp(0.0, 1.0)'}\"\n- _target_: torchvision.transforms.Normalize\nmean: [0.485, 0.456, 0.406]\nstd: [0.229, 0.224, 0.225]\ninstance_mask:\n_target_: torchvision.transforms.Compose\ntransforms:\n- _target_: ocl.preprocessing.DenseMaskToTensor\nsegmentation_mask:\n_target_: torchvision.transforms.Compose\ntransforms:\n- _target_: ocl.preprocessing.DenseMaskToTensor\nignore_mask:\n_target_: torchvision.transforms.Compose\ntransforms:\n- _target_: ocl.preprocessing.DenseMaskToTensor\nbatch_transform: false\n</code></pre>"},{"location":"configs/evaluation/projects/bridging/clustering_coco/","title":"configs/evaluation/projects/bridging/clustering_coco.yaml","text":"<pre><code># @package _global_\ndefaults:\n- /evaluation_clustering_config  # (1)!\n- /evaluation/projects/bridging/_base_clustering # (2)!\n- /evaluation/projects/bridging/_classes_coco # (3)!\n- /evaluation/projects/bridging/_preprocessing_coco_things # (4)!\n- /dataset: coco_nocrowd  # (5)!\n- _self_\nbatch_size: 1\nn_classes: 90\nignore_background: false\nuse_unmatched_as_background: true\nuse_ignore_masks: true\nrepresentation_type: RepresentationType.MASK_WEIGHTED_FEATURES\nclusterings:\nkmeans172_l2:\n_target_: ocl.metrics.SklearnClustering\nn_clusters: 172\nmethod: kmeans\nuse_l2_normalization: true\nclustering_kwargs:\nn_init: 20\n</code></pre> <ol> <li>/evaluation_clustering_config</li> <li>/evaluation/projects/bridging/_base_clustering</li> <li>/evaluation/projects/bridging/_classes_coco</li> <li>/evaluation/projects/bridging/_preprocessing_coco_things</li> <li>/dataset/coco_nocrowd</li> </ol>"},{"location":"configs/evaluation/projects/bridging/clustering_coco_20k/","title":"configs/evaluation/projects/bridging/clustering_coco_20k.yaml","text":"<pre><code># @package _global_\ndefaults:\n- /evaluation_clustering_config  # (1)!\n- /evaluation/projects/bridging/_base_clustering # (2)!\n- /evaluation/projects/bridging/_classes_coco # (3)!\n- /evaluation/projects/bridging/_preprocessing_coco # (4)!\n- /dataset: coco2014_20k  # (5)!\n- _self_\ndataset:\ntrain_shards:\ntest_shards:\nbatch_size: 1\nn_classes: 90\nignore_background: false\nuse_unmatched_as_background: true\nrepresentation_type: RepresentationType.MASK_WEIGHTED_FEATURES\nclusterings:\nkmeans172_l2:\n_target_: ocl.metrics.SklearnClustering\nn_clusters: 172\nmethod: kmeans\nuse_l2_normalization: true\nclustering_kwargs:\nn_init: 20\n</code></pre> <ol> <li>/evaluation_clustering_config</li> <li>/evaluation/projects/bridging/_base_clustering</li> <li>/evaluation/projects/bridging/_classes_coco</li> <li>/evaluation/projects/bridging/_preprocessing_coco</li> <li>/dataset/coco2014_20k</li> </ol>"},{"location":"configs/evaluation/projects/bridging/clustering_coco_27/","title":"configs/evaluation/projects/bridging/clustering_coco_27.yaml","text":"<pre><code># @package _global_\ndefaults:\n- /evaluation_clustering_config  # (1)!\n- /evaluation/projects/bridging/_base_clustering # (2)!\n- /evaluation/projects/bridging/_classes_coco_coarse # (3)!\n- /evaluation/projects/bridging/_preprocessing_coco_27 # (4)!\n- /dataset: coco_nocrowd  # (5)!\n- _self_\nbatch_size: 32\nn_classes: 27\nignore_background: true\nuse_ignore_masks: true\nrepresentation_type: RepresentationType.MASK_WEIGHTED_FEATURES\nclusterings:\nkmeans27_l2:\n_target_: ocl.metrics.SklearnClustering\nn_clusters: 27\nmethod: kmeans\nuse_l2_normalization: true\nclustering_kwargs:\nn_init: 20\n</code></pre> <ol> <li>/evaluation_clustering_config</li> <li>/evaluation/projects/bridging/_base_clustering</li> <li>/evaluation/projects/bridging/_classes_coco_coarse</li> <li>/evaluation/projects/bridging/_preprocessing_coco_27</li> <li>/dataset/coco_nocrowd</li> </ol>"},{"location":"configs/evaluation/projects/bridging/clustering_coco_cls/","title":"configs/evaluation/projects/bridging/clustering_coco_cls.yaml","text":"<pre><code># @package _global_\ndefaults:\n- /evaluation_clustering_config  # (1)!\n- /evaluation/projects/bridging/_base_clustering # (2)!\n- /evaluation/projects/bridging/_classes_coco # (3)!\n- /evaluation/projects/bridging/_preprocessing_coco_things # (4)!\n- /dataset: coco_nocrowd  # (5)!\n- _self_\nbatch_size: 1\nn_classes: 90\nignore_background: false\nuse_unmatched_as_background: true\nuse_ignore_masks: true\ntrain_config_overrides:\n- models.feature_extractor.aux_features=cls12\nfeatures_path: cls12\nrepresentation_type: RepresentationType.CLS_ON_MASKED_INPUT\nclusterings:\nkmeans172_l2:\n_target_: ocl.metrics.SklearnClustering\nn_clusters: 172\nmethod: kmeans\nuse_l2_normalization: true\nclustering_kwargs:\nn_init: 20\n</code></pre> <ol> <li>/evaluation_clustering_config</li> <li>/evaluation/projects/bridging/_base_clustering</li> <li>/evaluation/projects/bridging/_classes_coco</li> <li>/evaluation/projects/bridging/_preprocessing_coco_things</li> <li>/dataset/coco_nocrowd</li> </ol>"},{"location":"configs/evaluation/projects/bridging/clustering_coco_thingsandstuff/","title":"configs/evaluation/projects/bridging/clustering_coco_thingsandstuff.yaml","text":"<pre><code># @package _global_\ndefaults:\n- /evaluation_clustering_config  # (1)!\n- /evaluation/projects/bridging/_base_clustering # (2)!\n- /evaluation/projects/bridging/_classes_coco # (3)!\n- /evaluation/projects/bridging/_preprocessing_coco_thingsandstuff # (4)!\n- /dataset: coco_nocrowd  # (5)!\n- _self_\nbatch_size: 32\nn_classes: 183\nignore_background: true\nuse_ignore_masks: true\nrepresentation_type: RepresentationType.MASK_WEIGHTED_FEATURES\nclusterings:\nkmeans172_l2:\n_target_: ocl.metrics.SklearnClustering\nn_clusters: 172\nmethod: kmeans\nuse_l2_normalization: true\nclustering_kwargs:\nn_init: 20\n</code></pre> <ol> <li>/evaluation_clustering_config</li> <li>/evaluation/projects/bridging/_base_clustering</li> <li>/evaluation/projects/bridging/_classes_coco</li> <li>/evaluation/projects/bridging/_preprocessing_coco_thingsandstuff</li> <li>/dataset/coco_nocrowd</li> </ol>"},{"location":"configs/evaluation/projects/bridging/clustering_voc2012/","title":"configs/evaluation/projects/bridging/clustering_voc2012.yaml","text":"<pre><code># @package _global_\ndefaults:\n- /evaluation_clustering_config  # (1)!\n- /evaluation/projects/bridging/_base_clustering # (2)!\n- /evaluation/projects/bridging/_classes_voc # (3)!\n- /evaluation/projects/bridging/_preprocessing_voc2012_trainaug # (4)!\n- /dataset: voc2012_trainaug  # (5)!\n- _self_\nbatch_size: 1\nn_classes: 20\nignore_background: false\nuse_unmatched_as_background: true\nuse_ignore_masks: true\nrepresentation_type: RepresentationType.MASK_WEIGHTED_FEATURES\nclusterings:\nkmeans105_l2:\n_target_: ocl.metrics.SklearnClustering\nn_clusters: 105\nmethod: kmeans\nuse_l2_normalization: true\nclustering_kwargs:\nn_init: 20\n</code></pre> <ol> <li>/evaluation_clustering_config</li> <li>/evaluation/projects/bridging/_base_clustering</li> <li>/evaluation/projects/bridging/_classes_voc</li> <li>/evaluation/projects/bridging/_preprocessing_voc2012_trainaug</li> <li>/dataset/voc2012_trainaug</li> </ol>"},{"location":"configs/evaluation/projects/bridging/metrics_coco/","title":"configs/evaluation/projects/bridging/metrics_coco.yaml","text":"<pre><code># @package _global_\n# Evaluate bounding boxes and masks on COCO at original image resolution.\ndefaults:\n- /evaluation_config  # (1)!\n- /evaluation/projects/bridging/_base_metrics # (2)!\n- /evaluation/projects/bridging/_preprocessing_coco # (3)!\n- /evaluation/projects/bridging/_metrics_discovery_masks # (4)!\n- /evaluation/projects/bridging/_metrics_discovery_bboxes # (5)!\n- /dataset: coco_nocrowd  # (6)!\n- _self_\neval_batch_size: 1\n</code></pre> <ol> <li>/evaluation_config</li> <li>/evaluation/projects/bridging/_base_metrics</li> <li>/evaluation/projects/bridging/_preprocessing_coco</li> <li>/evaluation/projects/bridging/_metrics_discovery_masks</li> <li>/evaluation/projects/bridging/_metrics_discovery_bboxes</li> <li>/dataset/coco_nocrowd</li> </ol>"},{"location":"configs/evaluation/projects/bridging/metrics_coco_20k/","title":"configs/evaluation/projects/bridging/metrics_coco_20k.yaml","text":"<pre><code># @package _global_\n# Evaluate bounding boxes and masks on COCO-20k at original image resolution.\ndefaults:\n- /evaluation_config  # (1)!\n- /evaluation/projects/bridging/_base_metrics # (2)!\n- /evaluation/projects/bridging/_preprocessing_coco # (3)!\n- /evaluation/projects/bridging/_metrics_discovery_masks # (4)!\n- /evaluation/projects/bridging/_metrics_discovery_bboxes # (5)!\n- /dataset: coco2014_20k  # (6)!\n- _self_\ndataset:\ntrain_shards:\ntest_shards:\neval_batch_size: 1\n</code></pre> <ol> <li>/evaluation_config</li> <li>/evaluation/projects/bridging/_base_metrics</li> <li>/evaluation/projects/bridging/_preprocessing_coco</li> <li>/evaluation/projects/bridging/_metrics_discovery_masks</li> <li>/evaluation/projects/bridging/_metrics_discovery_bboxes</li> <li>/dataset/coco2014_20k</li> </ol>"},{"location":"configs/evaluation/projects/bridging/metrics_coco_ccrop/","title":"configs/evaluation/projects/bridging/metrics_coco_ccrop.yaml","text":"<pre><code># @package _global_\n# Evaluate center crop masks on COCO.\ndefaults:\n- /evaluation_config  # (1)!\n- /evaluation/projects/bridging/_base_metrics # (2)!\n- /evaluation/projects/bridging/_preprocessing_coco # (3)!\n- /evaluation/projects/bridging/_metrics_discovery_masks # (4)!\n- /evaluation/projects/bridging/_metrics_segmentation # (5)!\n- /dataset: coco_nocrowd  # (6)!\n- _self_\neval_batch_size: 16\nplugins:\n03b_preprocessing:\nevaluation_transforms:\nimage:\n_target_: torchvision.transforms.Compose\ntransforms:\n- _target_: torchvision.transforms.ToTensor\n- _target_: torchvision.transforms.Resize\nsize: 224\ninterpolation: ${torchvision_interpolation_mode:BICUBIC}\n- \"${lambda_fn:'lambda image: image.clamp(0.0, 1.0)'}\"\n- _target_: torchvision.transforms.CenterCrop\nsize: 224\n- _target_: torchvision.transforms.Normalize\nmean: [0.485, 0.456, 0.406]\nstd: [0.229, 0.224, 0.225]\ninstance_mask:\n_target_: torchvision.transforms.Compose\ntransforms:\n- _target_: ocl.preprocessing.DenseMaskToTensor\n- _target_: ocl.preprocessing.ResizeNearestExact\nsize: 320\n- _target_: torchvision.transforms.CenterCrop\nsize: 320\nsegmentation_mask:\n_target_: torchvision.transforms.Compose\ntransforms:\n- _target_: ocl.preprocessing.DenseMaskToTensor\n- _target_: ocl.preprocessing.ResizeNearestExact\nsize: 320\n- _target_: torchvision.transforms.CenterCrop\nsize: 320\n</code></pre> <ol> <li>/evaluation_config</li> <li>/evaluation/projects/bridging/_base_metrics</li> <li>/evaluation/projects/bridging/_preprocessing_coco</li> <li>/evaluation/projects/bridging/_metrics_discovery_masks</li> <li>/evaluation/projects/bridging/_metrics_segmentation</li> <li>/dataset/coco_nocrowd</li> </ol>"},{"location":"configs/evaluation/projects/bridging/metrics_coco_ccrop_random/","title":"configs/evaluation/projects/bridging/metrics_coco_ccrop_random.yaml","text":"<pre><code># @package _global_\n# Evaluate random masks on COCO center crops.\ndefaults:\n- /evaluation_config  # (1)!\n- /evaluation/projects/bridging/_base_metrics # (2)!\n- /evaluation/projects/bridging/_preprocessing_coco # (3)!\n- /evaluation/projects/bridging/_metrics_discovery_masks # (4)!\n- /evaluation/projects/bridging/_metrics_segmentation # (5)!\n- /dataset: coco_nocrowd  # (6)!\n- _self_\neval_batch_size: 16\nmodules:\nrandom_masks:\n_target_: routed.ocl.utils.masking.CreateRandomMaskPatterns\nmasks_path: masks_resized\npattern: blocks\nn_slots: 11\nn_cols: 3\nevaluation_metrics:\ninstance_mask_ari:\nprediction_path: random_masks\ninstance_mask_unsup_iou:\nprediction_path: random_masks\ninstance_mask_abo:\nprediction_path: random_masks\ninstance_mask_corloc:\nprediction_path: random_masks\ninstance_mask_recall:\nprediction_path: random_masks\nsegmentation_mask_ari:\nprediction_path: random_masks\nsegmentation_mask_unsup_iou:\nprediction_path: random_masks\nsegmentation_mask_abo:\nprediction_path: random_masks\nplugins:\n03b_preprocessing:\nevaluation_transforms:\nimage:\n_target_: torchvision.transforms.Compose\ntransforms:\n- _target_: torchvision.transforms.ToTensor\n- _target_: torchvision.transforms.Resize\nsize: 224\ninterpolation: ${torchvision_interpolation_mode:BICUBIC}\n- \"${lambda_fn:'lambda image: image.clamp(0.0, 1.0)'}\"\n- _target_: torchvision.transforms.CenterCrop\nsize: 224\n- _target_: torchvision.transforms.Normalize\nmean: [0.485, 0.456, 0.406]\nstd: [0.229, 0.224, 0.225]\ninstance_mask:\n_target_: torchvision.transforms.Compose\ntransforms:\n- _target_: ocl.preprocessing.DenseMaskToTensor\n- _target_: ocl.preprocessing.ResizeNearestExact\nsize: 320\n- _target_: torchvision.transforms.CenterCrop\nsize: 320\nsegmentation_mask:\n_target_: torchvision.transforms.Compose\ntransforms:\n- _target_: ocl.preprocessing.DenseMaskToTensor\n- _target_: ocl.preprocessing.ResizeNearestExact\nsize: 320\n- _target_: torchvision.transforms.CenterCrop\nsize: 320\n</code></pre> <ol> <li>/evaluation_config</li> <li>/evaluation/projects/bridging/_base_metrics</li> <li>/evaluation/projects/bridging/_preprocessing_coco</li> <li>/evaluation/projects/bridging/_metrics_discovery_masks</li> <li>/evaluation/projects/bridging/_metrics_segmentation</li> <li>/dataset/coco_nocrowd</li> </ol>"},{"location":"configs/evaluation/projects/bridging/metrics_coco_ccrop_sa/","title":"configs/evaluation/projects/bridging/metrics_coco_ccrop_sa.yaml","text":"<pre><code># @package _global_\n# Evaluate center crop masks on COCO from a slot attention model.\ndefaults:\n- /evaluation_config  # (1)!\n- /evaluation/projects/bridging/_base_metrics # (2)!\n- /evaluation/projects/bridging/_preprocessing_coco # (3)!\n- /evaluation/projects/bridging/_metrics_discovery_masks # (4)!\n- /evaluation/projects/bridging/_metrics_segmentation # (5)!\n- /dataset: coco_nocrowd  # (6)!\n- _self_\neval_batch_size: 16\nmodules:\nmasks_resized:\npatch_mode: false\nplugins:\n03b_preprocessing:\nevaluation_transforms:\nimage:\n_target_: torchvision.transforms.Compose\ntransforms:\n- _target_: torchvision.transforms.ToTensor\n- _target_: torchvision.transforms.Resize\nsize: 128\ninterpolation: ${torchvision_interpolation_mode:BILINEAR}\n- _target_: torchvision.transforms.CenterCrop\nsize: 128\n- _target_: torchvision.transforms.Normalize\nmean: [0.5, 0.5, 0.5]\nstd: [0.5, 0.5, 0.5]\ninstance_mask:\n_target_: torchvision.transforms.Compose\ntransforms:\n- _target_: ocl.preprocessing.DenseMaskToTensor\n- _target_: ocl.preprocessing.ResizeNearestExact\nsize: 320\n- _target_: torchvision.transforms.CenterCrop\nsize: 320\nsegmentation_mask:\n_target_: torchvision.transforms.Compose\ntransforms:\n- _target_: ocl.preprocessing.DenseMaskToTensor\n- _target_: ocl.preprocessing.ResizeNearestExact\nsize: 320\n- _target_: torchvision.transforms.CenterCrop\nsize: 320\n</code></pre> <ol> <li>/evaluation_config</li> <li>/evaluation/projects/bridging/_base_metrics</li> <li>/evaluation/projects/bridging/_preprocessing_coco</li> <li>/evaluation/projects/bridging/_metrics_discovery_masks</li> <li>/evaluation/projects/bridging/_metrics_segmentation</li> <li>/dataset/coco_nocrowd</li> </ol>"},{"location":"configs/evaluation/projects/bridging/metrics_movi_c/","title":"configs/evaluation/projects/bridging/metrics_movi_c.yaml","text":"<pre><code># @package _global_\n# Evaluate on MOVi-C\ndefaults:\n- /evaluation_config  # (1)!\n- /evaluation/projects/bridging/_base_metrics # (2)!\n- /evaluation/projects/bridging/_preprocessing_movi_c # (3)!\n- /evaluation/projects/bridging/_metrics_discovery_movi # (4)!\n- /dataset: movi_c_image  # (5)!\n- _self_\neval_batch_size: 16\nmodules:\nmasks_resized:\nsize_tensor_path: input.mask\n</code></pre> <ol> <li>/evaluation_config</li> <li>/evaluation/projects/bridging/_base_metrics</li> <li>/evaluation/projects/bridging/_preprocessing_movi_c</li> <li>/evaluation/projects/bridging/_metrics_discovery_movi</li> <li>/dataset/movi_c_image</li> </ol>"},{"location":"configs/evaluation/projects/bridging/metrics_movi_c_sa/","title":"configs/evaluation/projects/bridging/metrics_movi_c_sa.yaml","text":"<pre><code># @package _global_\n# Evaluate on MOVi-C with slot attention\ndefaults:\n- /evaluation_config  # (1)!\n- /evaluation/projects/bridging/_base_metrics # (2)!\n- /evaluation/projects/bridging/_preprocessing_movi_c # (3)!\n- /evaluation/projects/bridging/_metrics_discovery_movi # (4)!\n- /dataset: movi_c_image  # (5)!\n- _self_\neval_batch_size: 16\nmodules:\nmasks_resized:\nsize_tensor_path: input.mask\npatch_mode: false\nplugins:\n03_preprocessing:\nevaluation_transforms:\nimage:\n_target_: torchvision.transforms.Compose\ntransforms:\n- _target_: torchvision.transforms.ToTensor\n- _target_: torchvision.transforms.Normalize\nmean: [0.5, 0.5, 0.5]\nstd: [0.5, 0.5, 0.5]\n</code></pre> <ol> <li>/evaluation_config</li> <li>/evaluation/projects/bridging/_base_metrics</li> <li>/evaluation/projects/bridging/_preprocessing_movi_c</li> <li>/evaluation/projects/bridging/_metrics_discovery_movi</li> <li>/dataset/movi_c_image</li> </ol>"},{"location":"configs/evaluation/projects/bridging/metrics_movi_e/","title":"configs/evaluation/projects/bridging/metrics_movi_e.yaml","text":"<pre><code># @package _global_\n# Evaluate on MOVi-E\ndefaults:\n- /evaluation_config  # (1)!\n- /evaluation/projects/bridging/_base_metrics # (2)!\n- /evaluation/projects/bridging/_preprocessing_movi_e # (3)!\n- /evaluation/projects/bridging/_metrics_discovery_movi # (4)!\n- /dataset: movi_e_image  # (5)!\n- _self_\neval_batch_size: 16\nmodules:\nmasks_resized:\nsize_tensor_path: input.mask\n</code></pre> <ol> <li>/evaluation_config</li> <li>/evaluation/projects/bridging/_base_metrics</li> <li>/evaluation/projects/bridging/_preprocessing_movi_e</li> <li>/evaluation/projects/bridging/_metrics_discovery_movi</li> <li>/dataset/movi_e_image</li> </ol>"},{"location":"configs/evaluation/projects/bridging/metrics_movi_e_sa/","title":"configs/evaluation/projects/bridging/metrics_movi_e_sa.yaml","text":"<pre><code># @package _global_\n# Evaluate on MOVi-E with slot attention\ndefaults:\n- /evaluation_config  # (1)!\n- /evaluation/projects/bridging/_base_metrics # (2)!\n- /evaluation/projects/bridging/_preprocessing_movi_e # (3)!\n- /evaluation/projects/bridging/_metrics_discovery_movi # (4)!\n- /dataset: movi_e_image  # (5)!\n- _self_\neval_batch_size: 16\nmodules:\nmasks_resized:\nsize_tensor_path: input.mask\npatch_mode: false\nplugins:\n03_preprocessing:\nevaluation_transforms:\nimage:\n_target_: torchvision.transforms.Compose\ntransforms:\n- _target_: torchvision.transforms.ToTensor\n- _target_: torchvision.transforms.Normalize\nmean: [0.5, 0.5, 0.5]\nstd: [0.5, 0.5, 0.5]\n</code></pre> <ol> <li>/evaluation_config</li> <li>/evaluation/projects/bridging/_base_metrics</li> <li>/evaluation/projects/bridging/_preprocessing_movi_e</li> <li>/evaluation/projects/bridging/_metrics_discovery_movi</li> <li>/dataset/movi_e_image</li> </ol>"},{"location":"configs/evaluation/projects/bridging/metrics_movi_random/","title":"configs/evaluation/projects/bridging/metrics_movi_random.yaml","text":"<pre><code># @package _global_\n# Evaluate random masks on MOVi (applied to slot attention model).\ndefaults:\n- /evaluation_config  # (1)!\n- /evaluation/projects/bridging/_base_metrics # (2)!\n- /evaluation/projects/bridging/_preprocessing_movi_c # (3)!\n- /evaluation/projects/bridging/_metrics_discovery_movi # (4)!\n- /dataset: movi_c_image  # (5)!\n- _self_\neval_batch_size: 16\nmodules:\nmasks_resized:\nsize_tensor_path: input.mask\npatch_mode: false\nrandom_masks:\n_target_: routed.ocl.utils.masking.CreateRandomMaskPatterns\nmasks_path: masks_resized\npattern: blocks\nn_slots: 11\nn_cols: 3\nevaluation_metrics:\nari:\nprediction_path: random_masks\ntarget_path: input.mask\nabo:\nprediction_path: random_masks\ntarget_path: input.mask\nplugins:\n03_preprocessing:\nevaluation_transforms:\nimage:\n_target_: torchvision.transforms.Compose\ntransforms:\n- _target_: torchvision.transforms.ToTensor\n- _target_: torchvision.transforms.Normalize\nmean: [0.5, 0.5, 0.5]\nstd: [0.5, 0.5, 0.5]\n</code></pre> <ol> <li>/evaluation_config</li> <li>/evaluation/projects/bridging/_base_metrics</li> <li>/evaluation/projects/bridging/_preprocessing_movi_c</li> <li>/evaluation/projects/bridging/_metrics_discovery_movi</li> <li>/dataset/movi_c_image</li> </ol>"},{"location":"configs/evaluation/projects/bridging/metrics_voc2012_bboxes/","title":"configs/evaluation/projects/bridging/metrics_voc2012_bboxes.yaml","text":"<pre><code># @package _global_\n# Evaluating bounding box quality on PASCAL VOC 2012 trainval, commonly used in the unsupervised setting.\ndefaults:\n- /evaluation_config  # (1)!\n- /evaluation/projects/bridging/_base_metrics # (2)!\n- /evaluation/projects/bridging/_preprocessing_voc2012 # (3)!\n- /evaluation/projects/bridging/_metrics_discovery_bboxes # (4)!\n- /dataset: voc2012_trainval  # (5)!\n- _self_\neval_batch_size: 1\nmodules:\nmasks_resized:\nsize_tensor_path: input.image_size\n</code></pre> <ol> <li>/evaluation_config</li> <li>/evaluation/projects/bridging/_base_metrics</li> <li>/evaluation/projects/bridging/_preprocessing_voc2012</li> <li>/evaluation/projects/bridging/_metrics_discovery_bboxes</li> <li>/dataset/voc2012_trainval</li> </ol>"},{"location":"configs/evaluation/projects/bridging/metrics_voc2012_masks/","title":"configs/evaluation/projects/bridging/metrics_voc2012_masks.yaml","text":"<pre><code># @package _global_\n# Evaluating mask quality on PASCAL VOC 2012 trainaug validation at original image resolution.\ndefaults:\n- /evaluation_config  # (1)!\n- /evaluation/projects/bridging/_base_metrics # (2)!\n- /evaluation/projects/bridging/_preprocessing_voc2012_trainaug # (3)!\n- /evaluation/projects/bridging/_metrics_discovery_masks # (4)!\n- /evaluation/projects/bridging/_metrics_segmentation # (5)!\n- /evaluation/projects/bridging/_metrics_add_ignore_mask # (6)!\n- /dataset: voc2012_trainaug  # (7)!\n- _self_\neval_batch_size: 1\n</code></pre> <ol> <li>/evaluation_config</li> <li>/evaluation/projects/bridging/_base_metrics</li> <li>/evaluation/projects/bridging/_preprocessing_voc2012_trainaug</li> <li>/evaluation/projects/bridging/_metrics_discovery_masks</li> <li>/evaluation/projects/bridging/_metrics_segmentation</li> <li>/evaluation/projects/bridging/_metrics_add_ignore_mask</li> <li>/dataset/voc2012_trainaug</li> </ol>"},{"location":"configs/evaluation/projects/bridging/metrics_voc2012_masks_ccrop/","title":"configs/evaluation/projects/bridging/metrics_voc2012_masks_ccrop.yaml","text":"<pre><code># @package _global_\n# Evaluating center crop mask quality on PASCAL VOC 2012 trainaug validation\ndefaults:\n- /evaluation_config  # (1)!\n- /evaluation/projects/bridging/_base_metrics # (2)!\n- /evaluation/projects/bridging/_preprocessing_voc2012_trainaug # (3)!\n- /evaluation/projects/bridging/_preprocessing_to_ccrop_image # (4)!\n- /evaluation/projects/bridging/_preprocessing_to_ccrop_320_masks # (5)!\n- /evaluation/projects/bridging/_metrics_discovery_masks # (6)!\n- /evaluation/projects/bridging/_metrics_segmentation # (7)!\n- /evaluation/projects/bridging/_metrics_add_ignore_mask # (8)!\n- /dataset: voc2012_trainaug  # (9)!\n- _self_\neval_batch_size: 16\nplugins:\n03b_preprocessing:\nevaluation_transforms:\nignore_mask:\n_target_: torchvision.transforms.Compose\ntransforms:\n- _target_: ocl.preprocessing.DenseMaskToTensor\n- _target_: ocl.preprocessing.ResizeNearestExact\nsize: 320\n- _target_: torchvision.transforms.CenterCrop\nsize: 320\n</code></pre> <ol> <li>/evaluation_config</li> <li>/evaluation/projects/bridging/_base_metrics</li> <li>/evaluation/projects/bridging/_preprocessing_voc2012_trainaug</li> <li>/evaluation/projects/bridging/_preprocessing_to_ccrop_image</li> <li>/evaluation/projects/bridging/_preprocessing_to_ccrop_320_masks</li> <li>/evaluation/projects/bridging/_metrics_discovery_masks</li> <li>/evaluation/projects/bridging/_metrics_segmentation</li> <li>/evaluation/projects/bridging/_metrics_add_ignore_mask</li> <li>/dataset/voc2012_trainaug</li> </ol>"},{"location":"configs/evaluation/projects/bridging/metrics_voc2012_masks_ccrop_random/","title":"configs/evaluation/projects/bridging/metrics_voc2012_masks_ccrop_random.yaml","text":"<pre><code># @package _global_\n# Evaluating center crop mask quality on PASCAL VOC 2012 trainaug validation\ndefaults:\n- /evaluation_config  # (1)!\n- /evaluation/projects/bridging/_base_metrics # (2)!\n- /evaluation/projects/bridging/_preprocessing_voc2012_trainaug # (3)!\n- /evaluation/projects/bridging/_preprocessing_to_ccrop_image # (4)!\n- /evaluation/projects/bridging/_preprocessing_to_ccrop_320_masks # (5)!\n- /evaluation/projects/bridging/_metrics_discovery_masks # (6)!\n- /evaluation/projects/bridging/_metrics_segmentation # (7)!\n- /evaluation/projects/bridging/_metrics_add_ignore_mask # (8)!\n- /dataset: voc2012_trainaug  # (9)!\n- _self_\neval_batch_size: 16\nmodules:\nrandom_masks:\n_target_: routed.ocl.utils.masking.CreateRandomMaskPatterns\nmasks_path: masks_resized\npattern: blocks\nn_slots: 11\nn_cols: 3\nevaluation_metrics:\ninstance_mask_ari:\nprediction_path: random_masks\ninstance_mask_unsup_iou:\nprediction_path: random_masks\ninstance_mask_abo:\nprediction_path: random_masks\ninstance_mask_corloc:\nprediction_path: random_masks\ninstance_mask_recall:\nprediction_path: random_masks\nsegmentation_mask_ari:\nprediction_path: random_masks\nsegmentation_mask_unsup_iou:\nprediction_path: random_masks\nsegmentation_mask_abo:\nprediction_path: random_masks\nplugins:\n03b_preprocessing:\nevaluation_transforms:\nignore_mask:\n_target_: torchvision.transforms.Compose\ntransforms:\n- _target_: ocl.preprocessing.DenseMaskToTensor\n- _target_: ocl.preprocessing.ResizeNearestExact\nsize: 320\n- _target_: torchvision.transforms.CenterCrop\nsize: 320\n</code></pre> <ol> <li>/evaluation_config</li> <li>/evaluation/projects/bridging/_base_metrics</li> <li>/evaluation/projects/bridging/_preprocessing_voc2012_trainaug</li> <li>/evaluation/projects/bridging/_preprocessing_to_ccrop_image</li> <li>/evaluation/projects/bridging/_preprocessing_to_ccrop_320_masks</li> <li>/evaluation/projects/bridging/_metrics_discovery_masks</li> <li>/evaluation/projects/bridging/_metrics_segmentation</li> <li>/evaluation/projects/bridging/_metrics_add_ignore_mask</li> <li>/dataset/voc2012_trainaug</li> </ol>"},{"location":"configs/evaluation/projects/bridging/metrics_voc2012_masks_ccrop_sa/","title":"configs/evaluation/projects/bridging/metrics_voc2012_masks_ccrop_sa.yaml","text":"<pre><code># @package _global_\n# Evaluating center crop mask quality on PASCAL VOC 2012 trainaug validation\ndefaults:\n- /evaluation_config  # (1)!\n- /evaluation/projects/bridging/_base_metrics # (2)!\n- /evaluation/projects/bridging/_preprocessing_voc2012_trainaug # (3)!\n- /evaluation/projects/bridging/_preprocessing_to_ccrop_320_masks # (4)!\n- /evaluation/projects/bridging/_metrics_discovery_masks # (5)!\n- /evaluation/projects/bridging/_metrics_segmentation # (6)!\n- /evaluation/projects/bridging/_metrics_add_ignore_mask # (7)!\n- /dataset: voc2012_trainaug  # (8)!\n- _self_\neval_batch_size: 16\nmodules:\nmasks_resized:\npatch_mode: false\nplugins:\n03b_preprocessing:\nevaluation_transforms:\nimage:\n_target_: torchvision.transforms.Compose\ntransforms:\n- _target_: torchvision.transforms.ToTensor\n- _target_: torchvision.transforms.Resize\nsize: 128\ninterpolation: ${torchvision_interpolation_mode:BILINEAR}\n- _target_: torchvision.transforms.CenterCrop\nsize: 128\n- _target_: torchvision.transforms.Normalize\nmean: [0.5, 0.5, 0.5]\nstd: [0.5, 0.5, 0.5]\nignore_mask:\n_target_: torchvision.transforms.Compose\ntransforms:\n- _target_: ocl.preprocessing.DenseMaskToTensor\n- _target_: ocl.preprocessing.ResizeNearestExact\nsize: 320\n- _target_: torchvision.transforms.CenterCrop\nsize: 320\n</code></pre> <ol> <li>/evaluation_config</li> <li>/evaluation/projects/bridging/_base_metrics</li> <li>/evaluation/projects/bridging/_preprocessing_voc2012_trainaug</li> <li>/evaluation/projects/bridging/_preprocessing_to_ccrop_320_masks</li> <li>/evaluation/projects/bridging/_metrics_discovery_masks</li> <li>/evaluation/projects/bridging/_metrics_segmentation</li> <li>/evaluation/projects/bridging/_metrics_add_ignore_mask</li> <li>/dataset/voc2012_trainaug</li> </ol>"},{"location":"configs/evaluation/projects/bridging/outputs_coco_ccrop/","title":"configs/evaluation/projects/bridging/outputs_coco_ccrop.yaml","text":"<pre><code># @package _global_\n# Save model predictions on COCO\ndataset:\neval_transforms:\n02a_preprocessing:\n_target_: ocl.transforms.Map\ntransform: \"${lambda_fn:'lambda data: {\\\"orig_image\\\": data[\\\"image\\\"], **data}'}\"\nfields:\n- image\nbatch_transform: false\ndefaults:\n- /evaluation_config  # (1)!\n- /evaluation/projects/bridging/_base_metrics # (2)!\n- /evaluation/projects/bridging/_preprocessing_coco # (3)!\n- /dataset: coco  # (4)!\n- _self_\neval_batch_size: 1\nsave_outputs: true\nskip_metrics: true\nn_samples_to_store: 500\noutputs_to_store:\n- input.orig_image\n- input.instance_mask\n- masks_resized\nplugins:\n03b_preprocessing:\nevaluation_transforms:\norig_image:\n_target_: torchvision.transforms.Compose\ntransforms:\n- _target_: torchvision.transforms.ToTensor\n- _target_: ocl.preprocessing.OrigCenterCrop\nimage:\n_target_: torchvision.transforms.Compose\ntransforms:\n- _target_: torchvision.transforms.ToTensor\n- _target_: torchvision.transforms.Resize\nsize: 224\ninterpolation: ${torchvision_interpolation_mode:BICUBIC}\n- \"${lambda_fn:'lambda image: image.clamp(0.0, 1.0)'}\"\n- _target_: torchvision.transforms.CenterCrop\nsize: 224\n- _target_: torchvision.transforms.Normalize\nmean: [0.485, 0.456, 0.406]\nstd: [0.229, 0.224, 0.225]\ninstance_mask:\n_target_: torchvision.transforms.Compose\ntransforms:\n- _target_: ocl.preprocessing.DenseMaskToTensor\n- _target_: ocl.preprocessing.OrigCenterCrop\nsegmentation_mask:\n_target_: torchvision.transforms.Compose\ntransforms:\n- _target_: ocl.preprocessing.DenseMaskToTensor\n- _target_: ocl.preprocessing.OrigCenterCrop\n</code></pre> <ol> <li>/evaluation_config</li> <li>/evaluation/projects/bridging/_base_metrics</li> <li>/evaluation/projects/bridging/_preprocessing_coco</li> <li>/dataset/coco</li> </ol>"},{"location":"configs/evaluation/projects/bridging/outputs_coco_ccrop_sa/","title":"configs/evaluation/projects/bridging/outputs_coco_ccrop_sa.yaml","text":"<pre><code># @package _global_\n# Save model predictions on COCO with slot attention models\ndataset:\neval_transforms:\n02a_preprocessing:\n_target_: ocl.transforms.Map\ntransform: \"${lambda_fn:'lambda data: {\\\"orig_image\\\": data[\\\"image\\\"], **data}'}\"\nfields:\n- image\nbatch_transform: false\ndefaults:\n- /evaluation_config  # (1)!\n- /evaluation/projects/bridging/_base_metrics # (2)!\n- /evaluation/projects/bridging/_preprocessing_coco # (3)!\n- /dataset: coco  # (4)!\n- _self_\neval_batch_size: 1\nsave_outputs: true\nskip_metrics: true\nn_samples_to_store: 500\noutputs_to_store:\n- input.orig_image\n- input.instance_mask\n- masks_resized\n- object_decoder.reconstruction\nmodules:\nmasks_resized:\npatch_mode: false\nplugins:\n03b_preprocessing:\nevaluation_transforms:\norig_image:\n_target_: torchvision.transforms.Compose\ntransforms:\n- _target_: torchvision.transforms.ToTensor\n- _target_: ocl.preprocessing.OrigCenterCrop\nimage:\n_target_: torchvision.transforms.Compose\ntransforms:\n- _target_: torchvision.transforms.ToTensor\n- _target_: torchvision.transforms.Resize\nsize: 128\ninterpolation: ${torchvision_interpolation_mode:BILINEAR}\n- _target_: torchvision.transforms.CenterCrop\nsize: 128\n- _target_: torchvision.transforms.Normalize\nmean: [0.5, 0.5, 0.5]\nstd: [0.5, 0.5, 0.5]\ninstance_mask:\n_target_: torchvision.transforms.Compose\ntransforms:\n- _target_: ocl.preprocessing.DenseMaskToTensor\n- _target_: ocl.preprocessing.OrigCenterCrop\nsegmentation_mask:\n_target_: torchvision.transforms.Compose\ntransforms:\n- _target_: ocl.preprocessing.DenseMaskToTensor\n- _target_: ocl.preprocessing.OrigCenterCrop\n</code></pre> <ol> <li>/evaluation_config</li> <li>/evaluation/projects/bridging/_base_metrics</li> <li>/evaluation/projects/bridging/_preprocessing_coco</li> <li>/dataset/coco</li> </ol>"},{"location":"configs/evaluation/projects/bridging/outputs_coco_ccrop_slots/","title":"configs/evaluation/projects/bridging/outputs_coco_ccrop_slots.yaml","text":"<pre><code># @package _global_\n# Save model predictions on COCO\ndefaults:\n- /evaluation_config  # (1)!\n- /evaluation/projects/bridging/_base_metrics # (2)!\n- /evaluation/projects/bridging/_preprocessing_coco # (3)!\n- /dataset: coco  # (4)!\n- _self_\ndataset:\nshuffle_train: false\ntrain_transforms:\n02a_preprocessing:\n_target_: ocl.transforms.Map\ntransform: \"${lambda_fn:'lambda data: {\\\"orig_image\\\": data[\\\"image\\\"], **data}'}\"\nfields:\n- image\nbatch_transform: false\neval_transforms:\n02a_preprocessing:\n_target_: ocl.transforms.Map\ntransform: \"${lambda_fn:'lambda data: {\\\"orig_image\\\": data[\\\"image\\\"], **data}'}\"\nfields:\n- image\nbatch_transform: false\neval_batch_size: 1\neval_train: true\nsave_outputs: true\nskip_metrics: true\nn_samples_to_store: 11000\noutputs_dirname: slots\noutputs_to_store:\n- perceptual_grouping.objects\n- input.instance_mask\n# - input.orig_image\n# - masks_resized\nplugins:\n03a_preprocessing:\ntraining_fields:\n- image\n- instance_mask\n- instance_category\ntraining_transform:\n_target_: torchvision.transforms.Compose\ntransforms:\n- _target_: ocl.preprocessing.InstanceMasksToDenseMasks\n- _target_: ocl.preprocessing.AddSegmentationMaskFromInstanceMask\n- _target_: ocl.preprocessing.AddEmptyMasks\nmask_keys:\n- instance_mask\n- segmentation_mask\n- _target_: ocl.preprocessing.DropEntries\nkeys:\n- instance_category\nevaluation_fields:\n- image\n- instance_mask\n- instance_category\nevaluation_transform:\n_target_: torchvision.transforms.Compose\ntransforms:\n- _target_: ocl.preprocessing.InstanceMasksToDenseMasks\n- _target_: ocl.preprocessing.AddSegmentationMaskFromInstanceMask\n- _target_: ocl.preprocessing.AddEmptyMasks\nmask_keys:\n- instance_mask\n- segmentation_mask\n- _target_: ocl.preprocessing.DropEntries\nkeys:\n- instance_category\n03b_preprocessing:\ntraining_transforms:\norig_image:\n_target_: torchvision.transforms.Compose\ntransforms:\n- _target_: torchvision.transforms.ToTensor\n- _target_: ocl.preprocessing.OrigCenterCrop\nimage:\n_target_: torchvision.transforms.Compose\ntransforms:\n- _target_: torchvision.transforms.ToTensor\n- _target_: torchvision.transforms.Resize\nsize: 224\ninterpolation: ${torchvision_interpolation_mode:BICUBIC}\n- \"${lambda_fn:'lambda image: image.clamp(0.0, 1.0)'}\"\n- _target_: torchvision.transforms.CenterCrop\nsize: 224\n- _target_: torchvision.transforms.Normalize\nmean: [0.485, 0.456, 0.406]\nstd: [0.229, 0.224, 0.225]\ninstance_mask:\n_target_: torchvision.transforms.Compose\ntransforms:\n- _target_: ocl.preprocessing.DenseMaskToTensor\n- _target_: ocl.preprocessing.ResizeNearestExact\nsize: 224\n- _target_: torchvision.transforms.CenterCrop\nsize: 224\nevaluation_transforms:\norig_image:\n_target_: torchvision.transforms.Compose\ntransforms:\n- _target_: torchvision.transforms.ToTensor\n- _target_: ocl.preprocessing.OrigCenterCrop\nimage:\n_target_: torchvision.transforms.Compose\ntransforms:\n- _target_: torchvision.transforms.ToTensor\n- _target_: torchvision.transforms.Resize\nsize: 224\ninterpolation: ${torchvision_interpolation_mode:BICUBIC}\n- \"${lambda_fn:'lambda image: image.clamp(0.0, 1.0)'}\"\n- _target_: torchvision.transforms.CenterCrop\nsize: 224\n- _target_: torchvision.transforms.Normalize\nmean: [0.485, 0.456, 0.406]\nstd: [0.229, 0.224, 0.225]\ninstance_mask:\n_target_: torchvision.transforms.Compose\ntransforms:\n- _target_: ocl.preprocessing.DenseMaskToTensor\n- _target_: ocl.preprocessing.ResizeNearestExact\nsize: 224\n- _target_: torchvision.transforms.CenterCrop\nsize: 224\n</code></pre> <ol> <li>/evaluation_config</li> <li>/evaluation/projects/bridging/_base_metrics</li> <li>/evaluation/projects/bridging/_preprocessing_coco</li> <li>/dataset/coco</li> </ol>"},{"location":"configs/evaluation/projects/bridging/outputs_coco_ccrop_slots_sa/","title":"configs/evaluation/projects/bridging/outputs_coco_ccrop_slots_sa.yaml","text":"<pre><code># @package _global_\n# Save model predictions on COCO with slot attention models\ndefaults:\n- /evaluation_config  # (1)!\n- /evaluation/projects/bridging/_base_metrics # (2)!\n- /evaluation/projects/bridging/_preprocessing_coco # (3)!\n- /dataset: coco  # (4)!\n- _self_\ndataset:\nshuffle_train: false\ntrain_transforms:\n02a_preprocessing:\n_target_: ocl.transforms.Map\ntransform: \"${lambda_fn:'lambda data: {\\\"orig_image\\\": data[\\\"image\\\"], **data}'}\"\nfields:\n- image\nbatch_transform: false\neval_transforms:\n02a_preprocessing:\n_target_: ocl.transforms.Map\ntransform: \"${lambda_fn:'lambda data: {\\\"orig_image\\\": data[\\\"image\\\"], **data}'}\"\nfields:\n- image\nbatch_transform: false\neval_batch_size: 1\neval_train: true\nsave_outputs: true\nskip_metrics: true\nn_samples_to_store: 11000\noutputs_dirname: slots\noutputs_to_store:\n# - input.orig_image\n- perceptual_grouping.objects\n- input.instance_mask\nmodules:\nmasks_resized:\npatch_mode: false\nplugins:\n03a_preprocessing:\ntraining_fields:\n- image\n- instance_mask\n- instance_category\ntraining_transform:\n_target_: torchvision.transforms.Compose\ntransforms:\n- _target_: ocl.preprocessing.InstanceMasksToDenseMasks\n- _target_: ocl.preprocessing.AddSegmentationMaskFromInstanceMask\n- _target_: ocl.preprocessing.AddEmptyMasks\nmask_keys:\n- instance_mask\n- segmentation_mask\n- _target_: ocl.preprocessing.DropEntries\nkeys:\n- instance_category\nevaluation_fields:\n- image\n- instance_mask\n- instance_category\nevaluation_transform:\n_target_: torchvision.transforms.Compose\ntransforms:\n- _target_: ocl.preprocessing.InstanceMasksToDenseMasks\n- _target_: ocl.preprocessing.AddSegmentationMaskFromInstanceMask\n- _target_: ocl.preprocessing.AddEmptyMasks\nmask_keys:\n- instance_mask\n- segmentation_mask\n- _target_: ocl.preprocessing.DropEntries\nkeys:\n- instance_category\n03b_preprocessing:\ntraining_transforms:\norig_image:\n_target_: torchvision.transforms.Compose\ntransforms:\n- _target_: torchvision.transforms.ToTensor\n- _target_: ocl.preprocessing.OrigCenterCrop\nimage:\n_target_: torchvision.transforms.Compose\ntransforms:\n- _target_: torchvision.transforms.ToTensor\n- _target_: torchvision.transforms.Resize\nsize: 128\ninterpolation: ${torchvision_interpolation_mode:BILINEAR}\n- _target_: torchvision.transforms.CenterCrop\nsize: 128\n- _target_: torchvision.transforms.Normalize\nmean: [0.5, 0.5, 0.5]\nstd: [0.5, 0.5, 0.5]\ninstance_mask:\n_target_: torchvision.transforms.Compose\ntransforms:\n- _target_: ocl.preprocessing.DenseMaskToTensor\n- _target_: ocl.preprocessing.ResizeNearestExact\nsize: 224\n- _target_: torchvision.transforms.CenterCrop\nsize: 224\nevaluation_transforms:\norig_image:\n_target_: torchvision.transforms.Compose\ntransforms:\n- _target_: torchvision.transforms.ToTensor\n- _target_: ocl.preprocessing.OrigCenterCrop\nimage:\n_target_: torchvision.transforms.Compose\ntransforms:\n- _target_: torchvision.transforms.ToTensor\n- _target_: torchvision.transforms.Resize\nsize: 128\ninterpolation: ${torchvision_interpolation_mode:BILINEAR}\n- _target_: torchvision.transforms.CenterCrop\nsize: 128\n- _target_: torchvision.transforms.Normalize\nmean: [0.5, 0.5, 0.5]\nstd: [0.5, 0.5, 0.5]\ninstance_mask:\n_target_: torchvision.transforms.Compose\ntransforms:\n- _target_: ocl.preprocessing.DenseMaskToTensor\n- _target_: ocl.preprocessing.ResizeNearestExact\nsize: 224\n- _target_: torchvision.transforms.CenterCrop\nsize: 224\n</code></pre> <ol> <li>/evaluation_config</li> <li>/evaluation/projects/bridging/_base_metrics</li> <li>/evaluation/projects/bridging/_preprocessing_coco</li> <li>/dataset/coco</li> </ol>"},{"location":"configs/evaluation/projects/bridging/outputs_movi/","title":"configs/evaluation/projects/bridging/outputs_movi.yaml","text":"<pre><code># @package _global_\n# Save model predictions on MOVi datasets\ndataset:\neval_transforms:\n03_preprocessing:\n_target_: ocl.transforms.Map\ntransform: \"${lambda_fn:'lambda data: {\\\"orig_image\\\": data[\\\"image\\\"], **data}'}\"\nfields:\n- image\nbatch_transform: false\n04_preprocessing:\n_target_: ocl.transforms.SimpleTransform\ntransforms:\norig_image:\n_target_: torchvision.transforms.Compose\ntransforms:\n- _target_: torchvision.transforms.ToTensor\nimage:\n_target_: torchvision.transforms.Compose\ntransforms:\n- _target_: torchvision.transforms.ToTensor\n- _target_: torchvision.transforms.Resize\nsize: 224\ninterpolation: ${torchvision_interpolation_mode:BICUBIC}\n- \"${lambda_fn:'lambda image: image.clamp(0.0, 1.0)'}\" # Bicubic interpolation can get out of range\n- _target_: torchvision.transforms.Normalize\nmean: [0.485, 0.456, 0.406]\nstd: [0.229, 0.224, 0.225]\nmask:\n_target_: torchvision.transforms.Compose\ntransforms:\n- _target_: ocl.preprocessing.MultiMaskToTensor\n- _target_: ocl.preprocessing.ResizeNearestExact\nsize: 128\nbatch_transform: false\ndefaults:\n- /evaluation_config  # (1)!\n- /evaluation/projects/bridging/_base_metrics # (2)!\n- /dataset: movi_c_image  # (3)!\n- _self_\neval_batch_size: 1\nsave_outputs: true\nskip_metrics: true\nn_samples_to_store: 500\noutputs_to_store:\n- input.orig_image\n- input.mask\n- masks_resized\nmodules:\nmasks_resized:\nsize_tensor_path: input.mask\n</code></pre> <ol> <li>/evaluation_config</li> <li>/evaluation/projects/bridging/_base_metrics</li> <li>/dataset/movi_c_image</li> </ol>"},{"location":"configs/evaluation/projects/bridging/outputs_movi_e/","title":"configs/evaluation/projects/bridging/outputs_movi_e.yaml","text":"<pre><code># @package _global_\n# Save model predictions on MOVi datasets\ndataset:\neval_transforms:\n03_preprocessing:\n_target_: ocl.transforms.Map\ntransform: \"${lambda_fn:'lambda data: {\\\"orig_image\\\": data[\\\"image\\\"], **data}'}\"\nfields:\n- image\nbatch_transform: false\n04_preprocessing:\n_target_: ocl.transforms.SimpleTransform\ntransforms:\norig_image:\n_target_: torchvision.transforms.Compose\ntransforms:\n- _target_: torchvision.transforms.ToTensor\nimage:\n_target_: torchvision.transforms.Compose\ntransforms:\n- _target_: torchvision.transforms.ToTensor\n- _target_: torchvision.transforms.Resize\nsize: 224\ninterpolation: ${torchvision_interpolation_mode:BICUBIC}\n- \"${lambda_fn:'lambda image: image.clamp(0.0, 1.0)'}\" # Bicubic interpolation can get out of range\n- _target_: torchvision.transforms.Normalize\nmean: [0.485, 0.456, 0.406]\nstd: [0.229, 0.224, 0.225]\nmask:\n_target_: torchvision.transforms.Compose\ntransforms:\n- _target_: ocl.preprocessing.MultiMaskToTensor\n- _target_: ocl.preprocessing.ResizeNearestExact\nsize: 128\nbatch_transform: false\ndefaults:\n- /evaluation_config  # (1)!\n- /evaluation/projects/bridging/_base_metrics # (2)!\n- /dataset: movi_e_image  # (3)!\n- _self_\neval_batch_size: 1\nsave_outputs: true\nskip_metrics: true\nn_samples_to_store: 100\noutputs_to_store:\n- input.orig_image\n- input.mask\n- masks_resized\nmodules:\nmasks_resized:\nsize_tensor_path: input.mask\n</code></pre> <ol> <li>/evaluation_config</li> <li>/evaluation/projects/bridging/_base_metrics</li> <li>/dataset/movi_e_image</li> </ol>"},{"location":"configs/evaluation/projects/bridging/outputs_movi_e_sa/","title":"configs/evaluation/projects/bridging/outputs_movi_e_sa.yaml","text":"<pre><code># @package _global_\n# Save model predictions on MOVi datasets with slot attention models\ndataset:\neval_transforms:\n03_preprocessing:\n_target_: ocl.transforms.Map\ntransform: \"${lambda_fn:'lambda data: {\\\"orig_image\\\": data[\\\"image\\\"], **data}'}\"\nfields:\n- image\nbatch_transform: false\n04_preprocessing:\n_target_: ocl.transforms.SimpleTransform\ntransforms:\norig_image:\n_target_: torchvision.transforms.Compose\ntransforms:\n- _target_: torchvision.transforms.ToTensor\nimage:\n_target_: torchvision.transforms.Compose\ntransforms:\n- _target_: torchvision.transforms.ToTensor\n- _target_: torchvision.transforms.Resize\nsize: 128\ninterpolation: ${torchvision_interpolation_mode:BILINEAR}\n- _target_: torchvision.transforms.CenterCrop\nsize: 128\n- _target_: torchvision.transforms.Normalize\nmean: [0.5, 0.5, 0.5]\nstd: [0.5, 0.5, 0.5]\nmask:\n_target_: torchvision.transforms.Compose\ntransforms:\n- _target_: ocl.preprocessing.MultiMaskToTensor\n- _target_: ocl.preprocessing.ResizeNearestExact\nsize: 128\nbatch_transform: false\ndefaults:\n- /evaluation_config  # (1)!\n- /evaluation/projects/bridging/_base_metrics # (2)!\n- /dataset: movi_e_image  # (3)!\n- _self_\neval_batch_size: 1\nsave_outputs: true\nskip_metrics: true\nn_samples_to_store:\noutputs_to_store:\n- input.orig_image\n- input.mask\n- masks_resized\nmodules:\nmasks_resized:\nsize_tensor_path: input.mask\npatch_mode: false\n</code></pre> <ol> <li>/evaluation_config</li> <li>/evaluation/projects/bridging/_base_metrics</li> <li>/dataset/movi_e_image</li> </ol>"},{"location":"configs/evaluation/projects/bridging/outputs_movi_sa/","title":"configs/evaluation/projects/bridging/outputs_movi_sa.yaml","text":"<pre><code># @package _global_\n# Save model predictions on MOVi datasets with slot attention models\ndataset:\neval_transforms:\n03_preprocessing:\n_target_: ocl.transforms.Map\ntransform: \"${lambda_fn:'lambda data: {\\\"orig_image\\\": data[\\\"image\\\"], **data}'}\"\nfields:\n- image\nbatch_transform: false\n04_preprocessing:\n_target_: ocl.transforms.SimpleTransform\ntransforms:\norig_image:\n_target_: torchvision.transforms.Compose\ntransforms:\n- _target_: torchvision.transforms.ToTensor\nimage:\n_target_: torchvision.transforms.Compose\ntransforms:\n- _target_: torchvision.transforms.ToTensor\n- _target_: torchvision.transforms.Resize\nsize: 128\ninterpolation: ${torchvision_interpolation_mode:BILINEAR}\n- _target_: torchvision.transforms.CenterCrop\nsize: 128\n- _target_: torchvision.transforms.Normalize\nmean: [0.5, 0.5, 0.5]\nstd: [0.5, 0.5, 0.5]\nmask:\n_target_: torchvision.transforms.Compose\ntransforms:\n- _target_: ocl.preprocessing.MultiMaskToTensor\n- _target_: ocl.preprocessing.ResizeNearestExact\nsize: 128\nbatch_transform: false\ndefaults:\n- /evaluation_config  # (1)!\n- /evaluation/projects/bridging/_base_metrics # (2)!\n- /dataset: movi_c_image  # (3)!\n- _self_\neval_batch_size: 1\nsave_outputs: true\nskip_metrics: true\nn_samples_to_store: 100\noutputs_to_store:\n- input.orig_image\n- input.mask\n- masks_resized\nmodules:\nmasks_resized:\nsize_tensor_path: input.mask\npatch_mode: false\n</code></pre> <ol> <li>/evaluation_config</li> <li>/evaluation/projects/bridging/_base_metrics</li> <li>/dataset/movi_c_image</li> </ol>"},{"location":"configs/evaluation/projects/bridging/outputs_voc2012_bbox/","title":"configs/evaluation/projects/bridging/outputs_voc2012_bbox.yaml","text":"<pre><code># @package _global_\n# Save model predictions on VOC 2012 trainval\ndataset:\neval_transforms:\n02b_preprocessing:\n_target_: ocl.transforms.Map\ntransform: \"${lambda_fn:'lambda data: {\\\"orig_image\\\": data[\\\"image\\\"], **data}'}\"\nfields:\n- image\nbatch_transform: false\ndefaults:\n- /evaluation_config  # (1)!\n- /evaluation/projects/bridging/_base_metrics # (2)!\n- /evaluation/projects/bridging/_preprocessing_voc2012 # (3)!\n- /evaluation/projects/bridging/_metrics_discovery_bboxes # (4)!\n- /dataset: voc2012_trainval  # (5)!\n- _self_\neval_batch_size: 1\nsave_outputs: true\nskip_metrics: true\nn_samples_to_store: 350\noutputs_to_store:\n- input.orig_image\n- input.instance_bbox\n- masks_resized\nmodules:\nmasks_resized:\nsize_tensor_path: input.orig_image\nplugins:\n03b_preprocessing:\nevaluation_transforms:\norig_image:\n_target_: torchvision.transforms.Compose\ntransforms:\n- _target_: torchvision.transforms.ToTensor\nimage:\n_target_: torchvision.transforms.Compose\ntransforms:\n- _target_: torchvision.transforms.ToTensor\n- _target_: torchvision.transforms.Resize\nsize: 224\ninterpolation: ${torchvision_interpolation_mode:BICUBIC}\n- \"${lambda_fn:'lambda image: image.clamp(0.0, 1.0)'}\"\n- _target_: torchvision.transforms.CenterCrop\nsize: 224\n- _target_: torchvision.transforms.Normalize\nmean: [0.485, 0.456, 0.406]\nstd: [0.229, 0.224, 0.225]\n</code></pre> <ol> <li>/evaluation_config</li> <li>/evaluation/projects/bridging/_base_metrics</li> <li>/evaluation/projects/bridging/_preprocessing_voc2012</li> <li>/evaluation/projects/bridging/_metrics_discovery_bboxes</li> <li>/dataset/voc2012_trainval</li> </ol>"},{"location":"configs/evaluation/projects/bridging/outputs_voc2012_ccrop/","title":"configs/evaluation/projects/bridging/outputs_voc2012_ccrop.yaml","text":"<pre><code># @package _global_\n# Save model predictions on VOC 2012 trainval\ndataset:\neval_transforms:\n02c_preprocessing:\n_target_: ocl.transforms.Map\ntransform: \"${lambda_fn:'lambda data: {\\\"orig_image\\\": data[\\\"image\\\"], **data}'}\"\nfields:\n- image\nbatch_transform: false\ndefaults:\n- /evaluation_config  # (1)!\n- /evaluation/projects/bridging/_base_metrics # (2)!\n- /evaluation/projects/bridging/_preprocessing_voc2012_trainaug # (3)!\n- /evaluation/projects/bridging/_metrics_discovery_bboxes # (4)!\n- /evaluation/projects/bridging/_metrics_segmentation # (5)!\n- /dataset: voc2012_trainaug  # (6)!\n- _self_\neval_batch_size: 1\nsave_outputs: true\nskip_metrics: true\nn_samples_to_store: 350\noutputs_to_store:\n- input.orig_image\n- input.instance_mask\n- masks_resized\nplugins:\n03b_preprocessing:\nevaluation_transforms:\norig_image:\n_target_: torchvision.transforms.Compose\ntransforms:\n- _target_: torchvision.transforms.ToTensor\n- _target_: ocl.preprocessing.OrigCenterCrop\nimage:\n_target_: torchvision.transforms.Compose\ntransforms:\n- _target_: torchvision.transforms.ToTensor\n- _target_: torchvision.transforms.Resize\nsize: 224\ninterpolation: ${torchvision_interpolation_mode:BICUBIC}\n- \"${lambda_fn:'lambda image: image.clamp(0.0, 1.0)'}\"\n- _target_: torchvision.transforms.CenterCrop\nsize: 224\n- _target_: torchvision.transforms.Normalize\nmean: [0.485, 0.456, 0.406]\nstd: [0.229, 0.224, 0.225]\ninstance_mask:\n_target_: torchvision.transforms.Compose\ntransforms:\n- _target_: ocl.preprocessing.DenseMaskToTensor\n- _target_: ocl.preprocessing.OrigCenterCrop\nsegmentation_mask:\n_target_: torchvision.transforms.Compose\ntransforms:\n- _target_: ocl.preprocessing.DenseMaskToTensor\n- _target_: ocl.preprocessing.OrigCenterCrop\n</code></pre> <ol> <li>/evaluation_config</li> <li>/evaluation/projects/bridging/_base_metrics</li> <li>/evaluation/projects/bridging/_preprocessing_voc2012_trainaug</li> <li>/evaluation/projects/bridging/_metrics_discovery_bboxes</li> <li>/evaluation/projects/bridging/_metrics_segmentation</li> <li>/dataset/voc2012_trainaug</li> </ol>"},{"location":"configs/evaluation/slate/_base_metrics/","title":"configs/evaluation/slate/_base_metrics.yaml","text":"<pre><code># @package _global_\nhydra:\nrun:\ndir: \"${eval_lambda:'lambda a, b: a if a is not None else b',${output_dir},./outputs/evaluation/metrics/${now:%Y-%m-%d_%H-%M-%S}}\"\nmodules:\nmasks_resized:\n_target_: routed.ocl.utils.resizing.Resize\ninput_path: object_decoder.masks\nsize: 128\nresize_mode: bilinear\npatch_mode: true\n</code></pre>"},{"location":"configs/evaluation/slate/_metrics_discovery_masks/","title":"configs/evaluation/slate/_metrics_discovery_masks.yaml","text":"<pre><code># @package _global_\n# Metrics for object discovery with instance masks.\nevaluation_metrics:\nari:\n_target_: routed.ocl.metrics.ARIMetric\nprediction_path: masks_resized\ntarget_path: input.mask\nabo:\n_target_: routed.ocl.metrics.AverageBestOverlapMetric\nprediction_path: masks_resized\ntarget_path: input.mask\nignore_background: true\n</code></pre>"},{"location":"configs/evaluation/slate/_metrics_masks/","title":"configs/evaluation/slate/_metrics_masks.yaml","text":"<pre><code># @package _global_\nevaluation_metrics:\ninstance_mask_ari:\n_target_: routed.ocl.metrics.ARIMetric\nprediction_path: object_decoder.masks_as_image\ntarget_path: input.instance_mask\nforeground: false\nconvert_target_one_hot: true\ninstance_mask_iou:\n_target_: routed.ocl.metrics.UnsupervisedMaskIoUMetric\nprediction_path: object_decoder.masks_as_image\ntarget_path: input.instance_mask\nignore_overlaps: true\nsegmentation_mask_iou:\n_target_: routed.ocl.metrics.UnsupervisedMaskIoUMetric\nprediction_path: object_decoder.masks_as_image\ntarget_path: input.segmentation_mask\nignore_overlaps: true\ninstance_mask_abo:\n_target_: routed.ocl.metrics.AverageBestOverlapMetric\nprediction_path: object_decoder.masks_as_image\ntarget_path: input.instance_mask\nignore_overlaps: true\nsegmentation_mask_abo:\n_target_: routed.ocl.metrics.AverageBestOverlapMetric\nprediction_path: object_decoder.masks_as_image\ntarget_path: input.segmentation_mask\nignore_overlaps: true\ninstance_mask_corloc:\n_target_: routed.ocl.metrics.MaskCorLocMetric\nprediction_path: object_decoder.masks_as_image\ntarget_path: input.instance_mask\nignore_overlaps: true\n</code></pre>"},{"location":"configs/evaluation/slate/_movi_preprocessing/","title":"configs/evaluation/slate/_movi_preprocessing.yaml","text":"<pre><code># @package _global_\ndataset:\ntrain_transforms:\n03_preprocessing:\n_target_: ocl.transforms.SimpleTransform\ntransforms:\nimage:\n_target_: torchvision.transforms.Compose\ntransforms:\n- _target_: torchvision.transforms.ToTensor\n- _target_: torchvision.transforms.Resize\nsize: 128\n- _target_: torchvision.transforms.Normalize\nmean: [0.5, 0.5, 0.5]\nstd: [0.5, 0.5, 0.5]\nbatch_transform: false\neval_transforms:\n03_preprocessing:\n_target_: ocl.transforms.SimpleTransform\ntransforms:\nimage:\n_target_: torchvision.transforms.Compose\ntransforms:\n- _target_: torchvision.transforms.ToTensor\n- _target_: torchvision.transforms.Resize\nsize: 128\n- _target_: torchvision.transforms.Normalize\nmean: [0.5, 0.5, 0.5]\nstd: [0.5, 0.5, 0.5]\nmask:\n_target_: torchvision.transforms.Compose\ntransforms:\n- _target_: ocl.preprocessing.MultiMaskToTensor\n- _target_: ocl.preprocessing.ResizeNearestExact\nsize: 128\nbatch_transform: false\n</code></pre>"},{"location":"configs/evaluation/slate/_preprocessing_coco/","title":"configs/evaluation/slate/_preprocessing_coco.yaml","text":"<pre><code># @package _global_\ndataset:\neval_transforms:\n03a_preprocessing:\n_target_: ocl.transforms.Map\ntransform:\n_target_: torchvision.transforms.Compose\ntransforms:\n- _target_: ocl.preprocessing.InstanceMasksToDenseMasks\n- _target_: ocl.preprocessing.AddSegmentationMaskFromInstanceMask\n- _target_: ocl.preprocessing.AddEmptyMasks\nmask_keys:\n- instance_mask\n- segmentation_mask\n# Drop instance_category again as some images do not contain it\n- _target_: ocl.preprocessing.DropEntries\nkeys:\n- instance_category\nfields:\n- image\n- instance_mask\n- instance_category\nbatch_transform: false\n03b_preprocessing:\n_target_: ocl.transforms.SimpleTransform\ntransforms:\nimage:\n_target_: torchvision.transforms.Compose\ntransforms:\n- _target_: torchvision.transforms.ToTensor\n- _target_: torchvision.transforms.Resize\nsize: 128\ninterpolation: ${torchvision_interpolation_mode:BILINEAR}\n- _target_: torchvision.transforms.CenterCrop\nsize: 128\n- _target_: torchvision.transforms.Normalize\nmean: [0.5, 0.5, 0.5]\nstd: [0.5, 0.5, 0.5]\ninstance_mask:\n_target_: torchvision.transforms.Compose\ntransforms:\n- _target_: ocl.preprocessing.DenseMaskToTensor\n- _target_: ocl.preprocessing.ResizeNearestExact\nsize: 128\n- _target_: torchvision.transforms.CenterCrop\nsize: 128\nsegmentation_mask:\n_target_: torchvision.transforms.Compose\ntransforms:\n- _target_: ocl.preprocessing.DenseMaskToTensor\n- _target_: ocl.preprocessing.ResizeNearestExact\nsize: 128\n- _target_: torchvision.transforms.CenterCrop\nsize: 128\nbatch_transform: false\ntrain_transforms:\n03b_preprocessing:\n_target_: ocl.transforms.SimpleTransform\ntransforms:\nimage:\n_target_: torchvision.transforms.Compose\ntransforms:\n- _target_: torchvision.transforms.ToTensor\n- _target_: torchvision.transforms.Resize\nsize: 128\ninterpolation: ${torchvision_interpolation_mode:BILINEAR}\n- _target_: torchvision.transforms.CenterCrop\nsize: 128\n- _target_: torchvision.transforms.RandomHorizontalFlip\n- _target_: torchvision.transforms.Normalize\nmean: [0.5, 0.5, 0.5]\nstd: [0.5, 0.5, 0.5]\nbatch_transform: false\n</code></pre>"},{"location":"configs/evaluation/slate/metrics_coco/","title":"configs/evaluation/slate/metrics_coco.yaml","text":"<pre><code># @package _global_\n# Config file used with eval\ndefaults:\n- /evaluation_config  # (1)!\n- /evaluation/slate/_preprocessing_coco # (2)!\n- /evaluation/slate/_metrics_masks # (3)!\n- /dataset: coco  # (4)!\n- _self_\neval_batch_size: 1\n</code></pre> <ol> <li>/evaluation_config</li> <li>/evaluation/slate/_preprocessing_coco</li> <li>/evaluation/slate/_metrics_masks</li> <li>/dataset/coco</li> </ol>"},{"location":"configs/evaluation/slate/metrics_movi_c/","title":"configs/evaluation/slate/metrics_movi_c.yaml","text":"<pre><code># @package _global_\n# Config file used with eval\ndefaults:\n- /evaluation_config  # (1)!\n- /evaluation/slate/_base_metrics # (2)!\n- /evaluation/slate/_movi_preprocessing # (3)!\n- /evaluation/slate/_metrics_discovery_masks # (4)!\n- /dataset: movi_c  # (5)!\n- _self_\neval_batch_size: 1\n</code></pre> <ol> <li>/evaluation_config</li> <li>/evaluation/slate/_base_metrics</li> <li>/evaluation/slate/_movi_preprocessing</li> <li>/evaluation/slate/_metrics_discovery_masks</li> <li>/dataset/movi_c</li> </ol>"},{"location":"configs/evaluation/slate/metrics_movi_e/","title":"configs/evaluation/slate/metrics_movi_e.yaml","text":"<pre><code># @package _global_\n# Config file used with eval\ndefaults:\n- /evaluation_config  # (1)!\n- /evaluation/slate/_base_metrics # (2)!\n- /evaluation/slate/_movi_preprocessing # (3)!\n- /evaluation/slate/_metrics_discovery_masks # (4)!\n- /dataset: movi_e  # (5)!\n- _self_\neval_batch_size: 1\n</code></pre> <ol> <li>/evaluation_config</li> <li>/evaluation/slate/_base_metrics</li> <li>/evaluation/slate/_movi_preprocessing</li> <li>/evaluation/slate/_metrics_discovery_masks</li> <li>/dataset/movi_e</li> </ol>"},{"location":"configs/evaluation/slate/metrics_pascal/","title":"configs/evaluation/slate/metrics_pascal.yaml","text":"<pre><code># @package _global_\n# Config file used with eval\ndefaults:\n- /evaluation_config  # (1)!\n- /evaluation/slate/_preprocessing_coco # (2)!\n- /evaluation/slate/_metrics_masks # (3)!\n- /dataset: voc2012_trainval  # (4)!\n- _self_\neval_batch_size: 1\n</code></pre> <ol> <li>/evaluation_config</li> <li>/evaluation/slate/_preprocessing_coco</li> <li>/evaluation/slate/_metrics_masks</li> <li>/dataset/voc2012_trainval</li> </ol>"},{"location":"configs/experiment/","title":"Experiment configurations","text":"<p>An experiment configuration is typically applied to the global training_config by using the hydra command <code>@package _global_</code> in the header.  They are used to ensure reproducibility of experiments by including all relevant parameters for an experiment to run.</p> <p>For an introduction to the concept of experiments please consider reading up on this design pattern in the hydra docs.</p>"},{"location":"configs/experiment/_output_path/","title":"configs/experiment/_output_path.yaml","text":"<pre><code># @package hydra\n# Change the output path to reflect the run experiment.\n# Instead of all runs being stored in outputs/&lt;date and time&gt;,\n# they will be in outputs/&lt;experiment&gt;/&lt;date and time&gt;.\n# For example: outputs/slot_attention/clevr6/2023-04-20_09-30-00\n#\n# Additiionally, this config sets the output output_subdir of the hydra\n# config to be `config` instead of `.config`.\nrun:\ndir: ${oc.select:experiment.root_output_folder,outputs}/${hydra:runtime.choices.experiment}/${now:%Y-%m-%d_%H-%M-%S}\nsweep:\ndir: ${oc.select:experiment.root_output_folder,multirun}\nsubdir: ${hydra:runtime.choices.experiment}/${now:%Y-%m-%d_%H-%M-%S}\noutput_subdir: config\n</code></pre>"},{"location":"configs/experiment/SAVi/cater/","title":"configs/experiment/SAVi/cater.yaml","text":"<pre><code># @package _global_\n# Configuration to exactly reproduce unsupervised object recognition of the original slot attention\n# paper.\ndefaults:\n- /experiment/_output_path  # (1)!\n- /training_config # (2)!\n- /dataset: cater  # (3)!\n- _self_\ntrainer:\ndevices: 8\ngradient_clip_val: 0.05\ngradient_clip_algorithm: norm\nmax_epochs:\nmax_steps: 199999\ndataset:\nnum_workers: 4\nbatch_size: 8\ntrain_transforms:\n03_preprocessing:\n_target_: ocl.transforms.SimpleTransform\ntransforms:\nimage:\n_target_: ocl.preprocessing.VideoToTensor\nbatch_transform: false\n02_random_strided_window:\n_target_: ocl.transforms.SampleConsecutive\nfields:\n- image\nn_consecutive: 6\neval_transforms:\n03_preprocessing:\n_target_: ocl.transforms.SimpleTransform\ntransforms:\nimage:\n_target_: ocl.preprocessing.VideoToTensor\nmask:\n_target_: ocl.preprocessing.MultiMaskToTensor\nbatch_transform: false\nmodels:\nconditioning:\n_target_: routed.ocl.conditioning.LearntConditioning\nobject_dim: 128\nn_slots: 11\nbatch_size_path: input.batch_size\nfeature_extractor:\n# Use the smaller verion of the feature extractor architecture.\n_target_: routed.ocl.feature_extractors.SAViFeatureExtractor\nlarger_input_arch: false\nvideo_path: input.image\nperceptual_grouping:\n_target_: ocl.utils.routing.Recurrent\ninputs_to_split:\n# Split features over frame axis.\n- feature_extractor.features\ninitial_input_mapping:\nobjects: conditioning\nmodule:\n_target_: routed.ocl.perceptual_grouping.SlotAttentionGrouping\nconditioning_path: previous_output.objects\nfeature_dim: 32\nobject_dim: ${models.conditioning.object_dim}\niters: 2\nkvq_dim: 128\nuse_projection_bias: false\npositional_embedding:\n_target_: ocl.neural_networks.wrappers.Sequential\n_args_:\n- _target_: ocl.neural_networks.positional_embedding.SoftPositionEmbed\nn_spatial_dims: 2\nfeature_dim: 32\nsavi_style: true\n- _target_: ocl.neural_networks.build_two_layer_mlp\ninput_dim: 32\noutput_dim: 32\nhidden_dim: 64\ninitial_layer_norm: true\nff_mlp:\nfeature_path: feature_extractor\nobject_decoder:\n_target_: routed.ocl.decoding.SlotAttentionDecoder\ndecoder:\n_target_: ocl.decoding.get_savi_decoder_backbone\nobject_dim: ${models.perceptual_grouping.module.object_dim}\nlarger_input_arch: false\npositional_embedding:\n_target_: ocl.neural_networks.positional_embedding.SoftPositionEmbed\nn_spatial_dims: 2\nfeature_dim: ${models.perceptual_grouping.module.object_dim}\ncnn_channel_order: true\nsavi_style: true\nobject_features_path: perceptual_grouping.objects\nlosses:\nmse:\n_target_: routed.ocl.losses.ReconstructionLoss\nloss_type: mse_sum\ninput_path: object_decoder.reconstruction\ntarget_path: input.image\nvisualizations:\ninput:\n_target_: routed.ocl.visualizations.Video\ndenormalization:\nvideo_path: input.image\nreconstruction:\n_target_: routed.ocl.visualizations.Video\ndenormalization: ${..input.denormalization}\nvideo_path: object_decoder.reconstruction\nobjects:\n_target_: routed.ocl.visualizations.VisualObject\ndenormalization: ${..input.denormalization}\nobject_path: object_decoder.object_reconstructions\nmask_path: object_decoder.masks\nevaluation_metrics:\nari:\n_target_: routed.ocl.metrics.ARIMetric\nprediction_path: object_decoder.masks\ntarget_path: input.mask\noptimizers:\nopt0:\n_target_: ocl.optimization.OptimizationWrapper\noptimizer:\n_target_: torch.optim.Adam\n_partial_: true\nlr: 0.0002\nlr_scheduler:\n_target_: ocl.scheduling.cosine_annealing_with_optional_warmup\n_partial_: true\nT_max: 200000\neta_min: 0.0\nwarmup_steps: 2500\n</code></pre> <ol> <li>/experiment/_output_path</li> <li>/training_config</li> <li>/dataset/cater</li> </ol>"},{"location":"configs/experiment/SAVi/clevrer/","title":"configs/experiment/SAVi/clevrer.yaml","text":"<pre><code># @package _global_\n# Configuration to exactly reproduce unsupervised object recognition of the original slot attention\n# paper.\ndefaults:\n- /experiment/_output_path  # (1)!\n- /training_config # (2)!\n- /dataset: clevrer  # (3)!\n- _self_\ntrainer:\ndevices: 8\ngradient_clip_val: 0.05\ngradient_clip_algorithm: norm\nmax_epochs:\nmax_steps: 199999\ndataset:\nnum_workers: 4\nbatch_size: 8\ntrain_transforms:\n01_decoding:\n_target_: ocl.transforms.DecodeRandomStridedWindow\nfields: [video.mp4]\nn_consecutive_frames: 6\nstride: 2\nsplit_extension: true\nvideo_reader_kwargs:\nctx:\n_target_: decord.cpu\nheight: 64\nwidth: 64\nnum_threads: 1\n03a_batch_preprocessing:\n_target_: ocl.transforms.SimpleTransform\ntransforms:\nvideo: '${lambda_fn:\"lambda a: (a.permute(0, 1, 4, 2, 3).contiguous().float()/255.).detach()\"}'\nbatch_transform: true\neval_transforms:\n01_decoding:\n_target_: ocl.transforms.VideoDecoder\nfields: [video.mp4]\nstride: 2\nsplit_extension: true\nvideo_reader_kwargs:\nctx:\n_target_: decord.cpu\nheight: 64\nwidth: 64\nnum_threads: 1\n03a_batch_preprocessing:\n_target_: ocl.transforms.SimpleTransform\ntransforms:\nvideo: '${lambda_fn:\"lambda a: (a.permute(0, 1, 4, 2, 3).contiguous().float()/255.).detach()\"}'\nbatch_transform: true\nmodels:\nconditioning:\n_target_: routed.ocl.conditioning.LearntConditioning\nobject_dim: 128\nn_slots: 11\nbatch_size_path: input.batch_size\nfeature_extractor:\n# Use the smaller verion of the feature extractor architecture.\n_target_: routed.ocl.feature_extractors.SAViFeatureExtractor\nlarger_input_arch: false\nvideo_path: input.video\nperceptual_grouping:\n_target_: ocl.utils.routing.Recurrent\ninputs_to_split:\n# Split features over frame axis.\n- feature_extractor.features\ninitial_input_mapping:\nobjects: conditioning\nmodule:\n_target_: routed.ocl.perceptual_grouping.SlotAttentionGrouping\nconditioning_path: previous_output.objects\nfeature_dim: 32\nobject_dim: ${models.conditioning.object_dim}\niters: 2\nkvq_dim: 128\nuse_projection_bias: false\npositional_embedding:\n_target_: ocl.neural_networks.wrappers.Sequential\n_args_:\n- _target_: ocl.neural_networks.positional_embedding.SoftPositionEmbed\nn_spatial_dims: 2\nfeature_dim: 32\nsavi_style: true\n- _target_: ocl.neural_networks.build_two_layer_mlp\ninput_dim: 32\noutput_dim: 32\nhidden_dim: 64\ninitial_layer_norm: true\nff_mlp:\nfeature_path: feature_extractor\nobject_decoder:\n_target_: routed.ocl.decoding.SlotAttentionDecoder\ndecoder:\n_target_: ocl.decoding.get_savi_decoder_backbone\nobject_dim: ${models.perceptual_grouping.module.object_dim}\nlarger_input_arch: false\npositional_embedding:\n_target_: ocl.neural_networks.positional_embedding.SoftPositionEmbed\nn_spatial_dims: 2\nfeature_dim: ${models.perceptual_grouping.module.object_dim}\ncnn_channel_order: true\nsavi_style: true\nobject_features_path: perceptual_grouping.objects\nlosses:\nmse:\n_target_: routed.ocl.losses.ReconstructionLoss\nloss_type: mse_sum\ninput_path: object_decoder.reconstruction\ntarget_path: input.video\nvisualizations:\ninput:\n_target_: routed.ocl.visualizations.Video\ndenormalization:\nvideo_path: input.video\nreconstruction:\n_target_: routed.ocl.visualizations.Video\ndenormalization: ${..input.denormalization}\nvideo_path: object_decoder.reconstruction\nobjects:\n_target_: routed.ocl.visualizations.VisualObject\ndenormalization: ${..input.denormalization}\nobject_path: object_decoder.object_reconstructions\nmask_path: object_decoder.masks\n# evaluation_metrics:\n#   ari:\n#     prediction_path: object_decoder.masks\n#     target_path: input.mask\noptimizers:\nopt0:\n_target_: ocl.optimization.OptimizationWrapper\noptimizer:\n_target_: torch.optim.Adam\n_partial_: true\nlr: 0.0002\nlr_scheduler:\n_target_: ocl.scheduling.cosine_annealing_with_optional_warmup\n_partial_: true\nT_max: 200000\neta_min: 0.0\nwarmup_steps: 2500\n</code></pre> <ol> <li>/experiment/_output_path</li> <li>/training_config</li> <li>/dataset/clevrer</li> </ol>"},{"location":"configs/experiment/SAVi/clevrer_highres/","title":"configs/experiment/SAVi/clevrer_highres.yaml","text":"<pre><code># @package _global_\n# Configuration to exactly reproduce unsupervised object recognition of the original slot attention\n# paper.\ndefaults:\n- /experiment/_output_path  # (1)!\n- /training_config # (2)!\n- /dataset: clevrer  # (3)!\n- _self_\ntrainer:\ndevices: 8\ngradient_clip_val: 0.05\ngradient_clip_algorithm: norm\nmax_epochs:\nmax_steps: 199999\ndataset:\nnum_workers: 3\nbatch_size: 8\n# Evaluation videos are long, thus use small bs.\neval_batch_size: 1\ntrain_transforms:\n01_decoding:\n_target_: ocl.transforms.DecodeRandomStridedWindow\nfields: [video.mp4]\nn_consecutive_frames: 6\nstride: 2\nsplit_extension: true\nvideo_reader_kwargs:\nctx:\n_target_: decord.cpu\nheight: 128\nwidth: 128\nnum_threads: 1\n03a_batch_preprocessing:\n_target_: ocl.transforms.SimpleTransform\ntransforms:\nvideo: '${lambda_fn:\"lambda a: (a.permute(0, 1, 4, 2, 3).contiguous().float()/255.).detach()\"}'\nbatch_transform: true\neval_transforms:\n01_decoding:\n_target_: ocl.transforms.VideoDecoder\nfields: [video.mp4]\nstride: 2\nsplit_extension: true\nvideo_reader_kwargs:\nctx:\n_target_: decord.cpu\nheight: 128\nwidth: 128\nnum_threads: 1\n03a_batch_preprocessing:\n_target_: ocl.transforms.SimpleTransform\ntransforms:\nvideo: '${lambda_fn:\"lambda a: (a.permute(0, 1, 4, 2, 3).contiguous().float()/255.).detach()\"}'\nbatch_transform: true\nmodels:\nconditioning:\n_target_: routed.ocl.conditioning.LearntConditioning\nobject_dim: 128\nn_slots: 11\nbatch_size_path: input.batch_size\nfeature_extractor:\n# Use the smaller verion of the feature extractor architecture.\n_target_: routed.ocl.feature_extractors.SAViFeatureExtractor\nlarger_input_arch: true\nvideo_path: input.video\nperceptual_grouping:\n_target_: ocl.utils.routing.Recurrent\ninputs_to_split:\n# Split features over frame axis.\n- feature_extractor.features\ninitial_input_mapping:\nobjects: conditioning\nmodule:\n_target_: routed.ocl.perceptual_grouping.SlotAttentionGrouping\nconditioning_path: previous_output.objects\nfeature_dim: 64\nobject_dim: ${models.conditioning.object_dim}\niters: 2\nkvq_dim: 128\nuse_projection_bias: false\npositional_embedding:\n_target_: ocl.neural_networks.wrappers.Sequential\n_args_:\n- _target_: ocl.neural_networks.positional_embedding.SoftPositionEmbed\nn_spatial_dims: 2\nfeature_dim: 64\nsavi_style: true\n- _target_: ocl.neural_networks.build_two_layer_mlp\ninput_dim: 64\noutput_dim: 64\nhidden_dim: 128\ninitial_layer_norm: true\nff_mlp:\nfeature_path: feature_extractor\nobject_decoder:\n_target_: routed.ocl.decoding.SlotAttentionDecoder\ndecoder:\n_target_: ocl.decoding.get_savi_decoder_backbone\nobject_dim: ${models.perceptual_grouping.module.object_dim}\nlarger_input_arch: true\npositional_embedding:\n_target_: ocl.neural_networks.positional_embedding.SoftPositionEmbed\nn_spatial_dims: 2\nfeature_dim: ${models.perceptual_grouping.module.object_dim}\ncnn_channel_order: true\nsavi_style: true\nobject_features_path: perceptual_grouping.objects\nlosses:\nmse:\n_target_: routed.ocl.losses.ReconstructionLoss\nloss_type: mse_sum\ninput_path: object_decoder.reconstruction\ntarget_path: input.video\nvisualizations:\ninput:\n_target_: routed.ocl.visualizations.Video\ndenormalization:\nvideo_path: input.video\nreconstruction:\n_target_: routed.ocl.visualizations.Video\ndenormalization: ${..input.denormalization}\nvideo_path: object_decoder.reconstruction\nobjects:\n_target_: routed.ocl.visualizations.VisualObject\ndenormalization: ${..input.denormalization}\nobject_path: object_decoder.object_reconstructions\nmask_path: object_decoder.masks\n# evaluation_metrics:\n#   ari:\n#     prediction_path: object_decoder.masks\n#     target_path: input.mask\noptimizers:\nopt0:\n_target_: ocl.optimization.OptimizationWrapper\noptimizer:\n_target_: torch.optim.Adam\n_partial_: true\nlr: 0.0002\nlr_scheduler:\n_target_: ocl.scheduling.cosine_annealing_with_optional_warmup\n_partial_: true\nT_max: 200000\neta_min: 0.0\nwarmup_steps: 2500\n</code></pre> <ol> <li>/experiment/_output_path</li> <li>/training_config</li> <li>/dataset/clevrer</li> </ol>"},{"location":"configs/experiment/SAVi_code/cater/","title":"configs/experiment/SAVi_code/cater.yaml","text":"<pre><code># @package _global_\n# An example implementaiton of SAVi that leverages a model definition in code.\n# The code can be found in `ocl/models/savi.py`, the config is used to\n# instantiate the submodules used by the code.\ndefaults:\n- /experiment/_output_path  # (1)!\n- /training_config # (2)!\n- /dataset: cater  # (3)!\n- _self_\ntrainer:\ndevices: 8\ngradient_clip_val: 0.05\ngradient_clip_algorithm: norm\nmax_epochs:\nmax_steps: 199999\ncallbacks:\n- _target_: pytorch_lightning.callbacks.LearningRateMonitor\nlogging_interval: step\ndataset:\nnum_workers: 4\nbatch_size: 8\ntrain_transforms:\n03_preprocessing:\n_target_: ocl.transforms.SimpleTransform\ntransforms:\nimage:\n_target_: ocl.preprocessing.VideoToTensor\nbatch_transform: false\n02_random_strided_window:\n_target_: ocl.transforms.SampleConsecutive\nfields:\n- image\nn_consecutive: 6\neval_transforms:\n03_preprocessing:\n_target_: ocl.transforms.SimpleTransform\ntransforms:\nimage:\n_target_: ocl.preprocessing.VideoToTensor\nmask:\n_target_: ocl.preprocessing.MultiMaskToTensor\nbatch_transform: false\nmodels:\n_target_: ocl.models.SAVi\nconditioning:\n_target_: ocl.conditioning.RandomConditioning\nobject_dim: 128\nn_slots: 11\nfeature_extractor:\n# Use the smaller verion of the feature extractor architecture.\n_target_: ocl.feature_extractors.SAViFeatureExtractor\nlarger_input_arch: false\nperceptual_grouping:\n_target_: ocl.perceptual_grouping.SlotAttentionGrouping\nfeature_dim: 32\nobject_dim: ${models.conditioning.object_dim}\niters: 2\nkvq_dim: 128\nuse_projection_bias: false\npositional_embedding:\n_target_: ocl.neural_networks.wrappers.Sequential\n_args_:\n- _target_: ocl.neural_networks.positional_embedding.SoftPositionEmbed\nn_spatial_dims: 2\nfeature_dim: 32\nsavi_style: true\n- _target_: ocl.neural_networks.build_two_layer_mlp\ninput_dim: 32\noutput_dim: 32\nhidden_dim: 64\ninitial_layer_norm: true\nff_mlp:\ndecoder:\n_target_: ocl.decoding.SlotAttentionDecoder\ndecoder:\n_target_: ocl.decoding.get_savi_decoder_backbone\nobject_dim: ${models.perceptual_grouping.object_dim}\nlarger_input_arch: false\npositional_embedding:\n_target_: ocl.neural_networks.positional_embedding.SoftPositionEmbed\nn_spatial_dims: 2\nfeature_dim: ${models.perceptual_grouping.object_dim}\ncnn_channel_order: true\nsavi_style: true\ntransition_model:\n_target_: torch.nn.Identity\nlosses:\nmse:\n_target_: routed.ocl.losses.ReconstructionLoss\nloss_type: mse_sum\ninput_path: decoder.reconstruction\ntarget_path: input.image\nvisualizations:\ninput:\n_target_: routed.ocl.visualizations.Video\ndenormalization:\nvideo_path: input.image\nreconstruction:\n_target_: routed.ocl.visualizations.Video\ndenormalization: ${..input.denormalization}\nvideo_path: decoder.reconstruction\nobjects:\n_target_: routed.ocl.visualizations.VisualObject\ndenormalization: ${..input.denormalization}\nobject_path: decoder.object_reconstructions\nmask_path: decoder.masks\nevaluation_metrics:\nari:\n_target_: routed.ocl.metrics.ARIMetric\nprediction_path: decoder.masks\ntarget_path: input.mask\noptimizers:\nopt0:\n_target_: ocl.optimization.OptimizationWrapper\noptimizer:\n_target_: torch.optim.Adam\n_partial_: true\nlr: 0.0002\nlr_scheduler:\n_target_: ocl.scheduling.cosine_annealing_with_optional_warmup\n_partial_: true\nT_max: 200000\neta_min: 0.0\nwarmup_steps: 2500\n</code></pre> <ol> <li>/experiment/_output_path</li> <li>/training_config</li> <li>/dataset/cater</li> </ol>"},{"location":"configs/experiment/SAVi_code/cater_savi_with_predictor/","title":"configs/experiment/SAVi_code/cater_savi_with_predictor.yaml","text":"<pre><code># @package _global_\n# An example implementaiton of SAVi that leverages a model definition in code.\n# The code can be found in `ocl/models/savi.py`, the config is used to\n# instantiate the submodules used by the code.\ndefaults:\n- /experiment/_output_path  # (1)!\n- /training_config # (2)!\n- /dataset: cater  # (3)!\n- _self_\ntrainer:\ndevices: 8\ngradient_clip_val: 0.05\ngradient_clip_algorithm: norm\nmax_epochs:\nmax_steps: 199999\ncallbacks:\n- _target_: pytorch_lightning.callbacks.LearningRateMonitor\nlogging_interval: step\ndataset:\nnum_workers: 4\nbatch_size: 8\ntrain_transforms:\n03_preprocessing:\n_target_: ocl.transforms.SimpleTransform\ntransforms:\nimage:\n_target_: ocl.preprocessing.VideoToTensor\nbatch_transform: false\n02_random_strided_window:\n_target_: ocl.transforms.SampleConsecutive\nfields:\n- image\nn_consecutive: 6\neval_transforms:\n03_preprocessing:\n_target_: ocl.transforms.SimpleTransform\ntransforms:\nimage:\n_target_: ocl.preprocessing.VideoToTensor\nmask:\n_target_: ocl.preprocessing.MultiMaskToTensor\nbatch_transform: false\nmodels:\n_target_: ocl.models.SAVi\nconditioning:\n_target_: routed.ocl.conditioning.LearntConditioning\nobject_dim: 128\nn_slots: 11\nbatch_size_path: input.batch_size\nfeature_extractor:\n# Use the smaller verion of the feature extractor architecture.\n_target_: ocl.feature_extractors.SAViFeatureExtractor\nlarger_input_arch: false\nperceptual_grouping:\n_target_: ocl.perceptual_grouping.SlotAttentionGrouping\nfeature_dim: 32\nobject_dim: ${models.conditioning.object_dim}\niters: 2\nkvq_dim: 128\nuse_projection_bias: false\npositional_embedding:\n_target_: ocl.neural_networks.wrappers.Sequential\n_args_:\n- _target_: ocl.neural_networks.positional_embedding.SoftPositionEmbed\nn_spatial_dims: 2\nfeature_dim: 32\nsavi_style: true\n- _target_: ocl.neural_networks.build_two_layer_mlp\ninput_dim: 32\noutput_dim: 32\nhidden_dim: 64\ninitial_layer_norm: true\nff_mlp:\ndecoder:\n_target_: ocl.decoding.SlotAttentionDecoder\ndecoder:\n_target_: ocl.decoding.get_savi_decoder_backbone\nobject_dim: ${models.perceptual_grouping.object_dim}\nlarger_input_arch: false\npositional_embedding:\n_target_: ocl.neural_networks.positional_embedding.SoftPositionEmbed\nn_spatial_dims: 2\nfeature_dim: ${models.perceptual_grouping.object_dim}\ncnn_channel_order: true\nsavi_style: true\ntransition_model:\n_target_: torch.nn.TransformerEncoderLayer\nd_model: 128\nnhead: 4\ndim_feedforward: 256\nbatch_first: true\nlosses:\nmse:\n_target_: routed.ocl.losses.ReconstructionLoss\nloss_type: mse_sum\ninput_path: decoder.reconstruction\ntarget_path: input.image\nvisualizations:\ninput:\n_target_: routed.ocl.visualizations.Video\ndenormalization:\nvideo_path: input.image\nreconstruction:\n_target_: routed.ocl.visualizations.Video\ndenormalization: ${..input.denormalization}\nvideo_path: decoder.reconstruction\nobjects:\n_target_: routed.ocl.visualizations.VisualObject\ndenormalization: ${..input.denormalization}\nobject_path: decoder.object_reconstructions\nmask_path: decoder.masks\nobjectmot:\n_target_: routed.ocl.visualizations.TrackedObject_from_Mask\nn_clips: 1\ndenormalization:\nvideo_path: input.image\nobject_masks_path: decoder.masks\nevaluation_metrics:\nari:\n_target_: routed.ocl.metrics.ARIMetric\nprediction_path: decoder.masks\ntarget_path: input.mask\nmot:\n_target_: routed.ocl.metrics.MOTMetric\nprediction_path: decoder.masks\ntarget_path: input.mask\noptimizers:\nopt0:\n_target_: ocl.optimization.OptimizationWrapper\noptimizer:\n_target_: torch.optim.Adam\n_partial_: true\nlr: 0.0002\nlr_scheduler:\n_target_: ocl.scheduling.cosine_annealing_with_optional_warmup\n_partial_: true\nT_max: 200000\neta_min: 0.0\nwarmup_steps: 2500\n</code></pre> <ol> <li>/experiment/_output_path</li> <li>/training_config</li> <li>/dataset/cater</li> </ol>"},{"location":"configs/experiment/clip/coco_finetuning/","title":"configs/experiment/clip/coco_finetuning.yaml","text":"<pre><code># @package _global_\n# Default parameters for slot attention.\ndefaults:\n- /experiment/_output_path  # (1)!\n- /training_config # (2)!\n- /dataset: coco  # (3)!\n- _self_\ntrainer:\nmax_epochs: 50\ndevices: -1\ndataset:\nnum_workers: 4\nbatch_size: 24\ntrain_transforms:\n03b_preprocessing:\n_target_: ocl.transforms.SimpleTransform\ntransforms:\nimage:\n_target_: torchvision.transforms.Compose\ntransforms:\n- _target_: torchvision.transforms.ToTensor\n- _target_: torchvision.transforms.RandomResizedCrop\nsize: 224\nscale: [0.9, 1.0]\ninterpolation: ${torchvision_interpolation_mode:BICUBIC}\n- _target_: torchvision.transforms.Normalize\nmean: [0.48145466, 0.4578275, 0.40821073]\nstd: [0.26862954, 0.26130258, 0.27577711]\nbatch_transform: false\n03c_preprocessing_1:\n_target_: ocl.transforms.SimpleTransform\ntransforms:\ncaption:\n_target_: ocl.preprocessing.RandomSample\nbatch_transform: false\n03c_preprocessing_2:\n_target_: ocl.transforms.DuplicateFields\nmapping:\ncaption: caption_str\nbatch_transform: false\n03d_preprocessing:\n_target_: ocl.transforms.SimpleTransform\ntransforms:\ncaption:\n_target_: ocl.preprocessing.TokenizeText\nbatch_transform: false\neval_transforms:\n03b_preprocessing:\n_target_: ocl.transforms.SimpleTransform\ntransforms:\nimage:\n_target_: torchvision.transforms.Compose\ntransforms:\n- _target_: torchvision.transforms.ToTensor\n- _target_: torchvision.transforms.Resize\nsize: 224\ninterpolation: ${torchvision_interpolation_mode:BICUBIC}\n- _target_: torchvision.transforms.CenterCrop\nsize: 224\n- _target_: torchvision.transforms.Normalize\nmean: [0.48145466, 0.4578275, 0.40821073]\nstd: [0.26862954, 0.26130258, 0.27577711]\nbatch_transform: false\n03c_preprocessing_1:\n_target_: ocl.transforms.SimpleTransform\ntransforms:\ncaption:\n_target_: ocl.preprocessing.RandomSample\nbatch_transform: false\n03c_preprocessing_2:\n_target_: ocl.transforms.DuplicateFields\nmapping:\ncaption: caption_str\nbatch_transform: false\n03d_preprocessing:\n_target_: ocl.transforms.SimpleTransform\ntransforms:\ncaption:\n_target_: ocl.preprocessing.TokenizeText\nbatch_transform: false\nmodels:\nimage_model:\n_target_: routed.ocl.feature_extractors.ClipImageModel\nmodel_type: RN50\nvideo_path: input.image\ntext_model:\n_target_: routed.ocl.feature_extractors.ClipTextModel\nmodel_type: RN50\ntext_path: input.caption\nevaluation_metrics: {}\nlosses:\nclip:\n_target_: routed.ocl.losses.CLIPLoss\nnormalize_inputs: true\nlearn_scale: true\nfirst_path: image_model.features\nsecond_path: text_model\nmodel_path: model\nvisualizations:\ninput:\n_target_: routed.ocl.visualizations.Image\ndenormalization:\n_target_: ocl.preprocessing.Denormalize\nmean: [0.48145466, 0.4578275, 0.40821073]\nstd: [0.26862954, 0.26130258, 0.27577711]\nimage_path: input.image\nmatching:\n_target_: routed.ocl.visualizations.TextToImageMatching\ndenormalization:\n_target_: ocl.preprocessing.Denormalize\nmean: [0.48145466, 0.4578275, 0.40821073]\nstd: [0.26862954, 0.26130258, 0.27577711]\nimage_path: input.image\ntext_path: input.caption_str\nsimilarities_path: losses.clip.similarities\noptimizers:\nopt0:\n_target_: torch.optim.Adam\n_partial_: true\nlr: 1e-5\n</code></pre> <ol> <li>/experiment/_output_path</li> <li>/training_config</li> <li>/dataset/coco</li> </ol>"},{"location":"configs/experiment/examples/hp_scheduling/","title":"configs/experiment/examples/hp_scheduling.yaml","text":"<pre><code># @package _global_\n# Example showing how to use hyperparameter scheduling capabilities.\n#\n# Based on experiment/slot_attention/clevr10_lds.yaml\ndefaults:\n- /experiment/slot_attention/clevr10 # (1)!\n- _self_\nexperiment:\ncallbacks:\nupdate_hps:\n_target_: ocl.callbacks.UpdateHyperparameterScheduling\nlosses:\nadditional_loss:\n_target_: routed.ocl.losses.ReconstructionLoss\nloss_type: l1\nweight:\n# Linearly ramp up loss weight from 0 to 500, from training step 5000 to 10000\n_target_: ocl.scheduling.LinearHPScheduler\nstart_value: 0.0\nend_value: 500.0\nstart_step: 5000\nend_step: 10000\ninput_path: object_decoder.reconstruction\ntarget_path: input.image\n</code></pre> <ol> <li>/experiment/slot_attention/clevr10</li> </ol>"},{"location":"configs/experiment/examples/parameter_groups/","title":"configs/experiment/examples/parameter_groups.yaml","text":"<pre><code># @package _global_\n# Example showing how to use optimizers with parameter groups.\n#\n# Parameter groups allow one to use different optimizer settings (e.g. learning rate) for\n# different parts of the model. Parameter groups can be configured in the following way:\n#\n# - \"params\": One or multiple paths to modules whose parameters should be included in the group.\n# - \"predicate\": A function that takes in the name and parameter and should return whether that\n#   parameter should be included in the parameter group.\n#\n# Parameters not included in any parameter group will not be passed to the optimizer! Thus take\n# care to include all parts of the model that should be optimized when using parameter groups.\ndefaults:\n- /experiment/slot_attention/clevr6 # (1)!\n- _self_\noptimizers:\nopt0:\n_target_: ocl.optimization.OptimizationWrapper\noptimizer:\n# Settings here will set default values for all parameter groups\n_target_: torch.optim.Adam\n_partial_: true\nlr: 0.0005\nbetas: [0.5, 0.9]\nparameter_groups:\n# Optimize feature_extractor, perceptual_grouping, conditioning with lower learning rate\n- params: [models.feature_extractor, models.perceptual_grouping, models.conditioning]\nlr: 0.0001\n# Apply weight decay to object_decoder, but not to bias parameters\n- params: models.object_decoder\npredicate: \"${lambda_fn:'lambda name, param: \\\"bias\\\" not in name'}\"\nweight_decay: 0.5\n- params: models.object_decoder\npredicate: \"${lambda_fn:'lambda name, param: \\\"bias\\\" in name'}\"\nweight_decay: 0.0\n</code></pre> <ol> <li>/experiment/slot_attention/clevr6</li> </ol>"},{"location":"configs/experiment/examples/slot_masking/","title":"configs/experiment/examples/slot_masking.yaml","text":"<pre><code># @package _global_\n# Example showing how slot masking, i.e. how to use slot attention with a variable number of\n# slots per batch-element.\ndataset:\ntrain_transforms:\n04_preprocessing:\n_target_: ocl.transforms.Map\ntransform: \"${lambda_fn:'lambda data: {\\\"n_slots\\\": 2, **data}'}\"\nfields: []\nbatch_transform: false\neval_transforms:\n04_preprocessing:\n_target_: ocl.transforms.Map\ntransform: \"${lambda_fn:'lambda data: {\\\"n_slots\\\": 2, **data}'}\"\nfields: []\nbatch_transform: false\ndefaults:\n- /experiment/slot_attention/clevr6 # (1)!\n- _self_\nmodels:\nslot_mask:\n_target_: routed.ocl.utils.masking.CreateSlotMask\nmax_slots: ${..conditioning.n_slots}\nn_slots_path: input.n_slots\nperceptual_grouping:\nslot_mask_path: slot_mask\n# Replace masked slots with a dummy slot after slot attention\nuse_empty_slot_for_masked_slots: true\n</code></pre> <ol> <li>/experiment/slot_attention/clevr6</li> </ol>"},{"location":"configs/experiment/occluded_slot_attention/clevr6/","title":"configs/experiment/occluded_slot_attention/clevr6.yaml","text":"<pre><code># @package _global_\n# Configuration to exactly reproduce unsupervised object recognition of the original slot attention\n# paper.\ndefaults:\n- /training_config  # (1)!\n- /dataset: clevr6  # (2)!\n- _self_\ntrainer:\ndevices: -1\ndataset:\nnum_workers: 4\nbatch_size: 64\ntrain_transforms:\npreprocessing:\n_target_: ocl.transforms.SimpleTransform\ntransforms:\nimage:\n_target_: torchvision.transforms.Compose\ntransforms:\n- _target_: torchvision.transforms.ToTensor\n- _target_: torchvision.transforms.CenterCrop\nsize: [192, 192]\n- _target_: torchvision.transforms.Resize\nsize: 128\nbatch_transform: false\neval_transforms:\npreprocessing:\n_target_: ocl.transforms.SimpleTransform\ntransforms:\nimage:\n_target_: torchvision.transforms.Compose\ntransforms:\n- _target_: torchvision.transforms.ToTensor\n- _target_: torchvision.transforms.CenterCrop\nsize: [192, 192]\n- _target_: torchvision.transforms.Resize\nsize: 128\nmask:\n_target_: torchvision.transforms.Compose\ntransforms:\n- _target_: ocl.preprocessing.MaskToTensor\n- _target_: torchvision.transforms.CenterCrop\nsize: [192, 192]\n- _target_: ocl.preprocessing.ResizeNearestExact\nsize: 128\nbatch_transform: false\nmodels:\nfeature_extractor:\n_target_: routed.ocl.feature_extractors.SlotAttentionFeatureExtractor\nvideo_path: input.image\nconditioning:\n_target_: routed.ocl.conditioning.SlotwiseLearntConditioning\nn_slots: 7\nobject_dim: 64\nbatch_size_path: input.batch_size\nperceptual_grouping:\n_target_: routed.ocl.perceptual_grouping.SlotAttentionGrouping\nfeature_dim: 64\nobject_dim: 64\npositional_embedding:\n_target_: ocl.neural_networks.wrappers.Sequential\n_args_:\n- _target_: ocl.neural_networks.positional_embedding.SoftPositionEmbed\nn_spatial_dims: 2\nfeature_dim: 64\n- _target_: ocl.neural_networks.build_two_layer_mlp\ninput_dim: 64\noutput_dim: 64\nhidden_dim: 128\ninitial_layer_norm: true\nresidual: false\nff_mlp:\n_target_: ocl.neural_networks.build_two_layer_mlp\ninput_dim: 64\noutput_dim: 64\nhidden_dim: 128\ninitial_layer_norm: true\nresidual: true\nfeature_path: feature_extractor\nconditioning_path: conditioning\nobject_decoder:\n_target_: routed.ocl.decoding.DensityPredictingSlotAttentionDecoder\ndepth_positions: ${..conditioning.n_slots}\nwhite_background: true\nobject_dim: ${..perceptual_grouping.object_dim}\nobject_features_path: perceptual_grouping.objects\ndecoder:\n_target_: ocl.decoding.get_slotattention_decoder_backbone\nobject_dim: ${..object_dim}\n# Dynamically compute the needed output dimension, based on the number of depth positions\noutput_dim: \"${eval_lambda:'lambda depth_pos: 3 + depth_pos', ${..depth_positions}}\"\nlosses:\nmse:\n_target_: routed.ocl.losses.ReconstructionLoss\nloss_type: mse\ninput_path: object_decoder.reconstruction\ntarget_path: input.image\nvisualizations:\ninput:\n_target_: routed.ocl.visualizations.Image\ndenormalization: \"${lambda_fn:'lambda t: t * 0.5 + 0.5'}\"\nimage_path: input.image\nreconstruction:\n_target_: routed.ocl.visualizations.Image\ndenormalization: ${..input.denormalization}\nimage_path: object_decoder.reconstruction\nobjects:\n_target_: routed.ocl.visualizations.VisualObject\ndenormalization: ${..input.denormalization}\nobject_path: object_decoder.object_reconstructions\nmask_path: object_decoder.masks\npred_segmentation:\n_target_: routed.ocl.visualizations.Segmentation\ndenormalization: ${..input.denormalization}\nimage_path: input.image\nmask_path: object_decoder.masks\noptimizers:\nopt0:\n_target_: ocl.optimization.OptimizationWrapper\noptimizer:\n_target_: torch.optim.Adam\n_partial_: true\nlr: 0.0004\nlr_scheduler:\n_target_: ocl.scheduling.exponential_decay_after_optional_warmup\n_partial_: true\ndecay_rate: 0.5\ndecay_steps: 100000\nwarmup_steps: 10000\n</code></pre> <ol> <li>/training_config</li> <li>/dataset/clevr6</li> </ol>"},{"location":"configs/experiment/projects/bridging/","title":"Bridging the Gap to Real-World Object-Centric Learning","text":""},{"location":"configs/experiment/projects/bridging/dinosaur/_base_feature_recon/","title":"configs/experiment/projects/bridging/dinosaur/_base_feature_recon.yaml","text":"<pre><code># @package _global_\n# Default parameters for slot attention with a ViT decoder for feature reconstruction.\ndefaults:\n- /experiment/_output_path  # (1)!\n- /training_config # (2)!\n- _self_\ntrainer:\ngradient_clip_val: 1.0\nexperiment:\ninput_feature_dim: 384\nmodels:\nfeature_extractor:\n_target_: routed.ocl.feature_extractors.TimmFeatureExtractor\nmodel_name: vit_small_patch16_224_dino\npretrained: false\nfreeze: true\nfeature_level: 12\nvideo_path: input.image\nconditioning:\nperceptual_grouping:\n_target_: routed.ocl.perceptual_grouping.SlotAttentionGrouping\nfeature_dim: ${.object_dim}\nobject_dim: ${models.conditioning.object_dim}\nuse_projection_bias: false\npositional_embedding:\n_target_: ocl.neural_networks.wrappers.Sequential\n_args_:\n- _target_: ocl.neural_networks.positional_embedding.DummyPositionEmbed\n- _target_: ocl.neural_networks.build_two_layer_mlp\ninput_dim: ${experiment.input_feature_dim}\noutput_dim: ${....feature_dim}\nhidden_dim: ${experiment.input_feature_dim}\ninitial_layer_norm: true\nff_mlp:\n_target_: ocl.neural_networks.build_two_layer_mlp\ninput_dim: ${..object_dim}\noutput_dim: ${..object_dim}\nhidden_dim: \"${eval_lambda:'lambda dim: 4 * dim', ${..object_dim}}\"\ninitial_layer_norm: true\nresidual: true\nfeature_path: feature_extractor\nconditioning_path: conditioning\nobject_decoder:\nobject_dim: ${models.perceptual_grouping.object_dim}\noutput_dim: ${experiment.input_feature_dim}\nnum_patches: 196\nobject_features_path: perceptual_grouping.objects\ntarget_path: feature_extractor.features\nimage_path: input.image\nlosses:\nmse:\n_target_: routed.ocl.losses.ReconstructionLoss\nloss_type: mse\ninput_path: object_decoder.reconstruction\ntarget_path: object_decoder.target  # Object decoder does some resizing.\nvisualizations:\ninput:\n_target_: routed.ocl.visualizations.Image\ndenormalization:\n_target_: ocl.preprocessing.Denormalize\nmean: [0.485, 0.456, 0.406]\nstd: [0.229, 0.224, 0.225]\nimage_path: input.image\nmasks:\n_target_: routed.ocl.visualizations.Mask\nmask_path: object_decoder.masks_as_image\npred_segmentation:\n_target_: routed.ocl.visualizations.Segmentation\ndenormalization:\n_target_: ocl.preprocessing.Denormalize\nmean: [0.485, 0.456, 0.406]\nstd: [0.229, 0.224, 0.225]\nimage_path: input.image\nmask_path: object_decoder.masks_as_image\noptimizers:\nopt0:\n_target_: ocl.optimization.OptimizationWrapper\noptimizer:\n_target_: torch.optim.Adam\n_partial_: true\nlr: 0.0004\nlr_scheduler:\n_target_: ocl.scheduling.exponential_decay_after_optional_warmup\n_partial_: true\ndecay_rate: 0.5\ndecay_steps: 100000\nwarmup_steps: 10000\n</code></pre> <ol> <li>/experiment/_output_path</li> <li>/training_config</li> </ol>"},{"location":"configs/experiment/projects/bridging/dinosaur/_metrics_clevr_patch/","title":"configs/experiment/projects/bridging/dinosaur/_metrics_clevr_patch.yaml","text":"<pre><code># @package _global_\nevaluation_metrics:\nari:\n_target_: routed.ocl.metrics.ARIMetric\nprediction_path: masks_as_image\ntarget_path: input.mask\nabo:\n_target_: routed.ocl.metrics.AverageBestOverlapMetric\nprediction_path: masks_as_image\ntarget_path: input.mask\nignore_background: true\n</code></pre>"},{"location":"configs/experiment/projects/bridging/dinosaur/_metrics_coco/","title":"configs/experiment/projects/bridging/dinosaur/_metrics_coco.yaml","text":"<pre><code># @package _global_\nevaluation_metrics:\ninstance_mask_ari:\n_target_: routed.ocl.metrics.ARIMetric\nprediction_path: object_decoder.masks_as_image\ntarget_path: input.instance_mask\nforeground: false\nconvert_target_one_hot: true\nignore_overlaps: true\ninstance_mask_iou:\n_target_: routed.ocl.metrics.UnsupervisedMaskIoUMetric\nprediction_path: object_decoder.masks_as_image\ntarget_path: input.instance_mask\nuse_threshold: false\nignore_overlaps: true\nsegmentation_mask_iou:\n_target_: routed.ocl.metrics.UnsupervisedMaskIoUMetric\nprediction_path: object_decoder.masks_as_image\ntarget_path: input.segmentation_mask\nuse_threshold: false\ninstance_abo:\n_target_: routed.ocl.metrics.UnsupervisedMaskIoUMetric\nprediction_path: object_decoder.masks_as_image\ntarget_path: input.instance_mask\nuse_threshold: false\nmatching: best_overlap\nignore_overlaps: true\nsegmentation_abo:\n_target_: routed.ocl.metrics.UnsupervisedMaskIoUMetric\nprediction_path: object_decoder.masks_as_image\ntarget_path: input.segmentation_mask\nuse_threshold: false\nmatching: best_overlap\nignore_overlaps: true\nobject_recovery:\n_target_: routed.ocl.metrics.UnsupervisedMaskIoUMetric\nprediction_path: object_decoder.masks_as_image\ntarget_path: input.instance_mask\nuse_threshold: false\nmatching: best_overlap\ncompute_discovery_fraction: true\nignore_overlaps: true\ninstance_mask_corloc:\n_target_: routed.ocl.metrics.MaskCorLocMetric\nprediction_path: object_decoder.masks_as_image\ntarget_path: input.instance_mask\nuse_threshold: false\nignore_overlaps: true\n</code></pre>"},{"location":"configs/experiment/projects/bridging/dinosaur/_preprocessing_coco_dino_feature_recon_ccrop/","title":"configs/experiment/projects/bridging/dinosaur/_preprocessing_coco_dino_feature_recon_ccrop.yaml","text":"<pre><code># @package _global_\ndataset:\neval_transforms:\n03a_preprocessing:\n_target_: ocl.transforms.Map\ntransform:\n_target_: torchvision.transforms.Compose\ntransforms:\n- _target_: ocl.preprocessing.InstanceMasksToDenseMasks\n- _target_: ocl.preprocessing.AddSegmentationMaskFromInstanceMask\n# Drop instance_category again as some images do not contain it\n- \"${lambda_fn:'lambda data: {k: v for k, v in data.items() if k != \\\"instance_category\\\"\\\n}'}\"\n- _target_: ocl.preprocessing.AddEmptyMasks\nmask_keys:\n- instance_mask\n- segmentation_mask\nfields:\n- image\n- instance_mask\n- instance_category\nbatch_transform: false\n03b_preprocessing:\n_target_: ocl.transforms.SimpleTransform\ntransforms:\nimage:\n_target_: torchvision.transforms.Compose\ntransforms:\n- _target_: torchvision.transforms.ToTensor\n- _target_: torchvision.transforms.Resize\nsize: 224\ninterpolation: ${torchvision_interpolation_mode:BICUBIC}\n- \"${lambda_fn:'lambda image: image.clamp(0.0, 1.0)'}\" # Bicubic interpolation can get out of range\n- _target_: torchvision.transforms.CenterCrop\nsize: 224\n- _target_: torchvision.transforms.Normalize\nmean: [0.485, 0.456, 0.406]\nstd: [0.229, 0.224, 0.225]\ninstance_mask:\n_target_: torchvision.transforms.Compose\ntransforms:\n- _target_: ocl.preprocessing.DenseMaskToTensor\n- _target_: ocl.preprocessing.ResizeNearestExact\nsize: 224\n- _target_: torchvision.transforms.CenterCrop\nsize: 224\nsegmentation_mask:\n_target_: torchvision.transforms.Compose\ntransforms:\n- _target_: ocl.preprocessing.DenseMaskToTensor\n- _target_: ocl.preprocessing.ResizeNearestExact\nsize: 224\n- _target_: torchvision.transforms.CenterCrop\nsize: 224\nbatch_transform: false\ntrain_transforms:\n03b_preprocessing:\n_target_: ocl.transforms.SimpleTransform\ntransforms:\nimage:\n_target_: torchvision.transforms.Compose\ntransforms:\n- _target_: torchvision.transforms.ToTensor\n- _target_: torchvision.transforms.Resize\nsize: 224\ninterpolation: ${torchvision_interpolation_mode:BICUBIC}\n- \"${lambda_fn:'lambda image: image.clamp(0.0, 1.0)'}\" # Bicubic interpolation can get out of range\n- _target_: torchvision.transforms.CenterCrop\nsize: 224\n- _target_: torchvision.transforms.RandomHorizontalFlip\n- _target_: torchvision.transforms.Normalize\nmean: [0.485, 0.456, 0.406]\nstd: [0.229, 0.224, 0.225]\nbatch_transform: false\n</code></pre>"},{"location":"configs/experiment/projects/bridging/dinosaur/_preprocessing_coco_dino_feature_recon_origres/","title":"configs/experiment/projects/bridging/dinosaur/_preprocessing_coco_dino_feature_recon_origres.yaml","text":"<pre><code># @package _global_\ndataset:\neval_transforms:\n03a_preprocessing:\n_target_: ocl.transforms.Map\ntransform:\n_target_: torchvision.transforms.Compose\ntransforms:\n- _target_: ocl.preprocessing.InstanceMasksToDenseMasks\n- _target_: ocl.preprocessing.AddSegmentationMaskFromInstanceMask\n# Drop instance_category again as some images do not contain it\n- \"${lambda_fn:'lambda data: {k: v for k, v in data.items() if k != \\\"instance_category\\\"\\\n}'}\"\n- _target_: ocl.preprocessing.AddEmptyMasks\nmask_keys:\n- instance_mask\n- segmentation_mask\nfields:\n- image\n- instance_mask\n- instance_category\nbatch_transform: false\n03b_preprocessing:\n_target_: ocl.transforms.SimpleTransform\ntransforms:\nimage:\n_target_: torchvision.transforms.Compose\ntransforms:\n- _target_: torchvision.transforms.ToTensor\n- _target_: torchvision.transforms.Resize\n_convert_: partial\nsize: [224, 224]\ninterpolation: ${torchvision_interpolation_mode:BICUBIC}\n- \"${lambda_fn:'lambda image: image.clamp(0.0, 1.0)'}\" # Bicubic interpolation can get out of range\n- _target_: torchvision.transforms.Normalize\nmean: [0.485, 0.456, 0.406]\nstd: [0.229, 0.224, 0.225]\ninstance_mask:\n_target_: torchvision.transforms.Compose\ntransforms:\n- _target_: ocl.preprocessing.DenseMaskToTensor\n- _target_: ocl.preprocessing.ResizeNearestExact\n_convert_: partial\nsize: [224, 224]\nsegmentation_mask:\n_target_: torchvision.transforms.Compose\ntransforms:\n- _target_: ocl.preprocessing.DenseMaskToTensor\n- _target_: ocl.preprocessing.ResizeNearestExact\n_convert_: partial\nsize: [224, 224]\nbatch_transform: false\ntrain_transforms:\n03b_preprocessing:\n_target_: ocl.transforms.SimpleTransform\ntransforms:\nimage:\n_target_: torchvision.transforms.Compose\ntransforms:\n- _target_: torchvision.transforms.ToTensor\n- _target_: torchvision.transforms.Resize\n_convert_: partial\nsize: [224, 224]\ninterpolation: ${torchvision_interpolation_mode:BICUBIC}\n- \"${lambda_fn:'lambda image: image.clamp(0.0, 1.0)'}\" # Bicubic interpolation can get out of range\n- _target_: torchvision.transforms.RandomHorizontalFlip\n- _target_: torchvision.transforms.Normalize\nmean: [0.485, 0.456, 0.406]\nstd: [0.229, 0.224, 0.225]\nbatch_transform: false\n</code></pre>"},{"location":"configs/experiment/projects/bridging/dinosaur/_preprocessing_coco_dino_feature_recon_randcrop/","title":"configs/experiment/projects/bridging/dinosaur/_preprocessing_coco_dino_feature_recon_randcrop.yaml","text":"<pre><code># @package _global_\ndataset:\neval_transforms:\n03a_preprocessing:\n_target_: ocl.transforms.Map\ntransform:\n_target_: torchvision.transforms.Compose\ntransforms:\n- _target_: ocl.preprocessing.InstanceMasksToDenseMasks\n- _target_: ocl.preprocessing.AddSegmentationMaskFromInstanceMask\n# Drop instance_category again as some images do not contain it\n- \"${lambda_fn:'lambda data: {k: v for k, v in data.items() if k != \\\"instance_category\\\"\\\n}'}\"\n- _target_: ocl.preprocessing.AddEmptyMasks\nmask_keys:\n- instance_mask\n- segmentation_mask\nfields:\n- image\n- instance_mask\n- instance_category\nbatch_transform: false\n03b_preprocessing:\n_target_: ocl.transforms.SimpleTransform\ntransforms:\nimage:\n_target_: torchvision.transforms.Compose\ntransforms:\n- _target_: torchvision.transforms.ToTensor\n- _target_: torchvision.transforms.Resize\nsize: 224\ninterpolation: ${torchvision_interpolation_mode:BICUBIC}\n- \"${lambda_fn:'lambda image: image.clamp(0.0, 1.0)'}\" # Bicubic interpolation can get out of range\n- _target_: torchvision.transforms.CenterCrop\nsize: 224\n- _target_: torchvision.transforms.Normalize\nmean: [0.485, 0.456, 0.406]\nstd: [0.229, 0.224, 0.225]\ninstance_mask:\n_target_: torchvision.transforms.Compose\ntransforms:\n- _target_: ocl.preprocessing.DenseMaskToTensor\n- _target_: ocl.preprocessing.ResizeNearestExact\nsize: 224\n- _target_: torchvision.transforms.CenterCrop\nsize: 224\nsegmentation_mask:\n_target_: torchvision.transforms.Compose\ntransforms:\n- _target_: ocl.preprocessing.DenseMaskToTensor\n- _target_: ocl.preprocessing.ResizeNearestExact\nsize: 224\n- _target_: torchvision.transforms.CenterCrop\nsize: 224\nbatch_transform: false\ntrain_transforms:\n03b_preprocessing:\n_target_: ocl.transforms.SimpleTransform\ntransforms:\nimage:\n_target_: torchvision.transforms.Compose\ntransforms:\n- _target_: torchvision.transforms.ToTensor\n- _target_: torchvision.transforms.Resize\nsize: 224\ninterpolation: ${torchvision_interpolation_mode:BICUBIC}\n- \"${lambda_fn:'lambda image: image.clamp(0.0, 1.0)'}\" # Bicubic interpolation can get out of range\n- _target_: torchvision.transforms.RandomCrop\nsize: 224\n- _target_: torchvision.transforms.RandomHorizontalFlip\n- _target_: torchvision.transforms.Normalize\nmean: [0.485, 0.456, 0.406]\nstd: [0.229, 0.224, 0.225]\nbatch_transform: false\n</code></pre>"},{"location":"configs/experiment/projects/bridging/dinosaur/_preprocessing_coco_imagenet_feature_recon/","title":"configs/experiment/projects/bridging/dinosaur/_preprocessing_coco_imagenet_feature_recon.yaml","text":"<pre><code># @package dataset\n# Override parts of preprocessing, imagenet model used different normalization.\ntrain_transforms:\n03b_preprocessing:\ntransforms:\nimage:\n_target_: torchvision.transforms.Compose\ntransforms:\n- _target_: torchvision.transforms.ToTensor\n- _target_: torchvision.transforms.Resize\nsize: 224\ninterpolation: ${torchvision_interpolation_mode:BICUBIC}\n- \"${lambda_fn:'lambda image: image.clamp(0.0, 1.0)'}\"  # Bicubic interpolation can get out of range\n- _target_: torchvision.transforms.RandomCrop\nsize: 224\n- _target_: torchvision.transforms.RandomHorizontalFlip\n- _target_: torchvision.transforms.Normalize\nmean: [0.5, 0.5, 0.5]\nstd: [0.5, 0.5, 0.5]\neval_transforms:\n03b_preprocessing:\ntransforms:\nimage:\n_target_: torchvision.transforms.Compose\ntransforms:\n- _target_: torchvision.transforms.ToTensor\n- _target_: torchvision.transforms.Resize\nsize: 224\ninterpolation: ${torchvision_interpolation_mode:BICUBIC}\n- \"${lambda_fn:'lambda image: image.clamp(0.0, 1.0)'}\"  # Bicubic interpolation can get out of range\n- _target_: torchvision.transforms.CenterCrop\nsize: 224\n- _target_: torchvision.transforms.Normalize\nmean: [0.5, 0.5, 0.5]\nstd: [0.5, 0.5, 0.5]\n</code></pre>"},{"location":"configs/experiment/projects/bridging/dinosaur/_preprocessing_movi_dino_feature_recon/","title":"configs/experiment/projects/bridging/dinosaur/_preprocessing_movi_dino_feature_recon.yaml","text":"<pre><code># @package _global_\ndataset:\ntrain_transforms:\n03_preprocessing:\n_target_: ocl.transforms.SimpleTransform\ntransforms:\nimage:\n_target_: torchvision.transforms.Compose\ntransforms:\n- _target_: torchvision.transforms.ToTensor\n- _target_: torchvision.transforms.Resize\nsize: 224\ninterpolation: ${torchvision_interpolation_mode:BICUBIC}\n- \"${lambda_fn:'lambda image: image.clamp(0.0, 1.0)'}\" # Bicubic interpolation can get out of range\n- _target_: torchvision.transforms.Normalize\nmean: [0.485, 0.456, 0.406]\nstd: [0.229, 0.224, 0.225]\nbatch_transform: false\neval_transforms:\n03_preprocessing:\n_target_: ocl.transforms.SimpleTransform\ntransforms:\nimage:\n_target_: torchvision.transforms.Compose\ntransforms:\n- _target_: torchvision.transforms.ToTensor\n- _target_: torchvision.transforms.Resize\nsize: 224\ninterpolation: ${torchvision_interpolation_mode:BICUBIC}\n- \"${lambda_fn:'lambda image: image.clamp(0.0, 1.0)'}\" # Bicubic interpolation can get out of range\n- _target_: torchvision.transforms.Normalize\nmean: [0.485, 0.456, 0.406]\nstd: [0.229, 0.224, 0.225]\nmask:\n_target_: torchvision.transforms.Compose\ntransforms:\n- _target_: ocl.preprocessing.MultiMaskToTensor\n- _target_: ocl.preprocessing.ResizeNearestExact\nsize: 128\nbatch_transform: false\n</code></pre>"},{"location":"configs/experiment/projects/bridging/dinosaur/_preprocessing_voc2012_segm_dino_feature_recon/","title":"configs/experiment/projects/bridging/dinosaur/_preprocessing_voc2012_segm_dino_feature_recon.yaml","text":"<pre><code># @package _global_\ndataset:\neval_transforms:\n02a_format_consistency:\n_target_: ocl.transforms.SimpleTransform\ntransforms:\n# Convert to one-hot encoding.\nsegmentation-instance:\n_target_: ocl.preprocessing.IntegerToOneHotMask\nbatch_transform: false\n02b_format_consistency:\n_target_: ocl.transforms.Map\ntransform:\n_target_: torchvision.transforms.Compose\ntransforms:\n# Create segmentation mask.\n- _target_: ocl.preprocessing.VOCInstanceMasksToDenseMasks\ninstance_mask_key: segmentation-instance\nclass_mask_key: segmentation-class\nclasses_key: instance_category\n- _target_: ocl.preprocessing.RenameFields\nmapping:\nsegmentation-instance: instance_mask\nfields:\n- segmentation-instance\n- segmentation-class\n- image\nbatch_transform: false\n03a_preprocessing:\n_target_: ocl.transforms.Map\ntransform:\n_target_: torchvision.transforms.Compose\ntransforms:\n# This is not needed for VOC.\n# - _target_: ocl.preprocessing.InstanceMasksToDenseMasks\n- _target_: ocl.preprocessing.AddSegmentationMaskFromInstanceMask\n# Drop instance_category again as some images do not contain it\n- \"${lambda_fn:'lambda data: {k: v for k, v in data.items() if k != \\\"instance_category\\\"\\\n}'}\"\n- _target_: ocl.preprocessing.AddEmptyMasks\nmask_keys:\n- instance_mask\n- segmentation_mask\nfields:\n- image\n- instance_mask\n- instance_category\nbatch_transform: false\n03b_preprocessing:\n_target_: ocl.transforms.SimpleTransform\ntransforms:\nimage:\n_target_: torchvision.transforms.Compose\ntransforms:\n- _target_: torchvision.transforms.ToTensor\n- _target_: torchvision.transforms.Resize\nsize: 224\ninterpolation: ${torchvision_interpolation_mode:BICUBIC}\n- \"${lambda_fn:'lambda image: image.clamp(0.0, 1.0)'}\" # Bicubic interpolation can get out of range\n- _target_: torchvision.transforms.CenterCrop\nsize: 224\n- _target_: torchvision.transforms.Normalize\nmean: [0.485, 0.456, 0.406]\nstd: [0.229, 0.224, 0.225]\ninstance_mask:\n_target_: torchvision.transforms.Compose\ntransforms:\n- _target_: ocl.preprocessing.DenseMaskToTensor\n- _target_: ocl.preprocessing.ResizeNearestExact\nsize: 224\n- _target_: torchvision.transforms.CenterCrop\nsize: 224\nsegmentation_mask:\n_target_: torchvision.transforms.Compose\ntransforms:\n- _target_: ocl.preprocessing.DenseMaskToTensor\n- _target_: ocl.preprocessing.ResizeNearestExact\nsize: 224\n- _target_: torchvision.transforms.CenterCrop\nsize: 224\nbatch_transform: false\ntrain_transforms:\n03b_preprocessing:\n_target_: ocl.transforms.SimpleTransform\ntransforms:\nimage:\n_target_: torchvision.transforms.Compose\ntransforms:\n- _target_: torchvision.transforms.ToTensor\n- _target_: torchvision.transforms.Resize\nsize: 224\ninterpolation: ${torchvision_interpolation_mode:BICUBIC}\n- \"${lambda_fn:'lambda image: image.clamp(0.0, 1.0)'}\" # Bicubic interpolation can get out of range\n- _target_: torchvision.transforms.RandomCrop\nsize: 224\n- _target_: torchvision.transforms.RandomHorizontalFlip\n- _target_: torchvision.transforms.Normalize\nmean: [0.485, 0.456, 0.406]\nstd: [0.229, 0.224, 0.225]\nbatch_transform: false\n</code></pre>"},{"location":"configs/experiment/projects/bridging/dinosaur/coco_feat_rec_dino_base16/","title":"configs/experiment/projects/bridging/dinosaur/coco_feat_rec_dino_base16.yaml","text":"<pre><code># @package _global_\ndefaults:\n- /experiment/projects/bridging/dinosaur/_base_feature_recon  # (1)!\n- /dataset: coco  # (2)!\n- /experiment/projects/bridging/dinosaur/_preprocessing_coco_dino_feature_recon_ccrop # (3)!\n- /experiment/projects/bridging/dinosaur/_metrics_coco # (4)!\n- _self_\n# The following parameters assume training on 8 GPUs, leading to an effective batch size of 64.\ntrainer:\ndevices: 8\nmax_steps: 500000\nmax_epochs:\ndataset:\nnum_workers: 4\nbatch_size: 8\nexperiment:\ninput_feature_dim: 768\nmodels:\nconditioning:\n_target_: routed.ocl.conditioning.RandomConditioning\nn_slots: 7\nobject_dim: 256\nbatch_size_path: input.batch_size\nfeature_extractor:\nmodel_name: vit_base_patch16_224_dino\npretrained: ${when_testing:false,true}\nfreeze: true\nobject_decoder:\n_target_: routed.ocl.decoding.PatchDecoder\ndecoder:\n_target_: ocl.neural_networks.build_mlp\n_partial_: true\nfeatures: [2048, 2048, 2048]\nobject_features_path: perceptual_grouping.objects\n</code></pre> <ol> <li>/experiment/projects/bridging/dinosaur/_base_feature_recon</li> <li>/dataset/coco</li> <li>/experiment/projects/bridging/dinosaur/_preprocessing_coco_dino_feature_recon_ccrop</li> <li>/experiment/projects/bridging/dinosaur/_metrics_coco</li> </ol>"},{"location":"configs/experiment/projects/bridging/dinosaur/coco_feat_rec_dino_base16_auto/","title":"configs/experiment/projects/bridging/dinosaur/coco_feat_rec_dino_base16_auto.yaml","text":"<pre><code># @package _global_\n# ViT feature reconstruction on COCO using an autoregressive decoder (SLATE style)\ndefaults:\n- /experiment/projects/bridging/dinosaur/_base_feature_recon  # (1)!\n- /dataset: coco  # (2)!\n- /experiment/projects/bridging/dinosaur/_preprocessing_coco_dino_feature_recon_ccrop # (3)!\n- /experiment/projects/bridging/dinosaur/_metrics_coco # (4)!\n- _self_\n# The following parameters assume training on 8 GPUs, leading to an effective batch size of 64.\ntrainer:\ndevices: 8\nmax_steps: 500000\nmax_epochs:\ngradient_clip_val: 1.0\ndataset:\nnum_workers: 4\nbatch_size: 8\nmodels:\nconditioning:\n_target_: routed.ocl.conditioning.RandomConditioning\nn_slots: 7\nobject_dim: 256\nbatch_size_path: input.batch_size\nfeature_extractor:\nmodel_name: vit_base_patch16_224_dino\npretrained: ${when_testing:false,true}\nfreeze: true\nperceptual_grouping: {}\nobject_decoder:\n_target_: routed.ocl.decoding.AutoregressivePatchDecoder\ndecoder_cond_dim: ${.output_dim}\nuse_input_transform: true\nuse_decoder_masks: true\ndecoder:\n_target_: ocl.neural_networks.build_transformer_decoder\n_partial_: true\nn_layers: 4\nn_heads: 8\nreturn_attention_weights: true\nmasks_path: perceptual_grouping.feature_attributions\nobject_features_path: perceptual_grouping.objects\nexperiment:\ninput_feature_dim: 768\n</code></pre> <ol> <li>/experiment/projects/bridging/dinosaur/_base_feature_recon</li> <li>/dataset/coco</li> <li>/experiment/projects/bridging/dinosaur/_preprocessing_coco_dino_feature_recon_ccrop</li> <li>/experiment/projects/bridging/dinosaur/_metrics_coco</li> </ol>"},{"location":"configs/experiment/projects/bridging/dinosaur/coco_feat_rec_dino_small16_auto/","title":"configs/experiment/projects/bridging/dinosaur/coco_feat_rec_dino_small16_auto.yaml","text":"<pre><code># @package _global_\n# ViT feature reconstruction on COCO using an autoregressive decoder (SLATE style)\ndefaults:\n- /experiment/projects/bridging/dinosaur/_base_feature_recon  # (1)!\n- /dataset: coco  # (2)!\n- /experiment/projects/bridging/dinosaur/_preprocessing_coco_dino_feature_recon_ccrop # (3)!\n- /experiment/projects/bridging/dinosaur/_metrics_coco # (4)!\n- _self_\n# The following parameters assume training on 8 GPUs, leading to an effective batch size of 64.\ntrainer:\ndevices: 8\nmax_steps: 500000\nmax_epochs:\ngradient_clip_val: 1.0\ndataset:\nnum_workers: 4\nbatch_size: 8\nexperiment:\ninput_feature_dim: 384\nmodels:\nconditioning:\n_target_: routed.ocl.conditioning.RandomConditioning\nn_slots: 7\nobject_dim: 256\nbatch_size_path: input.batch_size\nfeature_extractor:\nmodel_name: vit_small_patch16_224_dino\npretrained: ${when_testing:false,true}\nfreeze: true\nobject_decoder:\n_target_: routed.ocl.decoding.AutoregressivePatchDecoder\ndecoder_cond_dim: ${.output_dim}\nuse_input_transform: true\nuse_decoder_masks: true\ndecoder:\n_target_: ocl.neural_networks.build_transformer_decoder\n_partial_: true\nn_layers: 4\nn_heads: 8\nreturn_attention_weights: true\nmasks_path: perceptual_grouping.feature_attributions\nobject_features_path: perceptual_grouping.objects\n</code></pre> <ol> <li>/experiment/projects/bridging/dinosaur/_base_feature_recon</li> <li>/dataset/coco</li> <li>/experiment/projects/bridging/dinosaur/_preprocessing_coco_dino_feature_recon_ccrop</li> <li>/experiment/projects/bridging/dinosaur/_metrics_coco</li> </ol>"},{"location":"configs/experiment/projects/bridging/dinosaur/coco_feat_rec_in_base16_auto/","title":"configs/experiment/projects/bridging/dinosaur/coco_feat_rec_in_base16_auto.yaml","text":"<pre><code># @package _global_\n# ViT feature reconstruction (ImageNet supervised) on COCO using an autoregressive decoder (SLATE style)\ndefaults:\n- /experiment/projects/bridging/dinosaur/_base_feature_recon  # (1)!\n- /dataset: coco  # (2)!\n- /experiment/projects/bridging/dinosaur/_preprocessing_coco_dino_feature_recon_ccrop # (3)!\n- /experiment/projects/bridging/dinosaur/_preprocessing_coco_imagenet_feature_recon # (4)!\n- /experiment/projects/bridging/dinosaur/_metrics_coco # (5)!\n- _self_\n# The following parameters assume training on 8 GPUs, leading to an effective batch size of 64.\ntrainer:\ndevices: 8\nmax_steps: 500000\nmax_epochs:\ngradient_clip_val: 1.0\nexperiment:\ninput_feature_dim: 768\ndataset:\nnum_workers: 4\nbatch_size: 8\nmodels:\nconditioning:\n_target_: routed.ocl.conditioning.RandomConditioning\nn_slots: 7\nobject_dim: 256\nbatch_size_path: input.batch_size\nfeature_extractor:\nmodel_name: vit_base_patch16_224\npretrained: ${when_testing:false,true}\nfreeze: true\nobject_decoder:\n_target_: routed.ocl.decoding.AutoregressivePatchDecoder\ndecoder_cond_dim: ${.output_dim}\nuse_input_transform: true\nuse_decoder_masks: true\ndecoder:\n_target_: ocl.neural_networks.build_transformer_decoder\n_partial_: true\nn_layers: 4\nn_heads: 8\nreturn_attention_weights: true\nmasks_path: perceptual_grouping.feature_attributions\nobject_features_path: perceptual_grouping.objects\nvisualizations:\ninput:\ndenormalization:\nmean: [0.5, 0.5, 0.5]\nstd: [0.5, 0.5, 0.5]\npred_segmentation:\ndenormalization:\nmean: [0.5, 0.5, 0.5]\nstd: [0.5, 0.5, 0.5]\n</code></pre> <ol> <li>/experiment/projects/bridging/dinosaur/_base_feature_recon</li> <li>/dataset/coco</li> <li>/experiment/projects/bridging/dinosaur/_preprocessing_coco_dino_feature_recon_ccrop</li> <li>/experiment/projects/bridging/dinosaur/_preprocessing_coco_imagenet_feature_recon</li> <li>/experiment/projects/bridging/dinosaur/_metrics_coco</li> </ol>"},{"location":"configs/experiment/projects/bridging/dinosaur/coco_feat_rec_in_small16_auto/","title":"configs/experiment/projects/bridging/dinosaur/coco_feat_rec_in_small16_auto.yaml","text":"<pre><code># @package _global_\n# ViT feature reconstruction (ImageNet supervised) on COCO using an autoregressive decoder (SLATE style)\ndefaults:\n- /experiment/projects/bridging/dinosaur/_base_feature_recon  # (1)!\n- /dataset: coco  # (2)!\n- /experiment/projects/bridging/dinosaur/_preprocessing_coco_dino_feature_recon_ccrop # (3)!\n- /experiment/projects/bridging/dinosaur/_preprocessing_coco_imagenet_feature_recon # (4)!\n- /experiment/projects/bridging/dinosaur/_metrics_coco # (5)!\n- _self_\n# The following parameters assume training on 8 GPUs, leading to an effective batch size of 64.\ntrainer:\ndevices: 8\nmax_steps: 500000\nmax_epochs:\ngradient_clip_val: 1.0\ndataset:\nnum_workers: 4\nbatch_size: 8\nexperiment:\ninput_feature_dim: 384\nmodels:\nconditioning:\n_target_: routed.ocl.conditioning.RandomConditioning\nn_slots: 7\nobject_dim: 256\nbatch_size_path: input.batch_size\nfeature_extractor:\nmodel_name: vit_small_patch16_224\npretrained: ${when_testing:false,true}\nfreeze: true\nobject_decoder:\n_target_: routed.ocl.decoding.AutoregressivePatchDecoder\ndecoder_cond_dim: ${.output_dim}\nuse_input_transform: true\nuse_decoder_masks: true\ndecoder:\n_target_: ocl.neural_networks.build_transformer_decoder\n_partial_: true\nn_layers: 4\nn_heads: 8\nreturn_attention_weights: true\nmasks_path: perceptual_grouping.feature_attributions\nobject_features_path: perceptual_grouping.objects\nvisualizations:\ninput:\ndenormalization:\nmean: [0.5, 0.5, 0.5]\nstd: [0.5, 0.5, 0.5]\npred_segmentation:\ndenormalization:\nmean: [0.5, 0.5, 0.5]\nstd: [0.5, 0.5, 0.5]\n</code></pre> <ol> <li>/experiment/projects/bridging/dinosaur/_base_feature_recon</li> <li>/dataset/coco</li> <li>/experiment/projects/bridging/dinosaur/_preprocessing_coco_dino_feature_recon_ccrop</li> <li>/experiment/projects/bridging/dinosaur/_preprocessing_coco_imagenet_feature_recon</li> <li>/experiment/projects/bridging/dinosaur/_metrics_coco</li> </ol>"},{"location":"configs/experiment/projects/bridging/dinosaur/coco_feat_rec_resnet50/","title":"configs/experiment/projects/bridging/dinosaur/coco_feat_rec_resnet50.yaml","text":"<pre><code># @package _global_\n# ResNet50 feature reconstruction (ImageNet supervised) on COCO\ndefaults:\n- /experiment/projects/bridging/dinosaur/_base_feature_recon  # (1)!\n- /dataset: coco  # (2)!\n- /experiment/projects/bridging/dinosaur/_preprocessing_coco_dino_feature_recon_ccrop # (3)!\n- /experiment/projects/bridging/dinosaur/_metrics_coco # (4)!\n- _self_\n# The following parameters assume training on 8 GPUs, leading to an effective batch size of 64.\ntrainer:\ndevices: 8\nmax_steps: 500000\nmax_epochs:\ngradient_clip_val: 1.0\ndataset:\nnum_workers: 4\nbatch_size: 8\nexperiment:\ninput_feature_dim: 1024\nmodels:\nconditioning:\n_target_: routed.ocl.conditioning.RandomConditioning\nn_slots: 7\nobject_dim: 256\nbatch_size_path: input.batch_size\nfeature_extractor:\nmodel_name: resnet50\nfeature_level: 3\npretrained: ${when_testing:false,true}\nfreeze: true\nperceptual_grouping:\nfeature_dim: ${.object_dim}\nobject_dim: ${models.conditioning.object_dim}\nuse_projection_bias: false\npositional_embedding:\n_target_: ocl.neural_networks.wrappers.Sequential\n_args_:\n- _target_: ocl.neural_networks.positional_embedding.SoftPositionEmbed\nn_spatial_dims: 2\nfeature_dim: ${experiment.input_feature_dim}\nsavi_style: true\n- _target_: ocl.neural_networks.build_two_layer_mlp\ninput_dim: ${experiment.input_feature_dim}\noutput_dim: ${....feature_dim}\nhidden_dim: ${experiment.input_feature_dim}\ninitial_layer_norm: true\nobject_decoder:\n_target_: routed.ocl.decoding.PatchDecoder\ndecoder:\n_target_: ocl.neural_networks.build_mlp\n_partial_: true\nfeatures: [2048, 2048, 2048]\nobject_features_path: perceptual_grouping.objects\n</code></pre> <ol> <li>/experiment/projects/bridging/dinosaur/_base_feature_recon</li> <li>/dataset/coco</li> <li>/experiment/projects/bridging/dinosaur/_preprocessing_coco_dino_feature_recon_ccrop</li> <li>/experiment/projects/bridging/dinosaur/_metrics_coco</li> </ol>"},{"location":"configs/experiment/projects/bridging/dinosaur/movi_c_feat_rec/","title":"configs/experiment/projects/bridging/dinosaur/movi_c_feat_rec.yaml","text":"<pre><code># @package _global_\n# ViT feature reconstruction on MOVI-C.\ndefaults:\n- /experiment/projects/bridging/dinosaur/_base_feature_recon  # (1)!\n- /dataset: movi_c_image  # (2)!\n- /experiment/projects/bridging/dinosaur/_preprocessing_movi_dino_feature_recon # (3)!\n- /experiment/projects/bridging/dinosaur/_metrics_clevr_patch # (4)!\n- _self_\n# The following parameters assume training on 8 GPUs, leading to an effective batch size of 64.\ntrainer:\ndevices: 8\nmax_steps: 500000\nmax_epochs:\ndataset:\nnum_workers: 4\nbatch_size: 8\nmodels:\nconditioning:\n_target_: routed.ocl.conditioning.RandomConditioning\nn_slots: 11\nobject_dim: 128\nbatch_size_path: input.batch_size\nfeature_extractor:\nmodel_name: vit_small_patch8_224_dino\nobject_decoder:\n_target_: routed.ocl.decoding.PatchDecoder\nnum_patches: 784\ndecoder:\n_target_: ocl.neural_networks.build_mlp\n_partial_: true\nfeatures: [1024, 1024, 1024]\nobject_features_path: perceptual_grouping.objects\nmasks_as_image:\n_target_: routed.ocl.utils.resizing.Resize\ninput_path: object_decoder.masks\nsize: 128\nresize_mode: bilinear\npatch_mode: true\n</code></pre> <ol> <li>/experiment/projects/bridging/dinosaur/_base_feature_recon</li> <li>/dataset/movi_c_image</li> <li>/experiment/projects/bridging/dinosaur/_preprocessing_movi_dino_feature_recon</li> <li>/experiment/projects/bridging/dinosaur/_metrics_clevr_patch</li> </ol>"},{"location":"configs/experiment/projects/bridging/dinosaur/movi_c_feat_rec_auto/","title":"configs/experiment/projects/bridging/dinosaur/movi_c_feat_rec_auto.yaml","text":"<pre><code># @package _global_\n# ViT feature reconstruction on MOVI-C using an autoregressive decoder (SLATE style)\ndefaults:\n- /experiment/projects/bridging/dinosaur/_base_feature_recon  # (1)!\n- /dataset: movi_c_image  # (2)!\n- /experiment/projects/bridging/dinosaur/_preprocessing_movi_dino_feature_recon # (3)!\n- /experiment/projects/bridging/dinosaur/_metrics_clevr_patch # (4)!\n- _self_\n# The following parameters assume training on 8 GPUs, leading to an effective batch size of 64.\ntrainer:\ndevices: 8\nmax_steps: 500000\nmax_epochs:\ndataset:\nnum_workers: 4\nbatch_size: 8\nmodels:\nconditioning:\n_target_: routed.ocl.conditioning.RandomConditioning\nn_slots: 11\nobject_dim: 128\nbatch_size_path: input.batch_size\nobject_decoder:\n_target_: routed.ocl.decoding.AutoregressivePatchDecoder\ndecoder_cond_dim: 384\nuse_input_transform: true\ndecoder:\n_target_: ocl.neural_networks.build_transformer_decoder\n_partial_: true\nn_layers: 4\nn_heads: 4\nmasks_path: perceptual_grouping.feature_attributions\nobject_features_path: perceptual_grouping.objects\nmasks_as_image:\n_target_: routed.ocl.utils.resizing.Resize\ninput_path: object_decoder.masks\nsize: 128\nresize_mode: bilinear\npatch_mode: true\n</code></pre> <ol> <li>/experiment/projects/bridging/dinosaur/_base_feature_recon</li> <li>/dataset/movi_c_image</li> <li>/experiment/projects/bridging/dinosaur/_preprocessing_movi_dino_feature_recon</li> <li>/experiment/projects/bridging/dinosaur/_metrics_clevr_patch</li> </ol>"},{"location":"configs/experiment/projects/bridging/dinosaur/movi_e_feat_rec/","title":"configs/experiment/projects/bridging/dinosaur/movi_e_feat_rec.yaml","text":"<pre><code># @package _global_\n# ViT feature reconstruction on MOVI-E.\ndefaults:\n- /experiment/projects/bridging/dinosaur/_base_feature_recon  # (1)!\n- /dataset: movi_e_image  # (2)!\n- /experiment/projects/bridging/dinosaur/_preprocessing_movi_dino_feature_recon # (3)!\n- /experiment/projects/bridging/dinosaur/_metrics_clevr_patch # (4)!\n- _self_\n# The following parameters assume training on 8 GPUs, leading to an effective batch size of 64.\ntrainer:\ndevices: 8\nmax_steps: 500000\nmax_epochs:\ndataset:\nnum_workers: 4\nbatch_size: 8\nmodels:\nconditioning:\n_target_: routed.ocl.conditioning.RandomConditioning\nn_slots: 24\nobject_dim: 128\nbatch_size_path: input.batch_size\nfeature_extractor:\nmodel_name: vit_small_patch8_224_dino\nobject_decoder:\n_target_: routed.ocl.decoding.PatchDecoder\nnum_patches: 784\ndecoder:\n_target_: ocl.neural_networks.build_mlp\n_partial_: true\nfeatures: [1024, 1024, 1024]\nobject_features_path: perceptual_grouping.objects\nmasks_as_image:\n_target_: routed.ocl.utils.resizing.Resize\ninput_path: object_decoder.masks\nsize: 128\nresize_mode: bilinear\npatch_mode: true\n</code></pre> <ol> <li>/experiment/projects/bridging/dinosaur/_base_feature_recon</li> <li>/dataset/movi_e_image</li> <li>/experiment/projects/bridging/dinosaur/_preprocessing_movi_dino_feature_recon</li> <li>/experiment/projects/bridging/dinosaur/_metrics_clevr_patch</li> </ol>"},{"location":"configs/experiment/projects/bridging/dinosaur/movi_e_feat_rec_auto/","title":"configs/experiment/projects/bridging/dinosaur/movi_e_feat_rec_auto.yaml","text":"<pre><code># @package _global_\n# ViT feature reconstruction on MOVI-E using an autoregressive decoder (SLATE style)\ndefaults:\n- /experiment/projects/bridging/dinosaur/_base_feature_recon  # (1)!\n- /dataset: movi_e_image  # (2)!\n- /experiment/projects/bridging/dinosaur/_preprocessing_movi_dino_feature_recon # (3)!\n- /experiment/projects/bridging/dinosaur/_metrics_clevr_patch # (4)!\n- _self_\n# The following parameters assume training on 8 GPUs, leading to an effective batch size of 64.\ntrainer:\ndevices: 8\nmax_steps: 500000\nmax_epochs:\ndataset:\nnum_workers: 4\nbatch_size: 8\nmodels:\nconditioning:\n_target_: routed.ocl.conditioning.RandomConditioning\nn_slots: 24\nobject_dim: 128\nbatch_size_path: input.batch_size\nobject_decoder:\n_target_: routed.ocl.decoding.AutoregressivePatchDecoder\ndecoder_cond_dim: 384\nuse_input_transform: true\ndecoder:\n_target_: ocl.neural_networks.build_transformer_decoder\n_partial_: true\nn_layers: 4\nn_heads: 4\nmasks_path: perceptual_grouping.feature_attributions\nobject_features_path: perceptual_grouping.objects\nmasks_as_image:\n_target_: routed.ocl.utils.resizing.Resize\ninput_path: object_decoder.masks\nsize: 128\nresize_mode: bilinear\npatch_mode: true\n</code></pre> <ol> <li>/experiment/projects/bridging/dinosaur/_base_feature_recon</li> <li>/dataset/movi_e_image</li> <li>/experiment/projects/bridging/dinosaur/_preprocessing_movi_dino_feature_recon</li> <li>/experiment/projects/bridging/dinosaur/_metrics_clevr_patch</li> </ol>"},{"location":"configs/experiment/projects/bridging/dinosaur/voc2012_trainaug_feat_rec_dino_base16_auto/","title":"configs/experiment/projects/bridging/dinosaur/voc2012_trainaug_feat_rec_dino_base16_auto.yaml","text":"<pre><code># @package _global_\ndefaults:\n- /experiment/projects/bridging/dinosaur/_base_feature_recon  # (1)!\n- /dataset: voc2012_trainaug  # (2)!\n- /experiment/projects/bridging/dinosaur/_preprocessing_voc2012_segm_dino_feature_recon # (3)!\n- /experiment/projects/bridging/dinosaur/_metrics_coco # (4)!\n- _self_\n# The following parameters assume training on 8 GPUs, leading to an effective batch size of 64.\ntrainer:\ndevices: 8\nmax_steps: 300000\nmax_epochs:\ncheck_val_every_n_epoch: 50\ndataset:\nnum_workers: 4\nbatch_size: 8\nmodels:\nconditioning:\n_target_: routed.ocl.conditioning.RandomConditioning\nn_slots: 6\nobject_dim: 256\nbatch_size_path: input.batch_size\nfeature_extractor:\nmodel_name: vit_base_patch16_224_dino\npretrained: ${when_testing:false,true}\nfreeze: true\nperceptual_grouping: {}\nobject_decoder:\n_target_: routed.ocl.decoding.AutoregressivePatchDecoder\ndecoder_cond_dim: ${.output_dim}\nuse_input_transform: true\nuse_decoder_masks: true\ndecoder:\n_target_: ocl.neural_networks.build_transformer_decoder\n_partial_: true\nn_layers: 4\nn_heads: 4\nreturn_attention_weights: true\nmasks_path: perceptual_grouping.feature_attributions\nobject_features_path: perceptual_grouping.objects\nexperiment:\ninput_feature_dim: 768\n</code></pre> <ol> <li>/experiment/projects/bridging/dinosaur/_base_feature_recon</li> <li>/dataset/voc2012_trainaug</li> <li>/experiment/projects/bridging/dinosaur/_preprocessing_voc2012_segm_dino_feature_recon</li> <li>/experiment/projects/bridging/dinosaur/_metrics_coco</li> </ol>"},{"location":"configs/experiment/projects/bridging/experiment_image_feature_rec/_base/","title":"configs/experiment/projects/bridging/experiment_image_feature_rec/_base.yaml","text":"<pre><code># @package _global_\n# Default parameters for slot attention with a ViT decoder for feature reconstruction.\ndefaults:\n- /experiment/_output_path  # (1)!\n- /training_config # (2)!\n- _self_\ntrainer:\ngradient_clip_val: 1.0\nmodels:\nfeature_extractor:\n_target_: routed.ocl.feature_extractors.TimmFeatureExtractor\nmodel_name: vit_small_patch16_224_dino\npretrained: false\nfreeze: true\nfeature_level: 12\nvideo_path: input.image\nconditioning:\nperceptual_grouping:\n_target_: routed.ocl.perceptual_grouping.SlotAttentionGrouping\nfeature_dim: ${.object_dim}\nobject_dim: ${models.conditioning.object_dim}\nuse_projection_bias: false\npositional_embedding:\n_target_: ocl.neural_networks.wrappers.Sequential\n_args_:\n- _target_: ocl.neural_networks.positional_embedding.DummyPositionEmbed\n- _target_: ocl.neural_networks.build_two_layer_mlp\ninput_dim: ${experiment.input_feature_dim}\noutput_dim: ${....feature_dim}\nhidden_dim: ${experiment.input_feature_dim}\ninitial_layer_norm: true\nff_mlp:\n_target_: ocl.neural_networks.build_two_layer_mlp\ninput_dim: ${..object_dim}\noutput_dim: ${..object_dim}\nhidden_dim: \"${eval_lambda:'lambda dim: 4 * dim', ${..object_dim}}\"\ninitial_layer_norm: true\nresidual: true\nfeature_path: feature_extractor\nconditioning_path: conditioning\nlosses:\nmse:\n_target_: routed.ocl.losses.ReconstructionLoss\nloss_type: mse\ninput_path: object_decoder.reconstruction\ntarget_path: object_decoder.target  # Object decoder does some resizing.\nvisualizations:\ninput:\n_target_: routed.ocl.visualizations.Image\ndenormalization:\n_target_: ocl.preprocessing.Denormalize\nmean: [0.485, 0.456, 0.406]\nstd: [0.229, 0.224, 0.225]\nimage_path: input.image\nmasks:\n_target_: routed.ocl.visualizations.Mask\nmask_path: object_decoder.masks_as_image\npred_segmentation:\n_target_: routed.ocl.visualizations.Segmentation\ndenormalization:\n_target_: ocl.preprocessing.Denormalize\nmean: [0.485, 0.456, 0.406]\nstd: [0.229, 0.224, 0.225]\nimage_path: input.image\nmask_path: object_decoder.masks_as_image\noptimizers:\nopt0:\n_target_: ocl.optimization.OptimizationWrapper\noptimizer:\n_target_: torch.optim.Adam\n_partial_: true\nlr: 0.0004\nlr_scheduler:\n_target_: ocl.scheduling.exponential_decay_after_optional_warmup\n_partial_: true\ndecay_rate: 0.5\ndecay_steps: 100000\nwarmup_steps: 10000\nexperiment:\ninput_feature_dim: 384\n</code></pre> <ol> <li>/experiment/_output_path</li> <li>/training_config</li> </ol>"},{"location":"configs/experiment/projects/bridging/experiment_image_feature_rec/_metrics_coco_image/","title":"configs/experiment/projects/bridging/experiment_image_feature_rec/_metrics_coco_image.yaml","text":"<pre><code># @package _global_\nevaluation_metrics:\ninstance_mask_ari:\n_target_: routed.ocl.metrics.ARIMetric\nprediction_path: object_decoder.masks\ntarget_path: input.instance_mask\nforeground: false\nconvert_target_one_hot: true\nignore_overlaps: true\ninstance_mask_iou:\n_target_: routed.ocl.metrics.UnsupervisedMaskIoUMetric\nprediction_path: object_decoder.masks\ntarget_path: input.instance_mask\nuse_threshold: false\nignore_overlaps: true\nsegmentation_mask_iou:\n_target_: routed.ocl.metrics.UnsupervisedMaskIoUMetric\nprediction_path: object_decoder.masks\ntarget_path: input.segmentation_mask\nuse_threshold: false\ninstance_abo:\n_target_: routed.ocl.metrics.UnsupervisedMaskIoUMetric\nprediction_path: object_decoder.masks\ntarget_path: input.instance_mask\nuse_threshold: false\nmatching: best_overlap\nignore_overlaps: true\nsegmentation_abo:\n_target_: routed.ocl.metrics.UnsupervisedMaskIoUMetric\nprediction_path: object_decoder.masks\ntarget_path: input.segmentation_mask\nuse_threshold: false\nmatching: best_overlap\nobject_recovery:\n_target_: routed.ocl.metrics.UnsupervisedMaskIoUMetric\nprediction_path: object_decoder.masks\ntarget_path: input.instance_mask\nuse_threshold: false\nmatching: best_overlap\ncompute_discovery_fraction: true\nignore_overlaps: true\ninstance_mask_corloc:\n_target_: routed.ocl.metrics.MaskCorLocMetric\nprediction_path: object_decoder.masks\ntarget_path: input.instance_mask\nuse_threshold: false\nignore_overlaps: true\n</code></pre>"},{"location":"configs/experiment/projects/bridging/experiment_image_feature_rec/_preprocessing_coco_dino_feature_recon/","title":"configs/experiment/projects/bridging/experiment_image_feature_rec/_preprocessing_coco_dino_feature_recon.yaml","text":"<pre><code># @package _global_\ndataset:\neval_transforms:\n03a_preprocessing:\n_target_: ocl.transforms.Map\ntransform:\n_target_: torchvision.transforms.Compose\ntransforms:\n- _target_: ocl.preprocessing.InstanceMasksToDenseMasks\n- _target_: ocl.preprocessing.AddSegmentationMaskFromInstanceMask\n# Drop instance_category again as some images do not contain it\n- \"${lambda_fn:'lambda data: {k: v for k, v in data.items() if k != \\\"instance_category\\\"\\\n}'}\"\n- _target_: ocl.preprocessing.AddEmptyMasks\nmask_keys:\n- instance_mask\n- segmentation_mask\nfields:\n- image\n- instance_mask\n- instance_category\nbatch_transform: false\n03b_preprocessing:\n_target_: ocl.transforms.SimpleTransform\ntransforms:\nimage:\n_target_: torchvision.transforms.Compose\ntransforms:\n- _target_: torchvision.transforms.ToTensor\n- _target_: torchvision.transforms.Resize\nsize: 224\ninterpolation: ${torchvision_interpolation_mode:BICUBIC}\n- \"${lambda_fn:'lambda image: image.clamp(0.0, 1.0)'}\" # Bicubic interpolation can get out of range\n- _target_: torchvision.transforms.CenterCrop\nsize: 224\n- _target_: torchvision.transforms.Normalize\nmean: [0.485, 0.456, 0.406]\nstd: [0.229, 0.224, 0.225]\ninstance_mask:\n_target_: torchvision.transforms.Compose\ntransforms:\n- _target_: ocl.preprocessing.DenseMaskToTensor\n- _target_: ocl.preprocessing.ResizeNearestExact\nsize: 224\n- _target_: torchvision.transforms.CenterCrop\nsize: 224\nsegmentation_mask:\n_target_: torchvision.transforms.Compose\ntransforms:\n- _target_: ocl.preprocessing.DenseMaskToTensor\n- _target_: ocl.preprocessing.ResizeNearestExact\nsize: 224\n- _target_: torchvision.transforms.CenterCrop\nsize: 224\nbatch_transform: false\ntrain_transforms:\n03b_preprocessing:\n_target_: ocl.transforms.SimpleTransform\ntransforms:\nimage:\n_target_: torchvision.transforms.Compose\ntransforms:\n- _target_: torchvision.transforms.ToTensor\n- _target_: torchvision.transforms.Resize\nsize: 224\ninterpolation: ${torchvision_interpolation_mode:BICUBIC}\n- \"${lambda_fn:'lambda image: image.clamp(0.0, 1.0)'}\" # Bicubic interpolation can get out of range\n- _target_: torchvision.transforms.CenterCrop\nsize: 224\n- _target_: torchvision.transforms.Normalize\nmean: [0.485, 0.456, 0.406]\nstd: [0.229, 0.224, 0.225]\nbatch_transform: false\n</code></pre>"},{"location":"configs/experiment/projects/bridging/experiment_image_feature_rec/_preprocessing_coco_dino_image_recon/","title":"configs/experiment/projects/bridging/experiment_image_feature_rec/_preprocessing_coco_dino_image_recon.yaml","text":"<pre><code># @package _global_\ndataset:\ntrain_transforms:\n03a_preprocessing:\n_target_: ocl.transforms.Map\ntransform: \"${lambda_fn:'lambda data: {\\\"target\\\": data[\\\"image\\\"], **data}'}\"\nfields:\n- image\nbatch_transform: false\n03b_preprocessing:\n_target_: ocl.transforms.SimpleTransform\ntransforms:\nimage:\n_target_: torchvision.transforms.Compose\ntransforms:\n- _target_: torchvision.transforms.ToTensor\n- _target_: torchvision.transforms.Resize\nsize: 224\ninterpolation: ${torchvision_interpolation_mode:BICUBIC}\n- \"${lambda_fn:'lambda image: image.clamp(0.0, 1.0)'}\" # Bicubic interpolation can get out of range\n- _target_: torchvision.transforms.CenterCrop\nsize: 224\n- _target_: torchvision.transforms.Normalize\nmean: [0.485, 0.456, 0.406]\nstd: [0.229, 0.224, 0.225]\ntarget:\n_target_: torchvision.transforms.Compose\ntransforms:\n- _target_: torchvision.transforms.ToTensor\n- _target_: torchvision.transforms.Resize\nsize: 128\ninterpolation: ${torchvision_interpolation_mode:BICUBIC}\n- \"${lambda_fn:'lambda image: image.clamp(0.0, 1.0)'}\" # Bicubic interpolation can get out of range\n- _target_: torchvision.transforms.CenterCrop\nsize: 128\n- _target_: torchvision.transforms.Normalize\nmean: [0.5, 0.5, 0.5]\nstd: [0.5, 0.5, 0.5]\nbatch_transform: false\neval_transforms:\n03a_preprocessing:\n_target_: ocl.transforms.Map\ntransform:\n_target_: torchvision.transforms.Compose\ntransforms:\n- \"${lambda_fn:'lambda data: {\\\"target\\\": data[\\\"image\\\"], **data}'}\"\n- _target_: ocl.preprocessing.InstanceMasksToDenseMasks\n- _target_: ocl.preprocessing.AddSegmentationMaskFromInstanceMask\n# Drop instance_category again as some images do not contain it\n- \"${lambda_fn:'lambda data: {k: v for k, v in data.items() if k != \\\"instance_category\\\"\\\n}'}\"\n- _target_: ocl.preprocessing.AddEmptyMasks\nmask_keys:\n- instance_mask\n- segmentation_mask\nfields:\n- image\n- instance_mask\n- instance_category\nbatch_transform: false\n03b_preprocessing:\n_target_: ocl.transforms.SimpleTransform\ntransforms:\nimage:\n_target_: torchvision.transforms.Compose\ntransforms:\n- _target_: torchvision.transforms.ToTensor\n- _target_: torchvision.transforms.Resize\nsize: 224\ninterpolation: ${torchvision_interpolation_mode:BICUBIC}\n- \"${lambda_fn:'lambda image: image.clamp(0.0, 1.0)'}\" # Bicubic interpolation can get out of range\n- _target_: torchvision.transforms.CenterCrop\nsize: 224\n- _target_: torchvision.transforms.Normalize\nmean: [0.485, 0.456, 0.406]\nstd: [0.229, 0.224, 0.225]\ntarget:\n_target_: torchvision.transforms.Compose\ntransforms:\n- _target_: torchvision.transforms.ToTensor\n- _target_: torchvision.transforms.Resize\nsize: 128\ninterpolation: ${torchvision_interpolation_mode:BICUBIC}\n- \"${lambda_fn:'lambda image: image.clamp(0.0, 1.0)'}\" # Bicubic interpolation can get out of range\n- _target_: torchvision.transforms.CenterCrop\nsize: 128\n- _target_: torchvision.transforms.Normalize\nmean: [0.5, 0.5, 0.5]\nstd: [0.5, 0.5, 0.5]\ninstance_mask:\n_target_: torchvision.transforms.Compose\ntransforms:\n- _target_: ocl.preprocessing.DenseMaskToTensor\n- _target_: ocl.preprocessing.ResizeNearestExact\nsize: 128\n- _target_: torchvision.transforms.CenterCrop\nsize: 128\nsegmentation_mask:\n_target_: torchvision.transforms.Compose\ntransforms:\n- _target_: ocl.preprocessing.DenseMaskToTensor\n- _target_: ocl.preprocessing.ResizeNearestExact\nsize: 128\n- _target_: torchvision.transforms.CenterCrop\nsize: 128\nbatch_transform: false\n</code></pre>"},{"location":"configs/experiment/projects/bridging/experiment_image_feature_rec/_preprocessing_coco_resnet_feature_recon/","title":"configs/experiment/projects/bridging/experiment_image_feature_rec/_preprocessing_coco_resnet_feature_recon.yaml","text":"<pre><code># @package _global_\ndataset:\neval_transforms:\n03a_preprocessing:\n_target_: ocl.transforms.Map\ntransform:\n_target_: torchvision.transforms.Compose\ntransforms:\n- _target_: ocl.preprocessing.InstanceMasksToDenseMasks\n- _target_: ocl.preprocessing.AddSegmentationMaskFromInstanceMask\n# Drop instance_category again as some images do not contain it\n- \"${lambda_fn:'lambda data: {k: v for k, v in data.items() if k != \\\"instance_category\\\"\\\n}'}\"\n- _target_: ocl.preprocessing.AddEmptyMasks\nmask_keys:\n- instance_mask\n- segmentation_mask\nfields:\n- image\n- instance_mask\n- instance_category\nbatch_transform: false\n03b_preprocessing:\n_target_: ocl.transforms.SimpleTransform\ntransforms:\nimage:\n_target_: torchvision.transforms.Compose\ntransforms:\n- _target_: torchvision.transforms.ToTensor\n- _target_: torchvision.transforms.Resize\nsize: 128\ninterpolation: ${torchvision_interpolation_mode:BICUBIC}\n- \"${lambda_fn:'lambda image: image.clamp(0.0, 1.0)'}\" # Bicubic interpolation can get out of range\n- _target_: torchvision.transforms.CenterCrop\nsize: 128\n- _target_: torchvision.transforms.Normalize\nmean: [0.485, 0.456, 0.406]\nstd: [0.229, 0.224, 0.225]\ninstance_mask:\n_target_: torchvision.transforms.Compose\ntransforms:\n- _target_: ocl.preprocessing.DenseMaskToTensor\n- _target_: ocl.preprocessing.ResizeNearestExact\nsize: 128\n- _target_: torchvision.transforms.CenterCrop\nsize: 128\nsegmentation_mask:\n_target_: torchvision.transforms.Compose\ntransforms:\n- _target_: ocl.preprocessing.DenseMaskToTensor\n- _target_: ocl.preprocessing.ResizeNearestExact\nsize: 128\n- _target_: torchvision.transforms.CenterCrop\nsize: 128\nbatch_transform: false\ntrain_transforms:\n03b_preprocessing:\n_target_: ocl.transforms.SimpleTransform\ntransforms:\nimage:\n_target_: torchvision.transforms.Compose\ntransforms:\n- _target_: torchvision.transforms.ToTensor\n- _target_: torchvision.transforms.Resize\nsize: 128\ninterpolation: ${torchvision_interpolation_mode:BICUBIC}\n- \"${lambda_fn:'lambda image: image.clamp(0.0, 1.0)'}\" # Bicubic interpolation can get out of range\n- _target_: torchvision.transforms.CenterCrop\nsize: 128\n- _target_: torchvision.transforms.Normalize\nmean: [0.485, 0.456, 0.406]\nstd: [0.229, 0.224, 0.225]\nbatch_transform: false\n</code></pre>"},{"location":"configs/experiment/projects/bridging/experiment_image_feature_rec/coco_feat_rec_dino_base16_fine/","title":"configs/experiment/projects/bridging/experiment_image_feature_rec/coco_feat_rec_dino_base16_fine.yaml","text":"<pre><code># @package _global_\ndefaults:\n- /experiment/projects/bridging/experiment_image_feature_rec/_base  # (1)!\n- /dataset: coco  # (2)!\n- /experiment/projects/bridging/experiment_image_feature_rec/_preprocessing_coco_dino_feature_recon # (3)!\n- /experiment/projects/bridging/dinosaur/_metrics_coco # (4)!\n- _self_\n# The following parameters assume training on 8 GPUs, leading to an effective batch size of 64.\ntrainer:\ndevices: 8\nmax_steps: 500000\nmax_epochs:\ndataset:\nnum_workers: 4\nbatch_size: 8\nmodels:\nconditioning:\n_target_: routed.ocl.conditioning.RandomConditioning\nn_slots: 7\nobject_dim: 256\nbatch_size_path: input.batch_size\nfeature_extractor:\nmodel_name: vit_base_patch16_224_dino\npretrained: ${when_testing:false,true}\nfreeze: false\nperceptual_grouping: {}\nobject_decoder:\n_target_: routed.ocl.decoding.PatchDecoder\nobject_dim: ${models.perceptual_grouping.object_dim}\noutput_dim: ${experiment.input_feature_dim}\nnum_patches: 196\nobject_features_path: perceptual_grouping.objects\ntarget_path: feature_extractor.features\nimage_path: input.image\ndecoder:\n_target_: ocl.neural_networks.build_mlp\n_partial_: true\nfeatures: [2048, 2048, 2048]\noptimizers:\nopt0:\n# Add parameter groups for different learning rates.\nparameter_groups:\n- params: models.feature_extractor\nlr: 0.00004\n- params: [models.conditioning, models.perceptual_grouping, models.object_decoder]\nlr: 0.0004\nexperiment:\ninput_feature_dim: 768\n</code></pre> <ol> <li>/experiment/projects/bridging/experiment_image_feature_rec/_base</li> <li>/dataset/coco</li> <li>/experiment/projects/bridging/experiment_image_feature_rec/_preprocessing_coco_dino_feature_recon</li> <li>/experiment/projects/bridging/dinosaur/_metrics_coco</li> </ol>"},{"location":"configs/experiment/projects/bridging/experiment_image_feature_rec/coco_feat_rec_dino_base16_frozen/","title":"configs/experiment/projects/bridging/experiment_image_feature_rec/coco_feat_rec_dino_base16_frozen.yaml","text":"<pre><code># @package _global_\ndefaults:\n- /experiment/projects/bridging/experiment_image_feature_rec/_base  # (1)!\n- /dataset: coco  # (2)!\n- /experiment/projects/bridging/experiment_image_feature_rec/_preprocessing_coco_dino_feature_recon # (3)!\n- /experiment/projects/bridging/dinosaur/_metrics_coco # (4)!\n- _self_\n# The following parameters assume training on 8 GPUs, leading to an effective batch size of 64.\ntrainer:\ndevices: 8\nmax_steps: 500000\nmax_epochs:\ndataset:\nnum_workers: 4\nbatch_size: 8\nmodels:\nconditioning:\n_target_: routed.ocl.conditioning.RandomConditioning\nn_slots: 7\nobject_dim: 256\nbatch_size_path: input.batch_size\nfeature_extractor:\nmodel_name: vit_base_patch16_224_dino\npretrained: ${when_testing:false,true}\nfreeze: true\nperceptual_grouping: {}\nobject_decoder:\n_target_: routed.ocl.decoding.PatchDecoder\nobject_dim: ${models.perceptual_grouping.object_dim}\noutput_dim: ${experiment.input_feature_dim}\nnum_patches: 196\nobject_features_path: perceptual_grouping.objects\ntarget_path: feature_extractor.features\nimage_path: input.image\ndecoder:\n_target_: ocl.neural_networks.build_mlp\n_partial_: true\nfeatures: [2048, 2048, 2048]\nexperiment:\ninput_feature_dim: 768\n</code></pre> <ol> <li>/experiment/projects/bridging/experiment_image_feature_rec/_base</li> <li>/dataset/coco</li> <li>/experiment/projects/bridging/experiment_image_feature_rec/_preprocessing_coco_dino_feature_recon</li> <li>/experiment/projects/bridging/dinosaur/_metrics_coco</li> </ol>"},{"location":"configs/experiment/projects/bridging/experiment_image_feature_rec/coco_feat_rec_dino_base16_resnet_scratch/","title":"configs/experiment/projects/bridging/experiment_image_feature_rec/coco_feat_rec_dino_base16_resnet_scratch.yaml","text":"<pre><code># @package _global_\ndefaults:\n- /experiment/projects/bridging/experiment_image_feature_rec/_base  # (1)!\n- /dataset: coco  # (2)!\n- /experiment/projects/bridging/experiment_image_feature_rec/_preprocessing_coco_dino_feature_recon # (3)!\n- /experiment/projects/bridging/dinosaur/_metrics_coco # (4)!\n- _self_\n# The following parameters assume training on 8 GPUs, leading to an effective batch size of 64.\ntrainer:\ndevices: 8\nmax_steps: 500000\nmax_epochs:\ndataset:\nnum_workers: 4\nbatch_size: 8\nmodels:\nconditioning:\n_target_: routed.ocl.conditioning.RandomConditioning\nn_slots: 7\nobject_dim: 256\nbatch_size_path: input.batch_size\nfeature_extractor:\nmodel_name: resnet34_savi\nfeature_level: 4\npretrained: false\nfreeze: false\nfeature_extractor_target:\n_target_: routed.ocl.feature_extractors.TimmFeatureExtractor\nmodel_name: vit_base_patch16_224_dino\npretrained: ${when_testing:false,true}\nfreeze: true\nfeature_level: 12\nvideo_path: input.image\nperceptual_grouping:\npositional_embedding:\n_target_: ocl.neural_networks.wrappers.Sequential\n_args_:\n- _target_: ocl.neural_networks.positional_embedding.SoftPositionEmbed\nn_spatial_dims: 2\nfeature_dim: 512\nsavi_style: true\n- _target_: ocl.neural_networks.build_two_layer_mlp\ninput_dim: 512\noutput_dim: ${models.perceptual_grouping.object_dim}\nhidden_dim: ${models.perceptual_grouping.object_dim}\ninitial_layer_norm: true\nobject_decoder:\n_target_: routed.ocl.decoding.PatchDecoder\nobject_dim: ${models.perceptual_grouping.object_dim}\noutput_dim: 768\nnum_patches: 196\nobject_features_path: perceptual_grouping.objects\ntarget_path: feature_extractor_target.features\nimage_path: input.image\ndecoder:\n_target_: ocl.neural_networks.build_mlp\n_partial_: true\nfeatures: [2048, 2048, 2048]\nexperiment:\ninput_feature_dim: 512\n</code></pre> <ol> <li>/experiment/projects/bridging/experiment_image_feature_rec/_base</li> <li>/dataset/coco</li> <li>/experiment/projects/bridging/experiment_image_feature_rec/_preprocessing_coco_dino_feature_recon</li> <li>/experiment/projects/bridging/dinosaur/_metrics_coco</li> </ol>"},{"location":"configs/experiment/projects/bridging/experiment_image_feature_rec/coco_feat_rec_dino_base16_scratch/","title":"configs/experiment/projects/bridging/experiment_image_feature_rec/coco_feat_rec_dino_base16_scratch.yaml","text":"<pre><code># @package _global_\ndefaults:\n- /experiment/projects/bridging/experiment_image_feature_rec/_base  # (1)!\n- /dataset: coco  # (2)!\n- /experiment/projects/bridging/experiment_image_feature_rec/_preprocessing_coco_dino_feature_recon # (3)!\n- /experiment/projects/bridging/dinosaur/_metrics_coco # (4)!\n- _self_\n# The following parameters assume training on 8 GPUs, leading to an effective batch size of 64.\ntrainer:\ndevices: 8\nmax_steps: 500000\nmax_epochs:\ndataset:\nnum_workers: 4\nbatch_size: 8\nmodels:\nconditioning:\n_target_: routed.ocl.conditioning.RandomConditioning\nn_slots: 7\nobject_dim: 256\nbatch_size_path: input.batch_size\nfeature_extractor:\nmodel_name: vit_base_patch16_224_dino\npretrained: false\nfreeze: false\nfeature_extractor_target:\n_target_: routed.ocl.feature_extractors.TimmFeatureExtractor\nmodel_name: vit_base_patch16_224_dino\npretrained: ${when_testing:false,true}\nfreeze: true\nfeature_level: 12\nvideo_path: input.image\nobject_decoder:\n_target_: routed.ocl.decoding.PatchDecoder\nobject_dim: ${models.perceptual_grouping.object_dim}\noutput_dim: ${experiment.input_feature_dim}\nnum_patches: 196\nobject_features_path: perceptual_grouping.objects\ntarget_path: feature_extractor_target.features\nimage_path: input.image\ndecoder:\n_target_: ocl.neural_networks.build_mlp\n_partial_: true\nfeatures: [2048, 2048, 2048]\nexperiment:\ninput_feature_dim: 768\n</code></pre> <ol> <li>/experiment/projects/bridging/experiment_image_feature_rec/_base</li> <li>/dataset/coco</li> <li>/experiment/projects/bridging/experiment_image_feature_rec/_preprocessing_coco_dino_feature_recon</li> <li>/experiment/projects/bridging/dinosaur/_metrics_coco</li> </ol>"},{"location":"configs/experiment/projects/bridging/experiment_image_feature_rec/coco_image_rec_base16_fine/","title":"configs/experiment/projects/bridging/experiment_image_feature_rec/coco_image_rec_base16_fine.yaml","text":"<pre><code># @package _global_\ndefaults:\n- /experiment/projects/bridging/experiment_image_feature_rec/_base  # (1)!\n- /dataset: coco  # (2)!\n- /experiment/projects/bridging/experiment_image_feature_rec/_preprocessing_coco_dino_image_recon # (3)!\n- /experiment/projects/bridging/experiment_image_feature_rec/_metrics_coco_image # (4)!\n- _self_\n# The following parameters assume training on 8 GPUs, leading to an effective batch size of 64.\ntrainer:\ndevices: 8\nmax_steps: 500000\nmax_epochs:\ndataset:\nnum_workers: 4\nbatch_size: 8\nmodels:\nconditioning:\n_target_: routed.ocl.conditioning.RandomConditioning\nn_slots: 7\nobject_dim: 256\nbatch_size_path: input.batch_size\nfeature_extractor:\nmodel_name: vit_base_patch16_224_dino\npretrained: ${when_testing:false,true}\nfreeze: false\nperceptual_grouping: {}\nobject_decoder:\n_target_: routed.ocl.decoding.SlotAttentionDecoder\nfinal_activation: tanh\ndecoder:\n_target_: ocl.decoding.get_savi_decoder_backbone\nobject_dim: ${models.perceptual_grouping.object_dim}\nlarger_input_arch: true\nchannel_multiplier: 1\npositional_embedding:\n_target_: ocl.neural_networks.positional_embedding.SoftPositionEmbed\nn_spatial_dims: 2\nfeature_dim: ${models.perceptual_grouping.object_dim}\ncnn_channel_order: true\nsavi_style: true\nobject_features_path: perceptual_grouping.objects\nlosses:\nmse:\n_target_: routed.ocl.losses.ReconstructionLoss\nloss_type: mse\ninput_path: object_decoder.reconstruction\ntarget_path: input.target\noptimizers:\nopt0:\n# Add parameter groups for different learning rates.\nparameter_groups:\n- params: models.feature_extractor\nlr: 0.00004\n- params: [models.conditioning, models.perceptual_grouping, models.object_decoder]\nlr: 0.0004\nvisualizations:\nreconstruction:\n_target_: routed.ocl.visualizations.Image\ndenormalization: \"${lambda_fn:'lambda t: t * 0.5 + 0.5'}\"\nimage_path: object_decoder.reconstruction\nmasks:\n_target_: routed.ocl.visualizations.Mask\nmask_path: object_decoder.masks\npred_segmentation:\n_target_: routed.ocl.visualizations.Segmentation\ndenormalization: \"${lambda_fn:'lambda t: t * 0.5 + 0.5'}\"\nimage_path: input.target\nmask_path: object_decoder.masks\nexperiment:\ninput_feature_dim: 768\n</code></pre> <ol> <li>/experiment/projects/bridging/experiment_image_feature_rec/_base</li> <li>/dataset/coco</li> <li>/experiment/projects/bridging/experiment_image_feature_rec/_preprocessing_coco_dino_image_recon</li> <li>/experiment/projects/bridging/experiment_image_feature_rec/_metrics_coco_image</li> </ol>"},{"location":"configs/experiment/projects/bridging/experiment_image_feature_rec/coco_image_rec_base16_frozen/","title":"configs/experiment/projects/bridging/experiment_image_feature_rec/coco_image_rec_base16_frozen.yaml","text":"<pre><code># @package _global_\ndefaults:\n- /experiment/projects/bridging/experiment_image_feature_rec/_base  # (1)!\n- /dataset: coco  # (2)!\n- /experiment/projects/bridging/experiment_image_feature_rec/_preprocessing_coco_dino_image_recon # (3)!\n- /experiment/projects/bridging/experiment_image_feature_rec/_metrics_coco_image # (4)!\n- _self_\n# The following parameters assume training on 8 GPUs, leading to an effective batch size of 64.\ntrainer:\ndevices: 8\nmax_steps: 500000\nmax_epochs:\ndataset:\nnum_workers: 4\nbatch_size: 8\nmodels:\nconditioning:\n_target_: routed.ocl.conditioning.RandomConditioning\nn_slots: 7\nobject_dim: 256\nbatch_size_path: input.batch_size\nfeature_extractor:\nmodel_name: vit_base_patch16_224_dino\npretrained: ${when_testing:false,true}\nfreeze: true\nperceptual_grouping: {}\nobject_decoder:\n_target_: routed.ocl.decoding.SlotAttentionDecoder\nfinal_activation: tanh\ndecoder:\n_target_: ocl.decoding.get_savi_decoder_backbone\nobject_dim: ${models.perceptual_grouping.object_dim}\nlarger_input_arch: true\nchannel_multiplier: 1\npositional_embedding:\n_target_: ocl.neural_networks.positional_embedding.SoftPositionEmbed\nn_spatial_dims: 2\nfeature_dim: ${models.perceptual_grouping.object_dim}\ncnn_channel_order: true\nsavi_style: true\nobject_features_path: perceptual_grouping.objects\nlosses:\nmse:\n_target_: routed.ocl.losses.ReconstructionLoss\nloss_type: mse\ninput_path: object_decoder.reconstruction\ntarget_path: input.target\nvisualizations:\nreconstruction:\n_target_: routed.ocl.visualizations.Image\ndenormalization: \"${lambda_fn:'lambda t: t * 0.5 + 0.5'}\"\nimage_path: object_decoder.reconstruction\nmasks:\n_target_: routed.ocl.visualizations.Mask\nmask_path: object_decoder.masks\npred_segmentation:\n_target_: routed.ocl.visualizations.Segmentation\ndenormalization: \"${lambda_fn:'lambda t: t * 0.5 + 0.5'}\"\nimage_path: input.target\nmask_path: object_decoder.masks\nexperiment:\ninput_feature_dim: 768\n</code></pre> <ol> <li>/experiment/projects/bridging/experiment_image_feature_rec/_base</li> <li>/dataset/coco</li> <li>/experiment/projects/bridging/experiment_image_feature_rec/_preprocessing_coco_dino_image_recon</li> <li>/experiment/projects/bridging/experiment_image_feature_rec/_metrics_coco_image</li> </ol>"},{"location":"configs/experiment/projects/bridging/experiment_image_feature_rec/coco_image_rec_base16_scratch/","title":"configs/experiment/projects/bridging/experiment_image_feature_rec/coco_image_rec_base16_scratch.yaml","text":"<pre><code># @package _global_\ndefaults:\n- /experiment/projects/bridging/experiment_image_feature_rec/_base  # (1)!\n- /dataset: coco  # (2)!\n- /experiment/projects/bridging/experiment_image_feature_rec/_preprocessing_coco_dino_image_recon # (3)!\n- /experiment/projects/bridging/experiment_image_feature_rec/_metrics_coco_image # (4)!\n- _self_\n# The following parameters assume training on 8 GPUs, leading to an effective batch size of 64.\ntrainer:\ndevices: 8\nmax_steps: 500000\nmax_epochs:\ndataset:\nnum_workers: 4\nbatch_size: 8\nmodels:\nconditioning:\n_target_: routed.ocl.conditioning.RandomConditioning\nn_slots: 7\nobject_dim: 256\nbatch_size_path: input.batch_size\nfeature_extractor:\nmodel_name: vit_base_patch16_224_dino\npretrained: false\nfreeze: false\nperceptual_grouping: {}\nobject_decoder:\n_target_: routed.ocl.decoding.SlotAttentionDecoder\nfinal_activation: tanh\ndecoder:\n_target_: ocl.decoding.get_savi_decoder_backbone\nobject_dim: ${models.perceptual_grouping.object_dim}\nlarger_input_arch: true\nchannel_multiplier: 1\npositional_embedding:\n_target_: ocl.neural_networks.positional_embedding.SoftPositionEmbed\nn_spatial_dims: 2\nfeature_dim: ${models.perceptual_grouping.object_dim}\ncnn_channel_order: true\nsavi_style: true\nobject_features_path: perceptual_grouping.objects\nlosses:\nmse:\n_target_: routed.ocl.losses.ReconstructionLoss\nloss_type: mse\ninput_path: object_decoder.reconstruction\ntarget_path: input.target\nvisualizations:\nreconstruction:\n_target_: routed.ocl.visualizations.Image\ndenormalization: \"${lambda_fn:'lambda t: t * 0.5 + 0.5'}\"\nimage_path: object_decoder.reconstruction\nmasks:\n_target_: routed.ocl.visualizations.Mask\nmask_path: object_decoder.masks\npred_segmentation:\n_target_: routed.ocl.visualizations.Segmentation\ndenormalization: \"${lambda_fn:'lambda t: t * 0.5 + 0.5'}\"\nimage_path: input.target\nmask_path: object_decoder.masks\nexperiment:\ninput_feature_dim: 768\n</code></pre> <ol> <li>/experiment/projects/bridging/experiment_image_feature_rec/_base</li> <li>/dataset/coco</li> <li>/experiment/projects/bridging/experiment_image_feature_rec/_preprocessing_coco_dino_image_recon</li> <li>/experiment/projects/bridging/experiment_image_feature_rec/_metrics_coco_image</li> </ol>"},{"location":"configs/experiment/projects/bridging/slot_attention/_base_large/","title":"configs/experiment/projects/bridging/slot_attention/_base_large.yaml","text":"<pre><code># @package _global_\n# Default parameters for slot attention on resolution 128x128 with a ResNet encoder\ndefaults:\n- /experiment/_output_path  # (1)!\n- /training_config # (2)!\n- _self_\nmodels:\nfeature_extractor:\n_target_: routed.ocl.feature_extractors.TimmFeatureExtractor\nmodel_name: resnet34_savi\nfeature_level: 4\npretrained: false\nfreeze: false\nvideo_path: input.image\nconditioning:\nperceptual_grouping:\n_target_: routed.ocl.perceptual_grouping.SlotAttentionGrouping\nfeature_dim: ${models.perceptual_grouping.object_dim}\nobject_dim: ${models.conditioning.object_dim}\nkvq_dim: ${models.perceptual_grouping.object_dim}\npositional_embedding:\n_target_: ocl.neural_networks.wrappers.Sequential\n_args_:\n- _target_: ocl.neural_networks.positional_embedding.SoftPositionEmbed\nn_spatial_dims: 2\nfeature_dim: 512\nsavi_style: true\n- _target_: ocl.neural_networks.build_two_layer_mlp\ninput_dim: 512\noutput_dim: ${models.perceptual_grouping.object_dim}\nhidden_dim: ${models.perceptual_grouping.object_dim}\ninitial_layer_norm: true\nff_mlp:\n_target_: ocl.neural_networks.build_two_layer_mlp\ninput_dim: ${models.perceptual_grouping.object_dim}\noutput_dim: ${models.perceptual_grouping.object_dim}\nhidden_dim: \"${eval_lambda:'lambda dim: 2 * dim', ${.input_dim}}\"\ninitial_layer_norm: true\nresidual: true\nfeature_path: feature_extractor\nconditioning_path: conditioning\nobject_decoder:\n_target_: routed.ocl.decoding.SlotAttentionDecoder\nfinal_activation: tanh\ndecoder:\n_target_: ocl.decoding.get_savi_decoder_backbone\nobject_dim: ${models.perceptual_grouping.object_dim}\nlarger_input_arch: true\nchannel_multiplier: 1\npositional_embedding:\n_target_: ocl.neural_networks.positional_embedding.SoftPositionEmbed\nn_spatial_dims: 2\nfeature_dim: ${models.perceptual_grouping.object_dim}\ncnn_channel_order: true\nsavi_style: true\nobject_features_path: perceptual_grouping.objects\nlosses:\nmse:\n_target_: routed.ocl.losses.ReconstructionLoss\nloss_type: mse\ninput_path: object_decoder.reconstruction\ntarget_path: input.image\nvisualizations:\ninput:\n_target_: routed.ocl.visualizations.Image\ndenormalization: \"${lambda_fn:'lambda t: t * 0.5 + 0.5'}\"\nimage_path: input.image\nreconstruction:\n_target_: routed.ocl.visualizations.Image\ndenormalization: ${..input.denormalization}\nimage_path: object_decoder.reconstruction\nobjects:\n_target_: routed.ocl.visualizations.VisualObject\ndenormalization: ${..input.denormalization}\nobject_path: object_decoder.object_reconstructions\nmask_path: object_decoder.masks\npred_segmentation:\n_target_: routed.ocl.visualizations.Segmentation\ndenormalization: ${..input.denormalization}\nimage_path: input.image\nmask_path: object_decoder.masks\noptimizers:\nopt0:\n_target_: ocl.optimization.OptimizationWrapper\noptimizer:\n_target_: torch.optim.Adam\n_partial_: true\nlr: 0.0002\nlr_scheduler:\n_target_: ocl.scheduling.cosine_annealing_with_optional_warmup\n_partial_: true\nwarmup_steps: 2500\nT_max: ${trainer.max_steps}\n</code></pre> <ol> <li>/experiment/_output_path</li> <li>/training_config</li> </ol>"},{"location":"configs/experiment/projects/bridging/slot_attention/_metrics_clevr/","title":"configs/experiment/projects/bridging/slot_attention/_metrics_clevr.yaml","text":"<pre><code># @package _global_\n# Metrics for CLEVR-like datasets\nevaluation_metrics:\nari:\n_target_: routed.ocl.metrics.ARIMetric\nprediction_path: object_decoder.masks\ntarget_path: input.mask\nabo:\n_target_: routed.ocl.metrics.AverageBestOverlapMetric\nprediction_path: object_decoder.masks\ntarget_path: input.mask\nignore_background: true\n</code></pre>"},{"location":"configs/experiment/projects/bridging/slot_attention/_metrics_coco/","title":"configs/experiment/projects/bridging/slot_attention/_metrics_coco.yaml","text":"<pre><code># @package _global_\n# Metrics for COCO-like datasets\nevaluation_metrics:\ninstance_mask_ari:\n_target_: routed.ocl.metrics.ARIMetric\nprediction_path: object_decoder.masks\ntarget_path: input.instance_mask\nforeground: false\nconvert_target_one_hot: true\nignore_overlaps: true\ninstance_mask_iou:\n_target_: routed.ocl.metrics.UnsupervisedMaskIoUMetric\nprediction_path: object_decoder.masks\ntarget_path: input.instance_mask\nignore_overlaps: true\nsegmentation_mask_iou:\n_target_: routed.ocl.metrics.UnsupervisedMaskIoUMetric\nprediction_path: object_decoder.masks\ntarget_path: input.segmentation_mask\ninstance_mask_abo:\n_target_: routed.ocl.metrics.AverageBestOverlapMetric\nprediction_path: object_decoder.masks\ntarget_path: input.instance_mask\nignore_overlaps: true\nsegmentation_mask_abo:\n_target_: routed.ocl.metrics.AverageBestOverlapMetric\nprediction_path: object_decoder.masks\ntarget_path: input.segmentation_mask\ninstance_mask_corloc:\n_target_: routed.ocl.metrics.MaskCorLocMetric\nprediction_path: object_decoder.masks\ntarget_path: input.instance_mask\nuse_threshold: false\nignore_overlaps: true\n</code></pre>"},{"location":"configs/experiment/projects/bridging/slot_attention/_preprocessing_coco/","title":"configs/experiment/projects/bridging/slot_attention/_preprocessing_coco.yaml","text":"<pre><code># @package _global_\ndataset:\neval_transforms:\n03a_preprocessing:\n_target_: ocl.transforms.Map\ntransform:\n_target_: torchvision.transforms.Compose\ntransforms:\n- _target_: ocl.preprocessing.InstanceMasksToDenseMasks\n- _target_: ocl.preprocessing.AddSegmentationMaskFromInstanceMask\n- _target_: ocl.preprocessing.AddEmptyMasks\nmask_keys:\n- instance_mask\n- segmentation_mask\n# Drop instance_category again as some images do not contain it\n- _target_: ocl.preprocessing.DropEntries\nkeys:\n- instance_category\nfields:\n- image\n- instance_mask\n- instance_category\nbatch_transform: false\n03b_preprocessing:\n_target_: ocl.transforms.SimpleTransform\ntransforms:\nimage:\n_target_: torchvision.transforms.Compose\ntransforms:\n- _target_: torchvision.transforms.ToTensor\n- _target_: torchvision.transforms.Resize\nsize: 128\ninterpolation: ${torchvision_interpolation_mode:BILINEAR}\n- _target_: torchvision.transforms.CenterCrop\nsize: 128\n- _target_: torchvision.transforms.Normalize\nmean: [0.5, 0.5, 0.5]\nstd: [0.5, 0.5, 0.5]\ninstance_mask:\n_target_: torchvision.transforms.Compose\ntransforms:\n- _target_: ocl.preprocessing.DenseMaskToTensor\n- _target_: ocl.preprocessing.ResizeNearestExact\nsize: 128\n- _target_: torchvision.transforms.CenterCrop\nsize: 128\nsegmentation_mask:\n_target_: torchvision.transforms.Compose\ntransforms:\n- _target_: ocl.preprocessing.DenseMaskToTensor\n- _target_: ocl.preprocessing.ResizeNearestExact\nsize: 128\n- _target_: torchvision.transforms.CenterCrop\nsize: 128\nbatch_transform: false\ntrain_transforms:\n03b_preprocessing:\n_target_: ocl.transforms.SimpleTransform\ntransforms:\nimage:\n_target_: torchvision.transforms.Compose\ntransforms:\n- _target_: torchvision.transforms.ToTensor\n- _target_: torchvision.transforms.Resize\nsize: 128\ninterpolation: ${torchvision_interpolation_mode:BILINEAR}\n- _target_: torchvision.transforms.RandomCrop\nsize: 128\n- _target_: torchvision.transforms.RandomHorizontalFlip\n- _target_: torchvision.transforms.Normalize\nmean: [0.5, 0.5, 0.5]\nstd: [0.5, 0.5, 0.5]\nbatch_transform: false\n</code></pre>"},{"location":"configs/experiment/projects/bridging/slot_attention/_preprocessing_movi/","title":"configs/experiment/projects/bridging/slot_attention/_preprocessing_movi.yaml","text":"<pre><code># @package _global_\ndataset:\ntrain_transforms:\n03_preprocessing:\n_target_: ocl.transforms.SimpleTransform\ntransforms:\nimage:\n_target_: torchvision.transforms.Compose\ntransforms:\n- _target_: torchvision.transforms.ToTensor\n- _target_: torchvision.transforms.Resize\nsize: 128\n- _target_: torchvision.transforms.Normalize\nmean: [0.5, 0.5, 0.5]\nstd: [0.5, 0.5, 0.5]\nbatch_transform: false\neval_transforms:\n03_preprocessing:\n_target_: ocl.transforms.SimpleTransform\ntransforms:\nimage:\n_target_: torchvision.transforms.Compose\ntransforms:\n- _target_: torchvision.transforms.ToTensor\n- _target_: torchvision.transforms.Resize\nsize: 128\n- _target_: torchvision.transforms.Normalize\nmean: [0.5, 0.5, 0.5]\nstd: [0.5, 0.5, 0.5]\nmask:\n_target_: torchvision.transforms.Compose\ntransforms:\n- _target_: ocl.preprocessing.MultiMaskToTensor\n- _target_: ocl.preprocessing.ResizeNearestExact\nsize: 128\nbatch_transform: false\n</code></pre>"},{"location":"configs/experiment/projects/bridging/slot_attention/_preprocessing_voc2012_trainaug/","title":"configs/experiment/projects/bridging/slot_attention/_preprocessing_voc2012_trainaug.yaml","text":"<pre><code># @package _global_\ndataset:\neval_transforms:\n02a_format_consistency:\n_target_: ocl.transforms.SimpleTransform\ntransforms:\n# Convert to one-hot encoding.\nsegmentation-instance:\n_target_: ocl.preprocessing.IntegerToOneHotMask\nbatch_transform: false\n02b_format_consistency:\n_target_: ocl.transforms.Map\ntransform:\n_target_: torchvision.transforms.Compose\ntransforms:\n# Create segmentation mask.\n- _target_: ocl.preprocessing.VOCInstanceMasksToDenseMasks\ninstance_mask_key: segmentation-instance\nclass_mask_key: segmentation-class\nclasses_key: instance_category\n- _target_: ocl.preprocessing.RenameFields\nmapping:\nsegmentation-instance: instance_mask\nfields:\n- segmentation-instance\n- segmentation-class\n- image\nbatch_transform: false\n03a_preprocessing:\n_target_: ocl.transforms.Map\ntransform:\n_target_: torchvision.transforms.Compose\ntransforms:\n# This is not needed for VOC.\n# - _target_: ocl.preprocessing.InstanceMasksToDenseMasks\n- _target_: ocl.preprocessing.AddSegmentationMaskFromInstanceMask\n# Drop instance_category again as some images do not contain it\n- \"${lambda_fn:'lambda data: {k: v for k, v in data.items() if k != \\\"instance_category\\\"\\\n}'}\"\n- _target_: ocl.preprocessing.AddEmptyMasks\nmask_keys:\n- instance_mask\n- segmentation_mask\nfields:\n- image\n- instance_mask\n- instance_category\nbatch_transform: false\n03b_preprocessing:\n_target_: ocl.transforms.SimpleTransform\ntransforms:\nimage:\n_target_: torchvision.transforms.Compose\ntransforms:\n- _target_: torchvision.transforms.ToTensor\n- _target_: torchvision.transforms.Resize\nsize: 128\ninterpolation: ${torchvision_interpolation_mode:BILINEAR}\n- _target_: torchvision.transforms.CenterCrop\nsize: 128\n- _target_: torchvision.transforms.Normalize\nmean: [0.5, 0.5, 0.5]\nstd: [0.5, 0.5, 0.5]\ninstance_mask:\n_target_: torchvision.transforms.Compose\ntransforms:\n- _target_: ocl.preprocessing.DenseMaskToTensor\n- _target_: ocl.preprocessing.ResizeNearestExact\nsize: 128\n- _target_: torchvision.transforms.CenterCrop\nsize: 128\nsegmentation_mask:\n_target_: torchvision.transforms.Compose\ntransforms:\n- _target_: ocl.preprocessing.DenseMaskToTensor\n- _target_: ocl.preprocessing.ResizeNearestExact\nsize: 128\n- _target_: torchvision.transforms.CenterCrop\nsize: 128\nbatch_transform: false\ntrain_transforms:\n03b_preprocessing:\n_target_: ocl.transforms.SimpleTransform\ntransforms:\nimage:\n_target_: torchvision.transforms.Compose\ntransforms:\n- _target_: torchvision.transforms.ToTensor\n- _target_: torchvision.transforms.Resize\nsize: 128\ninterpolation: ${torchvision_interpolation_mode:BILINEAR}\n- _target_: torchvision.transforms.RandomCrop\nsize: 128\n- _target_: torchvision.transforms.RandomHorizontalFlip\n- _target_: torchvision.transforms.Normalize\nmean: [0.5, 0.5, 0.5]\nstd: [0.5, 0.5, 0.5]\nbatch_transform: false\n</code></pre>"},{"location":"configs/experiment/projects/bridging/slot_attention/coco/","title":"configs/experiment/projects/bridging/slot_attention/coco.yaml","text":"<pre><code># @package _global_\ndefaults:\n- /experiment/projects/bridging/slot_attention/_base_large  # (1)!\n- /dataset: coco  # (2)!\n- /experiment/projects/bridging/slot_attention/_preprocessing_coco # (3)!\n- /experiment/projects/bridging/slot_attention/_metrics_coco # (4)!\n- _self_\ntrainer:\ndevices: 8\nmax_steps: 500000\nmax_epochs:\ngradient_clip_val: 1.0\ndataset:\nnum_workers: 4\nbatch_size: 8\nmodels:\nconditioning:\n_target_: routed.ocl.conditioning.RandomConditioning\nobject_dim: 256\nn_slots: 11\nbatch_size_path: input.batch_size\n</code></pre> <ol> <li>/experiment/projects/bridging/slot_attention/_base_large</li> <li>/dataset/coco</li> <li>/experiment/projects/bridging/slot_attention/_preprocessing_coco</li> <li>/experiment/projects/bridging/slot_attention/_metrics_coco</li> </ol>"},{"location":"configs/experiment/projects/bridging/slot_attention/movi_c/","title":"configs/experiment/projects/bridging/slot_attention/movi_c.yaml","text":"<pre><code># @package _global_\ndefaults:\n- /experiment/projects/bridging/slot_attention/_base_large  # (1)!\n- /dataset: movi_c_image  # (2)!\n- /experiment/projects/bridging/slot_attention/_preprocessing_movi # (3)!\n- /experiment/projects/bridging/slot_attention/_metrics_clevr # (4)!\n- _self_\ntrainer:\ndevices: 8\nmax_steps: 500000\nmax_epochs:\ngradient_clip_val: 1.0\ndataset:\nnum_workers: 4\nbatch_size: 8\nmodels:\nconditioning:\n_target_: routed.ocl.conditioning.RandomConditioning\nobject_dim: 256\nn_slots: 11\nbatch_size_path: input.batch_size\n</code></pre> <ol> <li>/experiment/projects/bridging/slot_attention/_base_large</li> <li>/dataset/movi_c_image</li> <li>/experiment/projects/bridging/slot_attention/_preprocessing_movi</li> <li>/experiment/projects/bridging/slot_attention/_metrics_clevr</li> </ol>"},{"location":"configs/experiment/projects/bridging/slot_attention/movi_e/","title":"configs/experiment/projects/bridging/slot_attention/movi_e.yaml","text":"<pre><code># @package _global_\ndefaults:\n- /experiment/projects/bridging/slot_attention/_base_large  # (1)!\n- /dataset: movi_e_image  # (2)!\n- /experiment/projects/bridging/slot_attention/_preprocessing_movi # (3)!\n- /experiment/projects/bridging/slot_attention/_metrics_clevr # (4)!\n- _self_\ntrainer:\ndevices: 8\nmax_steps: 500000\nmax_epochs:\ngradient_clip_val: 1.0\ndataset:\nnum_workers: 4\nbatch_size: 8\nmodels:\nconditioning:\n_target_: routed.ocl.conditioning.RandomConditioning\nobject_dim: 256\nn_slots: 24\nbatch_size_path: input.batch_size\n</code></pre> <ol> <li>/experiment/projects/bridging/slot_attention/_base_large</li> <li>/dataset/movi_e_image</li> <li>/experiment/projects/bridging/slot_attention/_preprocessing_movi</li> <li>/experiment/projects/bridging/slot_attention/_metrics_clevr</li> </ol>"},{"location":"configs/experiment/projects/bridging/slot_attention/voc2012_trainaug/","title":"configs/experiment/projects/bridging/slot_attention/voc2012_trainaug.yaml","text":"<pre><code># @package _global_\ndefaults:\n- /experiment/projects/bridging/slot_attention/_base_large  # (1)!\n- /dataset: voc2012_trainaug  # (2)!\n- /experiment/projects/bridging/slot_attention/_preprocessing_voc2012_trainaug # (3)!\n- /experiment/projects/bridging/slot_attention/_metrics_coco # (4)!\n- _self_\ntrainer:\ndevices: 8\nmax_steps: 500000\nmax_epochs:\ngradient_clip_val: 1.0\ncheck_val_every_n_epoch: 50\ndataset:\nnum_workers: 4\nbatch_size: 8\nmodels:\nconditioning:\n_target_: routed.ocl.conditioning.RandomConditioning\nobject_dim: 256\nn_slots: 11\nbatch_size_path: input.batch_size\n</code></pre> <ol> <li>/experiment/projects/bridging/slot_attention/_base_large</li> <li>/dataset/voc2012_trainaug</li> <li>/experiment/projects/bridging/slot_attention/_preprocessing_voc2012_trainaug</li> <li>/experiment/projects/bridging/slot_attention/_metrics_coco</li> </ol>"},{"location":"configs/experiment/slate/_base/","title":"configs/experiment/slate/_base.yaml","text":"<pre><code># @package _global_\n# Default parameters for SLATE.\ndefaults:\n- /experiment/_output_path  # (1)!\n- /training_config # (2)!\n- _self_\ntrainer:\ngradient_clip_val: 1.0\nexperiment:\ncallbacks:\nupdate_hps:\n_target_: ocl.callbacks.UpdateHyperparameterScheduling\nmodels:\nfeature_extractor:\n_target_: routed.ocl.feature_extractors.DVAEFeatureExtractor\nencoder:\n_target_: ocl.decoding.get_dvae_encoder\npatch_size: 4\nvocab_size: 4096\ntau:\n_target_: ocl.scheduling.CosineAnnealingHPScheduler\nstart_value: 1.0\nend_value: 0.1\nstart_step: 0\nend_step: 30000\nhard: false\ndictionary:\n_target_: ocl.neural_networks.slate.OneHotDictionary\nvocab_size: \"${eval_lambda:'lambda dim: dim + 1', ${..encoder.vocab_size}}\"\nemb_size: 192\npositional_encoder:\n_target_: ocl.neural_networks.positional_embedding.LearnedAdditivePositionalEmbed\nmax_len: \"${eval_lambda:'lambda x: x + 1', ${models.object_decoder.num_patches}}\"\nd_model: 192\ndropout: 0.1\nvideo_path: input.image\nconditioning:\n_target_: routed.ocl.conditioning.RandomConditioning\nobject_dim: 192\nbatch_size_path: input.batch_size\nperceptual_grouping:\n_target_: routed.ocl.perceptual_grouping.SlotAttentionGrouping\nfeature_dim: 192\nobject_dim: ${..conditioning.object_dim}\niters: 7\nff_mlp:\n_target_: ocl.neural_networks.build_two_layer_mlp\ninput_dim: ${..object_dim}\noutput_dim: ${..object_dim}\nhidden_dim: ${..object_dim}\ninitial_layer_norm: true\nresidual: true\nfeature_path: feature_extractor\nconditioning_path: conditioning\ndecoder_dvae:\n_target_: routed.ocl.decoding.DVAEDecoder\nfeatures_path: feature_extractor.aux_features.z\ndecoder:\n_target_: ocl.decoding.get_dvae_decoder\nvocab_size: ${models.feature_extractor.encoder.vocab_size}\nobject_decoder:\n_target_: routed.ocl.decoding.AutoregressivePatchDecoder\nobject_dim: ${models.perceptual_grouping.object_dim}\ndecoder_dim: ${models.perceptual_grouping.object_dim}\ndecoder_cond_dim: ${models.perceptual_grouping.object_dim}\noutput_dim: ${models.feature_extractor.encoder.vocab_size}\nnum_patches: 1024\nobject_features_path: perceptual_grouping.objects\ntarget_path: feature_extractor.aux_features.targets\nimage_path: input.image\nuse_bos_token: false\nuse_output_transform: true\nuse_input_norm: true\ndecoder:\n_target_: ocl.neural_networks.build_transformer_decoder\n_partial_: true\ndropout: ${models.feature_extractor.positional_encoder.dropout}\nn_layers: 8\nn_heads: 8\nmasks_path: perceptual_grouping.feature_attributions\nlosses:\nvq_ce:\n_target_: routed.ocl.losses.ReconstructionLoss\nloss_type: cross_entropy_sum\ninput_path: object_decoder.reconstruction\ntarget_path: feature_extractor.aux_features.z_hard\nvq_mse:\n_target_: routed.ocl.losses.ReconstructionLoss\ninput_path: decoder_dvae.reconstruction\ntarget_path: input.image\nloss_type: mse_sum\nvisualizations:\ninput:\n_target_: routed.ocl.visualizations.Image\ndenormalization: \"${lambda_fn:'lambda t: t * 0.5 + 0.5'}\"\nimage_path: input.image\nreconstruction:\n_target_: routed.ocl.visualizations.Image\ndenormalization: ${..input.denormalization}\nimage_path: decoder_dvae.reconstruction\nmasks:\n_target_: routed.ocl.visualizations.Mask\nmask_path: object_decoder.masks_as_image\noptimizers:\nopt0:\n_target_: ocl.optimization.OptimizationWrapper\noptimizer:\n_target_: torch.optim.Adam\n_partial_: true\nlr: 0.0003\nlr_scheduler:\n_target_: ocl.scheduling.exponential_decay_after_optional_warmup\n_partial_: true\ndecay_rate: 0.5\ndecay_steps: 250000\nwarmup_steps: 30000\n</code></pre> <ol> <li>/experiment/_output_path</li> <li>/training_config</li> </ol>"},{"location":"configs/experiment/slate/_base_large/","title":"configs/experiment/slate/_base_large.yaml","text":"<pre><code># @package _global_\n# Default parameters for SLATE.\ndefaults:\n- /experiment/_output_path  # (1)!\n- /training_config # (2)!\n- _self_\ntrainer:\ngradient_clip_val: 1.0\nexperiment:\ncallbacks:\nupdate_hps:\n_target_: ocl.callbacks.UpdateHyperparameterScheduling\nmodels:\nfeature_extractor:\n_target_: routed.ocl.feature_extractors.DVAEFeatureExtractor\nencoder:\n_target_: ocl.decoding.get_dvae_encoder\npatch_size: 4\nvocab_size: 4096\ntau:\n_target_: ocl.scheduling.CosineAnnealingHPScheduler\nstart_value: 1.0\nend_value: 0.1\nstart_step: 0\nend_step: 30000\nhard: false\ndictionary:\n_target_: ocl.neural_networks.slate.OneHotDictionary\nvocab_size: \"${eval_lambda:'lambda dim: dim + 1', ${..encoder.vocab_size}}\"\nemb_size: 192\npositional_encoder:\n_target_: ocl.neural_networks.positional_embedding.LearnedAdditivePositionalEmbed\nmax_len: \"${eval_lambda:'lambda x: x + 1', ${models.object_decoder.num_patches}}\"\nd_model: 192\ndropout: 0.1\nvideo_path: input.image\nconditioning:\n_target_: routed.ocl.conditioning.RandomConditioning\nobject_dim: 192\nbatch_size_path: input.batch_size\nperceptual_grouping:\n_target_: routed.ocl.perceptual_grouping.SlotAttentionGrouping\nfeature_dim: 192\nobject_dim: ${..conditioning.object_dim}\niters: 7\nff_mlp:\n_target_: ocl.neural_networks.build_two_layer_mlp\ninput_dim: ${..object_dim}\noutput_dim: ${..object_dim}\nhidden_dim: ${..object_dim}\ninitial_layer_norm: true\nresidual: true\nfeature_path: feature_extractor\nconditioning_path: conditioning\ndecoder_dvae:\n_target_: routed.ocl.decoding.DVAEDecoder\nfeatures_path: feature_extractor.aux_features.z\ndecoder:\n_target_: ocl.decoding.get_dvae_decoder\nvocab_size: ${models.feature_extractor.encoder.vocab_size}\nobject_decoder:\n_target_: routed.ocl.decoding.AutoregressivePatchDecoder\nobject_dim: ${models.perceptual_grouping.object_dim}\ndecoder_dim: ${models.perceptual_grouping.object_dim}\ndecoder_cond_dim: ${models.perceptual_grouping.object_dim}\noutput_dim: ${models.feature_extractor.encoder.vocab_size}\nnum_patches: 1024\nobject_features_path: perceptual_grouping.objects\ntarget_path: feature_extractor.aux_features.targets\nimage_path: input.image\nuse_bos_token: false\nuse_output_transform: true\nuse_input_norm: true\ndecoder:\n_target_: ocl.neural_networks.build_transformer_decoder\n_partial_: true\ndropout: ${models.feature_extractor.positional_encoder.dropout}\nn_layers: 8\nn_heads: 8\nmasks_path: perceptual_grouping.feature_attributions\nlosses:\nvq_ce:\n_target_: routed.ocl.losses.ReconstructionLoss\nloss_type: cross_entropy_sum\ninput_path: object_decoder.reconstruction\ntarget_path: feature_extractor.aux_features.z_hard\nvq_mse:\n_target_: routed.ocl.losses.ReconstructionLoss\ninput_path: decoder_dvae.reconstruction\ntarget_path: input.image\nloss_type: mse_sum\nvisualizations:\ninput:\n_target_: routed.ocl.visualizations.Image\ndenormalization: \"${lambda_fn:'lambda t: t * 0.5 + 0.5'}\"\nimage_path: input.image\nreconstruction:\n_target_: routed.ocl.visualizations.Image\ndenormalization: ${..input.denormalization}\nimage_path: decoder_dvae.reconstruction\nmasks:\n_target_: routed.ocl.visualizations.Mask\nmask_path: object_decoder.masks_as_image\noptimizers:\nopt0:\n_target_: ocl.optimization.OptimizationWrapper\noptimizer:\n_target_: torch.optim.Adam\n_partial_: true\nlr: 0.0003\nlr_scheduler:\n_target_: ocl.scheduling.exponential_decay_after_optional_warmup\n_partial_: true\ndecay_rate: 0.5\ndecay_steps: 250000\nwarmup_steps: 30000\n</code></pre> <ol> <li>/experiment/_output_path</li> <li>/training_config</li> </ol>"},{"location":"configs/experiment/slate/_clevr_preprocessing/","title":"configs/experiment/slate/_clevr_preprocessing.yaml","text":"<pre><code># @package _global_\ndataset:\ntrain_transforms:\n03_preprocessing:\n_target_: ocl.transforms.SimpleTransform\ntransforms:\nimage:\n_target_: torchvision.transforms.Compose\ntransforms:\n- _target_: torchvision.transforms.ToTensor\n- _target_: torchvision.transforms.CenterCrop\nsize: [192, 192]\n- _target_: torchvision.transforms.Resize\nsize: 128\n- _target_: torchvision.transforms.Normalize\nmean: [0.5, 0.5, 0.5]\nstd: [0.5, 0.5, 0.5]\nbatch_transform: false\neval_transforms:\n03_preprocessing:\n_target_: ocl.transforms.SimpleTransform\ntransforms:\nimage:\n_target_: torchvision.transforms.Compose\ntransforms:\n- _target_: torchvision.transforms.ToTensor\n- _target_: torchvision.transforms.CenterCrop\nsize: [192, 192]\n- _target_: torchvision.transforms.Resize\nsize: 128\n- _target_: torchvision.transforms.Normalize\nmean: [0.5, 0.5, 0.5]\nstd: [0.5, 0.5, 0.5]\nmask:\n_target_: torchvision.transforms.Compose\ntransforms:\n- _target_: ocl.preprocessing.MaskToTensor\n- _target_: torchvision.transforms.CenterCrop\nsize: [192, 192]\n- _target_: ocl.preprocessing.ResizeNearestExact\nsize: 128\nbatch_transform: false\n</code></pre>"},{"location":"configs/experiment/slate/_metrics_clevr_patch/","title":"configs/experiment/slate/_metrics_clevr_patch.yaml","text":"<pre><code># @package _global_\nevaluation_metrics:\nari:\n_target_: routed.ocl.metrics.ARIMetric\nprediction_path: masks_as_image\ntarget_path: input.mask\nabo:\n_target_: routed.ocl.metrics.AverageBestOverlapMetric\nprediction_path: masks_as_image\ntarget_path: input.mask\nignore_background: true\n</code></pre>"},{"location":"configs/experiment/slate/_metrics_masks/","title":"configs/experiment/slate/_metrics_masks.yaml","text":"<pre><code># @package _global_\nevaluation_metrics:\ninstance_mask_ari:\n_target_: routed.ocl.metrics.ARIMetric\nprediction_path: object_decoder.masks_as_image\ntarget_path: input.instance_mask\nforeground: false\nconvert_target_one_hot: true\ninstance_mask_iou:\n_target_: routed.ocl.metrics.UnsupervisedMaskIoUMetric\nprediction_path: object_decoder.masks_as_image\ntarget_path: input.instance_mask\nignore_overlaps: true\nsegmentation_mask_iou:\n_target_: routed.ocl.metrics.UnsupervisedMaskIoUMetric\nprediction_path: object_decoder.masks_as_image\ntarget_path: input.segmentation_mask\nignore_overlaps: true\ninstance_mask_abo:\n_target_: routed.ocl.metrics.AverageBestOverlapMetric\nprediction_path: object_decoder.masks_as_image\ntarget_path: input.instance_mask\nignore_overlaps: true\nsegmentation_mask_abo:\n_target_: routed.ocl.metrics.AverageBestOverlapMetric\nprediction_path: object_decoder.masks_as_image\ntarget_path: input.segmentation_mask\nignore_overlaps: true\ninstance_mask_corloc:\n_target_: routed.ocl.metrics.MaskCorLocMetric\nprediction_path: object_decoder.masks_as_image\ntarget_path: input.instance_mask\nignore_overlaps: true\n</code></pre>"},{"location":"configs/experiment/slate/_movi_preprocessing/","title":"configs/experiment/slate/_movi_preprocessing.yaml","text":"<pre><code># @package _global_\ndataset:\ntrain_transforms:\n03_preprocessing:\n_target_: ocl.transforms.SimpleTransform\ntransforms:\nimage:\n_target_: torchvision.transforms.Compose\ntransforms:\n- _target_: torchvision.transforms.ToTensor\n- _target_: torchvision.transforms.Resize\nsize: 128\n- _target_: torchvision.transforms.Normalize\nmean: [0.5, 0.5, 0.5]\nstd: [0.5, 0.5, 0.5]\nbatch_transform: false\neval_transforms:\n03_preprocessing:\n_target_: ocl.transforms.SimpleTransform\ntransforms:\nimage:\n_target_: torchvision.transforms.Compose\ntransforms:\n- _target_: torchvision.transforms.ToTensor\n- _target_: torchvision.transforms.Resize\nsize: 128\n- _target_: torchvision.transforms.Normalize\nmean: [0.5, 0.5, 0.5]\nstd: [0.5, 0.5, 0.5]\nmask:\n_target_: torchvision.transforms.Compose\ntransforms:\n- _target_: ocl.preprocessing.MultiMaskToTensor\n- _target_: ocl.preprocessing.ResizeNearestExact\nsize: 128\nbatch_transform: false\n</code></pre>"},{"location":"configs/experiment/slate/_preprocessing_coco/","title":"configs/experiment/slate/_preprocessing_coco.yaml","text":"<pre><code># @package _global_\ndataset:\neval_transforms:\n03a_preprocessing:\n_target_: ocl.transforms.Map\ntransform:\n_target_: torchvision.transforms.Compose\ntransforms:\n- _target_: ocl.preprocessing.InstanceMasksToDenseMasks\n- _target_: ocl.preprocessing.AddSegmentationMaskFromInstanceMask\n- _target_: ocl.preprocessing.AddEmptyMasks\nmask_keys:\n- instance_mask\n- segmentation_mask\n# Drop instance_category again as some images do not contain it\n- _target_: ocl.preprocessing.DropEntries\nkeys:\n- instance_category\nfields:\n- image\n- instance_mask\n- instance_category\nbatch_transform: false\n03b_preprocessing:\n_target_: ocl.transforms.SimpleTransform\ntransforms:\nimage:\n_target_: torchvision.transforms.Compose\ntransforms:\n- _target_: torchvision.transforms.ToTensor\n- _target_: torchvision.transforms.Resize\nsize: 128\ninterpolation: ${torchvision_interpolation_mode:BILINEAR}\n- _target_: torchvision.transforms.CenterCrop\nsize: 128\n- _target_: torchvision.transforms.Normalize\nmean: [0.5, 0.5, 0.5]\nstd: [0.5, 0.5, 0.5]\ninstance_mask:\n_target_: torchvision.transforms.Compose\ntransforms:\n- _target_: ocl.preprocessing.DenseMaskToTensor\n- _target_: ocl.preprocessing.ResizeNearestExact\nsize: 128\n- _target_: torchvision.transforms.CenterCrop\nsize: 128\nsegmentation_mask:\n_target_: torchvision.transforms.Compose\ntransforms:\n- _target_: ocl.preprocessing.DenseMaskToTensor\n- _target_: ocl.preprocessing.ResizeNearestExact\nsize: 128\n- _target_: torchvision.transforms.CenterCrop\nsize: 128\nbatch_transform: false\ntrain_transforms:\n03b_preprocessing:\n_target_: ocl.transforms.SimpleTransform\ntransforms:\nimage:\n_target_: torchvision.transforms.Compose\ntransforms:\n- _target_: torchvision.transforms.ToTensor\n- _target_: torchvision.transforms.Resize\nsize: 128\ninterpolation: ${torchvision_interpolation_mode:BILINEAR}\n- _target_: torchvision.transforms.CenterCrop\nsize: 128\n- _target_: torchvision.transforms.RandomHorizontalFlip\n- _target_: torchvision.transforms.Normalize\nmean: [0.5, 0.5, 0.5]\nstd: [0.5, 0.5, 0.5]\nbatch_transform: false\n</code></pre>"},{"location":"configs/experiment/slate/_preprocessing_voc2012_trainaug/","title":"configs/experiment/slate/_preprocessing_voc2012_trainaug.yaml","text":"<pre><code># @package _global_\ndataset:\neval_transforms:\n02a_format_consistency:\n_target_: ocl.transforms.SimpleTransform\ntransforms:\n# Convert to one-hot encoding.\nsegmentation-instance:\n_target_: ocl.preprocessing.IntegerToOneHotMask\nbatch_transform: false\n02b_format_consistency:\n_target_: ocl.transforms.Map\ntransform:\n_target_: torchvision.transforms.Compose\ntransforms:\n# Create segmentation mask.\n- _target_: ocl.preprocessing.VOCInstanceMasksToDenseMasks\ninstance_mask_key: segmentation-instance\nclass_mask_key: segmentation-class\nclasses_key: instance_category\n- _target_: ocl.preprocessing.RenameFields\nmapping:\nsegmentation-instance: instance_mask\nfields:\n- segmentation-instance\n- segmentation-class\n- image\nbatch_transform: false\n03a_preprocessing:\n_target_: ocl.transforms.Map\ntransform:\n_target_: torchvision.transforms.Compose\ntransforms:\n# This is not needed for VOC.\n# - _target_: ocl.preprocessing.InstanceMasksToDenseMasks\n- _target_: ocl.preprocessing.AddSegmentationMaskFromInstanceMask\n# Drop instance_category again as some images do not contain it\n- \"${lambda_fn:'lambda data: {k: v for k, v in data.items() if k != \\\"instance_category\\\"\\\n}'}\"\n- _target_: ocl.preprocessing.AddEmptyMasks\nmask_keys:\n- instance_mask\n- segmentation_mask\nfields:\n- image\n- instance_mask\n- instance_category\nbatch_transform: false\n03b_preprocessing:\n_target_: ocl.transforms.SimpleTransform\ntransforms:\nimage:\n_target_: torchvision.transforms.Compose\ntransforms:\n- _target_: torchvision.transforms.ToTensor\n- _target_: torchvision.transforms.Resize\nsize: 128\ninterpolation: ${torchvision_interpolation_mode:BILINEAR}\n- _target_: torchvision.transforms.CenterCrop\nsize: 128\n- _target_: torchvision.transforms.Normalize\nmean: [0.5, 0.5, 0.5]\nstd: [0.5, 0.5, 0.5]\ninstance_mask:\n_target_: torchvision.transforms.Compose\ntransforms:\n- _target_: ocl.preprocessing.DenseMaskToTensor\n- _target_: ocl.preprocessing.ResizeNearestExact\nsize: 128\n- _target_: torchvision.transforms.CenterCrop\nsize: 128\nsegmentation_mask:\n_target_: torchvision.transforms.Compose\ntransforms:\n- _target_: ocl.preprocessing.DenseMaskToTensor\n- _target_: ocl.preprocessing.ResizeNearestExact\nsize: 128\n- _target_: torchvision.transforms.CenterCrop\nsize: 128\nbatch_transform: false\ntrain_transforms:\n03b_preprocessing:\n_target_: ocl.transforms.SimpleTransform\ntransforms:\nimage:\n_target_: torchvision.transforms.Compose\ntransforms:\n- _target_: torchvision.transforms.ToTensor\n- _target_: torchvision.transforms.Resize\nsize: 128\ninterpolation: ${torchvision_interpolation_mode:BILINEAR}\n- _target_: torchvision.transforms.RandomCrop\nsize: 128\n- _target_: torchvision.transforms.RandomHorizontalFlip\n- _target_: torchvision.transforms.Normalize\nmean: [0.5, 0.5, 0.5]\nstd: [0.5, 0.5, 0.5]\nbatch_transform: false\n</code></pre>"},{"location":"configs/experiment/slate/clevr6/","title":"configs/experiment/slate/clevr6.yaml","text":"<pre><code># @package _global_\n# Configuration to exactly reproduce unsupervised object recognition of the original SLATE\n# paper.\ndefaults:\n- /experiment/slate/_base  # (1)!\n- /dataset: clevr6  # (2)!\n- /experiment/slate/_clevr_preprocessing # (3)!\n- /experiment/slate/_metrics_clevr_patch # (4)!\n- _self_\n# The following parameters assume training on 8 GPUs, leading to an effective batch size of 64.\ntrainer:\ndevices: 8\nmax_steps: 50000\nmax_epochs:\ndataset:\nnum_workers: 4\nbatch_size: 9\nmodels:\nconditioning:\nn_slots: 11\nmasks_as_image:\n_target_: routed.ocl.utils.resizing.Resize\ninput_path: object_decoder.masks\nsize: 128\nresize_mode: bilinear\npatch_mode: true\n</code></pre> <ol> <li>/experiment/slate/_base</li> <li>/dataset/clevr6</li> <li>/experiment/slate/_clevr_preprocessing</li> <li>/experiment/slate/_metrics_clevr_patch</li> </ol>"},{"location":"configs/experiment/slate/coco/","title":"configs/experiment/slate/coco.yaml","text":"<pre><code># @package _global_\ndefaults:\n- /experiment/slate/_base_large  # (1)!\n- /dataset: coco  # (2)!\n- /experiment/slate/_preprocessing_coco # (3)!\n- /experiment/slate/_metrics_masks # (4)!\n- _self_\ntrainer:\ndevices: 8\nmax_steps: 250000\nmax_epochs:\ngradient_clip_val: 1.0\ndataset:\nnum_workers: 4\nbatch_size: 8\nmodels:\nconditioning:\nn_slots: 11\n</code></pre> <ol> <li>/experiment/slate/_base_large</li> <li>/dataset/coco</li> <li>/experiment/slate/_preprocessing_coco</li> <li>/experiment/slate/_metrics_masks</li> </ol>"},{"location":"configs/experiment/slate/movi_c/","title":"configs/experiment/slate/movi_c.yaml","text":"<pre><code># @package _global_\n# Configuration to exactly reproduce unsupervised object recognition of the original SLATE\n# paper.\ndefaults:\n- /experiment/slate/_base  # (1)!\n- /dataset: movi_c_image  # (2)!\n- /experiment/slate/_movi_preprocessing # (3)!\n- /experiment/slate/_metrics_clevr_patch # (4)!\n- _self_\ntrainer:\ndevices: 8\nmax_steps: 200000\nmax_epochs:\ndataset:\nnum_workers: 8\nbatch_size: 8\nmodels:\nconditioning:\nn_slots: 11\nobject_decoder:\ndecoder:\nn_layers: 8\nn_heads: 4\nmasks_path: perceptual_grouping.feature_attributions\nperceptual_grouping:\niters: 7\nmasks_as_image:\n_target_: routed.ocl.utils.resizing.Resize\ninput_path: object_decoder.masks\nsize: 128\nresize_mode: bilinear\npatch_mode: true\n</code></pre> <ol> <li>/experiment/slate/_base</li> <li>/dataset/movi_c_image</li> <li>/experiment/slate/_movi_preprocessing</li> <li>/experiment/slate/_metrics_clevr_patch</li> </ol>"},{"location":"configs/experiment/slate/movi_e/","title":"configs/experiment/slate/movi_e.yaml","text":"<pre><code># @package _global_\n# Configuration to exactly reproduce unsupervised object recognition of the original SLATE\n# paper.\ndefaults:\n- /experiment/slate/_base  # (1)!\n- /dataset: movi_e_image  # (2)!\n- /experiment/slate/_movi_preprocessing # (3)!\n- /experiment/slate/_metrics_clevr_patch # (4)!\n- _self_\ntrainer:\ndevices: 8\nmax_steps: 200000\nmax_epochs:\ndataset:\nnum_workers: 8\nbatch_size: 8\nmodels:\nconditioning:\nn_slots: 24\nobject_decoder:\ndecoder:\nn_layers: 8\nn_heads: 4\nmasks_path: perceptual_grouping.feature_attributions\nperceptual_grouping:\niters: 7\nmasks_as_image:\n_target_: routed.ocl.utils.resizing.Resize\ninput_path: object_decoder.masks\nsize: 128\nresize_mode: bilinear\npatch_mode: true\n</code></pre> <ol> <li>/experiment/slate/_base</li> <li>/dataset/movi_e_image</li> <li>/experiment/slate/_movi_preprocessing</li> <li>/experiment/slate/_metrics_clevr_patch</li> </ol>"},{"location":"configs/experiment/slate/voc2012_trainaug/","title":"configs/experiment/slate/voc2012_trainaug.yaml","text":"<pre><code># @package _global_\ndefaults:\n- /experiment/slate/_base_large  # (1)!\n- /dataset: voc2012_trainaug  # (2)!\n- /experiment/slate/_preprocessing_voc2012_trainaug # (3)!\n- /experiment/slate/_metrics_masks # (4)!\n- _self_\ntrainer:\ndevices: 8\nmax_steps: 400000\nmax_epochs:\ngradient_clip_val: 1.0\ncheck_val_every_n_epoch: 50\ndataset:\nnum_workers: 4\nbatch_size: 8\nmodels:\nconditioning:\nn_slots: 7\n</code></pre> <ol> <li>/experiment/slate/_base_large</li> <li>/dataset/voc2012_trainaug</li> <li>/experiment/slate/_preprocessing_voc2012_trainaug</li> <li>/experiment/slate/_metrics_masks</li> </ol>"},{"location":"configs/experiment/slot_attention/_base/","title":"configs/experiment/slot_attention/_base.yaml","text":"<pre><code># @package _global_\n# Default parameters for slot attention.\ndefaults:\n- /experiment/_output_path  # (1)!\n- /training_config # (2)!\n- _self_\nmodels:\nfeature_extractor:\n_target_: routed.ocl.feature_extractors.SlotAttentionFeatureExtractor\nvideo_path: input.image\nconditioning:\n_target_: routed.ocl.conditioning.RandomConditioning\nobject_dim: 64\nbatch_size_path: input.batch_size\nperceptual_grouping:\n_target_: routed.ocl.perceptual_grouping.SlotAttentionGrouping\nfeature_dim: 64\nobject_dim: ${..conditioning.object_dim}\nkvq_dim: 128\npositional_embedding:\n_target_: ocl.neural_networks.wrappers.Sequential\n_args_:\n- _target_: ocl.neural_networks.positional_embedding.SoftPositionEmbed\nn_spatial_dims: 2\nfeature_dim: 64\n- _target_: ocl.neural_networks.build_two_layer_mlp\ninput_dim: 64\noutput_dim: 64\nhidden_dim: 128\ninitial_layer_norm: true\nresidual: false\nff_mlp:\n_target_: ocl.neural_networks.build_two_layer_mlp\ninput_dim: 64\noutput_dim: 64\nhidden_dim: 128\ninitial_layer_norm: true\nresidual: true\nfeature_path: feature_extractor\nconditioning_path: conditioning\nobject_decoder:\n_target_: routed.ocl.decoding.SlotAttentionDecoder\nobject_features_path: perceptual_grouping.objects\ndecoder:\n_target_: ocl.decoding.get_slotattention_decoder_backbone\nobject_dim: ${models.perceptual_grouping.object_dim}\npositional_embedding:\n_target_: ocl.neural_networks.positional_embedding.SoftPositionEmbed\nn_spatial_dims: 2\nfeature_dim: ${models.perceptual_grouping.object_dim}\ncnn_channel_order: true\nlosses:\nmse:\n_target_: routed.ocl.losses.ReconstructionLoss\nloss_type: mse_sum\ninput_path: object_decoder.reconstruction\ntarget_path: input.image\nvisualizations:\ninput:\n_target_: routed.ocl.visualizations.Image\ndenormalization: \"${lambda_fn:'lambda t: t * 0.5 + 0.5'}\"\nimage_path: input.image\nreconstruction:\n_target_: routed.ocl.visualizations.Image\ndenormalization: ${..input.denormalization}\nimage_path: object_decoder.reconstruction\nobjects:\n_target_: routed.ocl.visualizations.VisualObject\ndenormalization: ${..input.denormalization}\nobject_path: object_decoder.object_reconstructions\nmask_path: object_decoder.masks\npred_segmentation:\n_target_: routed.ocl.visualizations.Segmentation\ndenormalization: ${..input.denormalization}\nimage_path: input.image\nmask_path: object_decoder.masks\noptimizers:\nopt0:\n_target_: ocl.optimization.OptimizationWrapper\noptimizer:\n_target_: torch.optim.Adam\n_partial_: true\nlr: 0.0004\nlr_scheduler:\n_target_: ocl.scheduling.exponential_decay_after_optional_warmup\n_partial_: true\ndecay_rate: 0.5\ndecay_steps: 100000\nwarmup_steps: 10000\n</code></pre> <ol> <li>/experiment/_output_path</li> <li>/training_config</li> </ol>"},{"location":"configs/experiment/slot_attention/_base_large/","title":"configs/experiment/slot_attention/_base_large.yaml","text":"<pre><code># @package _global_\n# Default parameters for slot attention on resolution 128x128 with a ResNet encoder\ndefaults:\n- /experiment/_output_path  # (1)!\n- /training_config # (2)!\n- _self_\nmodels:\nconditioning:\nfeature_extractor:\n_target_: routed.ocl.feature_extractors.TimmFeatureExtractor\nmodel_name: resnet34_savi\nfeature_level: 4\npretrained: false\nfreeze: false\nvideo_path: input.image\nperceptual_grouping:\n_target_: routed.ocl.perceptual_grouping.SlotAttentionGrouping\nfeature_dim: ${models.perceptual_grouping.object_dim}\nobject_dim: ${models.conditioning.object_dim}\nkvq_dim: ${models.perceptual_grouping.object_dim}\npositional_embedding:\n_target_: ocl.neural_networks.wrappers.Sequential\n_args_:\n- _target_: ocl.neural_networks.positional_embedding.SoftPositionEmbed\nn_spatial_dims: 2\nfeature_dim: 512\nsavi_style: true\n- _target_: ocl.neural_networks.build_two_layer_mlp\ninput_dim: 512\noutput_dim: ${models.perceptual_grouping.object_dim}\nhidden_dim: ${models.perceptual_grouping.object_dim}\ninitial_layer_norm: true\nff_mlp:\n_target_: ocl.neural_networks.build_two_layer_mlp\ninput_dim: ${models.perceptual_grouping.object_dim}\noutput_dim: ${models.perceptual_grouping.object_dim}\nhidden_dim: \"${eval_lambda:'lambda dim: 2 * dim', ${.input_dim}}\"\ninitial_layer_norm: true\nresidual: true\nfeature_path: feature_extractor\nconditioning_path: conditioning\nobject_decoder:\n_target_: routed.ocl.decoding.SlotAttentionDecoder\nfinal_activation: tanh\ndecoder:\n_target_: ocl.decoding.get_savi_decoder_backbone\nobject_dim: ${models.perceptual_grouping.object_dim}\nlarger_input_arch: true\nchannel_multiplier: 1\npositional_embedding:\n_target_: ocl.neural_networks.positional_embedding.SoftPositionEmbed\nn_spatial_dims: 2\nfeature_dim: ${models.perceptual_grouping.object_dim}\ncnn_channel_order: true\nsavi_style: true\nobject_features_path: perceptual_grouping.objects\nlosses:\nmse:\n_target_: routed.ocl.losses.ReconstructionLoss\nloss_type: mse\ninput_path: object_decoder.reconstruction\ntarget_path: input.image\nvisualizations:\ninput:\n_target_: routed.ocl.visualizations.Image\ndenormalization: \"${lambda_fn:'lambda t: t * 0.5 + 0.5'}\"\nimage_path: input.image\nreconstruction:\n_target_: routed.ocl.visualizations.Image\ndenormalization: ${..input.denormalization}\nimage_path: object_decoder.reconstruction\nobjects:\n_target_: routed.ocl.visualizations.VisualObject\ndenormalization: ${..input.denormalization}\nobject_path: object_decoder.object_reconstructions\nmask_path: object_decoder.masks\npred_segmentation:\n_target_: routed.ocl.visualizations.Segmentation\ndenormalization: ${..input.denormalization}\nimage_path: input.image\nmask_path: object_decoder.masks\noptimizers:\nopt0:\n_target_: ocl.optimization.OptimizationWrapper\noptimizer:\n_target_: torch.optim.Adam\n_partial_: true\nlr: 0.0002\nlr_scheduler:\n_target_: ocl.scheduling.cosine_annealing_with_optional_warmup\n_partial_: true\nwarmup_steps: 2500\nT_max: ${trainer.max_steps}\n</code></pre> <ol> <li>/experiment/_output_path</li> <li>/training_config</li> </ol>"},{"location":"configs/experiment/slot_attention/_base_optical_flow/","title":"configs/experiment/slot_attention/_base_optical_flow.yaml","text":"<pre><code># @package _global_\n# Default parameters for slot attention.\ndefaults:\n- /experiment/_output_path  # (1)!\n- /training_config # (2)!\n- _self_\nmodels:\nfeature_extractor:\n_target_: routed.ocl.feature_extractors.SlotAttentionFeatureExtractor\nvideo_path: input.image\nconditioning:\n_target_: routed.ocl.conditioning.RandomConditioning\nobject_dim: 64\nbatch_size_path: input.batch_size\nperceptual_grouping:\n_target_: routed.ocl.perceptual_grouping.SlotAttentionGrouping\nfeature_dim: 64\nobject_dim: ${..conditioning.object_dim}\nkvq_dim: 128\npositional_embedding:\n_target_: ocl.neural_networks.wrappers.Sequential\n_args_:\n- _target_: ocl.neural_networks.positional_embedding.SoftPositionEmbed\nn_spatial_dims: 2\nfeature_dim: 64\n- _target_: ocl.neural_networks.build_two_layer_mlp\ninput_dim: 64\noutput_dim: 64\nhidden_dim: 128\ninitial_layer_norm: true\nresidual: false\nff_mlp:\n_target_: ocl.neural_networks.build_two_layer_mlp\ninput_dim: 64\noutput_dim: 64\nhidden_dim: 128\ninitial_layer_norm: true\nresidual: true\nfeature_path: feature_extractor\nconditioning_path: conditioning\nobject_decoder:\n_target_: routed.ocl.decoding.SlotAttentionOpticalFlowDecoder\nobject_features_path: perceptual_grouping.objects\ndecoder:\n_target_: ocl.decoding.get_slotattention_decoder_backbone\nobject_dim: ${models.perceptual_grouping.object_dim}\noutput_dim: 3  # Optical flow dim = 2, alpha mask dim = 1.\npositional_embedding:\n_target_: ocl.neural_networks.positional_embedding.SoftPositionEmbed\nn_spatial_dims: 2\nfeature_dim: ${models.perceptual_grouping.object_dim}\ncnn_channel_order: true\n# TODO(flwenzel): add object masks (also for visualizations).\nevaluation_metrics:\nari:\n_target_: routed.ocl.metrics.ARIMetric\nprediction_path: object_decoder.masks\ntarget_path: input.mask\nlosses:\nmse:\n_target_: routed.ocl.losses.ReconstructionLoss\nloss_type: mse_sum\ninput_path: object_decoder.predicted_flow\ntarget_path: input.backward_flow\nvisualizations:\ninput:\n_target_: routed.ocl.visualizations.Image\ndenormalization: \"${lambda_fn:'lambda t: t * 0.5 + 0.5'}\"\nimage_path: input.image\ninput_flow:\n_target_: routed.ocl.visualizations.Flow\nflow_path: input.backward_flow\npredicted_flow:\n_target_: routed.ocl.visualizations.Flow\nflow_path: object_decoder.predicted_flow\nobjects:\n_target_: routed.ocl.visualizations.VisualObject\nobject_path: object_decoder.object_flows\nmask_path: object_decoder.masks\noptimizers:\nopt0:\n_target_: ocl.optimization.OptimizationWrapper\noptimizer:\n_target_: torch.optim.Adam\n_partial_: true\nlr: 0.0004\nlr_scheduler:\n_target_: ocl.scheduling.exponential_decay_after_optional_warmup\n_partial_: true\ndecay_rate: 0.5\ndecay_steps: 100000\nwarmup_steps: 10000\n</code></pre> <ol> <li>/experiment/_output_path</li> <li>/training_config</li> </ol>"},{"location":"configs/experiment/slot_attention/_metrics_clevr/","title":"configs/experiment/slot_attention/_metrics_clevr.yaml","text":"<pre><code># @package _global_\n# Metrics for CLEVR-like datasets\nevaluation_metrics:\nari:\n_target_: routed.ocl.metrics.ARIMetric\nprediction_path: object_decoder.masks\ntarget_path: input.mask\nabo:\n_target_: routed.ocl.metrics.AverageBestOverlapMetric\nprediction_path: object_decoder.masks\ntarget_path: input.mask\nignore_background: true\n</code></pre>"},{"location":"configs/experiment/slot_attention/_metrics_coco/","title":"configs/experiment/slot_attention/_metrics_coco.yaml","text":"<pre><code># @package _global_\n# Metrics for COCO-like datasets\nevaluation_metrics:\ninstance_mask_ari:\n_target_: routed.ocl.metrics.ARIMetric\nprediction_path: object_decoder.masks\ntarget_path: input.instance_mask\nforeground: false\nignore_overlaps: true\nconvert_target_one_hot: true\ninstance_mask_iou:\n_target_: routed.ocl.metrics.UnsupervisedMaskIoUMetric\nprediction_path: object_decoder.masks\ntarget_path: input.instance_mask\nignore_overlaps: true\nsegmentation_mask_iou:\n_target_: routed.ocl.metrics.UnsupervisedMaskIoUMetric\nprediction_path: object_decoder.masks\ntarget_path: input.segmentation_mask\ninstance_mask_abo:\n_target_: routed.ocl.metrics.AverageBestOverlapMetric\nprediction_path: object_decoder.masks\ntarget_path: input.instance_mask\nignore_overlaps: true\nsegmentation_mask_abo:\n_target_: routed.ocl.metrics.AverageBestOverlapMetric\nprediction_path: object_decoder.masks\ntarget_path: input.segmentation_mask\ninstance_mask_corloc:\n_target_: routed.ocl.metrics.MaskCorLocMetric\nprediction_path: object_decoder.masks\ntarget_path: input.instance_mask\nuse_threshold: false\nignore_overlaps: true\n</code></pre>"},{"location":"configs/experiment/slot_attention/_preprocessing_cater/","title":"configs/experiment/slot_attention/_preprocessing_cater.yaml","text":"<pre><code># @package _global_\ndataset:\ntrain_transforms:\n03_preprocessing:\n_target_: ocl.transforms.SimpleTransform\ntransforms:\nimage:\n_target_: torchvision.transforms.Compose\ntransforms:\n- _target_: torchvision.transforms.ToTensor\n- _target_: torchvision.transforms.Resize\nsize: 128\n- _target_: torchvision.transforms.Normalize\nmean: [0.5, 0.5, 0.5]\nstd: [0.5, 0.5, 0.5]\nbatch_transform: false\neval_transforms:\n03_preprocessing:\n_target_: ocl.transforms.SimpleTransform\ntransforms:\nimage:\n_target_: torchvision.transforms.Compose\ntransforms:\n- _target_: torchvision.transforms.ToTensor\n- _target_: torchvision.transforms.Resize\nsize: 128\n- _target_: torchvision.transforms.Normalize\nmean: [0.5, 0.5, 0.5]\nstd: [0.5, 0.5, 0.5]\nmask:\n_target_: torchvision.transforms.Compose\ntransforms:\n- _target_: ocl.preprocessing.MultiMaskToTensor\n- _target_: ocl.preprocessing.ResizeNearestExact\nsize: 128\nbatch_transform: false\n</code></pre>"},{"location":"configs/experiment/slot_attention/_preprocessing_clevr/","title":"configs/experiment/slot_attention/_preprocessing_clevr.yaml","text":"<pre><code># @package _global_\ndataset:\ntrain_transforms:\n03_preprocessing:\n_target_: ocl.transforms.SimpleTransform\ntransforms:\nimage:\n_target_: torchvision.transforms.Compose\ntransforms:\n- _target_: torchvision.transforms.ToTensor\n- _target_: torchvision.transforms.CenterCrop\nsize: [192, 192]\n- _target_: torchvision.transforms.Resize\nsize: 128\n- _target_: torchvision.transforms.Normalize\nmean: [0.5, 0.5, 0.5]\nstd: [0.5, 0.5, 0.5]\nbatch_transform: false\neval_transforms:\n03_preprocessing:\n_target_: ocl.transforms.SimpleTransform\ntransforms:\nimage:\n_target_: torchvision.transforms.Compose\ntransforms:\n- _target_: torchvision.transforms.ToTensor\n- _target_: torchvision.transforms.CenterCrop\nsize: [192, 192]\n- _target_: torchvision.transforms.Resize\nsize: 128\n- _target_: torchvision.transforms.Normalize\nmean: [0.5, 0.5, 0.5]\nstd: [0.5, 0.5, 0.5]\nmask:\n_target_: torchvision.transforms.Compose\ntransforms:\n- _target_: ocl.preprocessing.MaskToTensor\n- _target_: torchvision.transforms.CenterCrop\nsize: [192, 192]\n- _target_: ocl.preprocessing.ResizeNearestExact\nsize: 128\nbatch_transform: false\n</code></pre>"},{"location":"configs/experiment/slot_attention/_preprocessing_coco/","title":"configs/experiment/slot_attention/_preprocessing_coco.yaml","text":"<pre><code># @package _global_\ndataset:\neval_transforms:\n03a_preprocessing:\n_target_: ocl.transforms.Map\ntransform:\n_target_: torchvision.transforms.Compose\ntransforms:\n- _target_: ocl.preprocessing.InstanceMasksToDenseMasks\n- _target_: ocl.preprocessing.AddSegmentationMaskFromInstanceMask\n- _target_: ocl.preprocessing.AddEmptyMasks\nmask_keys:\n- instance_mask\n- segmentation_mask\n# Drop instance_category again as some images do not contain it\n- _target_: ocl.preprocessing.DropEntries\nkeys:\n- instance_category\nfields:\n- image\n- instance_mask\n- instance_category\nbatch_transform: false\n03b_preprocessing:\n_target_: ocl.transforms.SimpleTransform\ntransforms:\nimage:\n_target_: torchvision.transforms.Compose\ntransforms:\n- _target_: torchvision.transforms.ToTensor\n- _target_: torchvision.transforms.Resize\nsize: 128\ninterpolation: ${torchvision_interpolation_mode:BILINEAR}\n- _target_: torchvision.transforms.CenterCrop\nsize: 128\n- _target_: torchvision.transforms.Normalize\nmean: [0.5, 0.5, 0.5]\nstd: [0.5, 0.5, 0.5]\ninstance_mask:\n_target_: torchvision.transforms.Compose\ntransforms:\n- _target_: ocl.preprocessing.DenseMaskToTensor\n- _target_: ocl.preprocessing.ResizeNearestExact\nsize: 128\n- _target_: torchvision.transforms.CenterCrop\nsize: 128\nsegmentation_mask:\n_target_: torchvision.transforms.Compose\ntransforms:\n- _target_: ocl.preprocessing.DenseMaskToTensor\n- _target_: ocl.preprocessing.ResizeNearestExact\nsize: 128\n- _target_: torchvision.transforms.CenterCrop\nsize: 128\nbatch_transform: false\ntrain_transforms:\n03b_preprocessing:\n_target_: ocl.transforms.SimpleTransform\ntransforms:\nimage:\n_target_: torchvision.transforms.Compose\ntransforms:\n- _target_: torchvision.transforms.ToTensor\n- _target_: torchvision.transforms.Resize\nsize: 128\ninterpolation: ${torchvision_interpolation_mode:BILINEAR}\n- _target_: torchvision.transforms.RandomCrop\nsize: 128\n- _target_: torchvision.transforms.RandomHorizontalFlip\n- _target_: torchvision.transforms.Normalize\nmean: [0.5, 0.5, 0.5]\nstd: [0.5, 0.5, 0.5]\nbatch_transform: false\n</code></pre>"},{"location":"configs/experiment/slot_attention/_preprocessing_movi/","title":"configs/experiment/slot_attention/_preprocessing_movi.yaml","text":"<pre><code># @package _global_\ndataset:\ntrain_transforms:\n03_preprocessing:\n_target_: ocl.transforms.SimpleTransform\ntransforms:\nimage:\n_target_: torchvision.transforms.Compose\ntransforms:\n- _target_: torchvision.transforms.ToTensor\n- _target_: torchvision.transforms.Resize\nsize: 128\n- _target_: torchvision.transforms.Normalize\nmean: [0.5, 0.5, 0.5]\nstd: [0.5, 0.5, 0.5]\nbatch_transform: false\neval_transforms:\n03_preprocessing:\n_target_: ocl.transforms.SimpleTransform\ntransforms:\nimage:\n_target_: torchvision.transforms.Compose\ntransforms:\n- _target_: torchvision.transforms.ToTensor\n- _target_: torchvision.transforms.Resize\nsize: 128\n- _target_: torchvision.transforms.Normalize\nmean: [0.5, 0.5, 0.5]\nstd: [0.5, 0.5, 0.5]\nmask:\n_target_: torchvision.transforms.Compose\ntransforms:\n- _target_: ocl.preprocessing.MultiMaskToTensor\n- _target_: ocl.preprocessing.ResizeNearestExact\nsize: 128\nbatch_transform: false\n</code></pre>"},{"location":"configs/experiment/slot_attention/_preprocessing_voc2012_trainaug/","title":"configs/experiment/slot_attention/_preprocessing_voc2012_trainaug.yaml","text":"<pre><code># @package _global_\ndataset:\neval_transforms:\n02a_format_consistency:\n_target_: ocl.transforms.SimpleTransform\ntransforms:\n# Convert to one-hot encoding.\nsegmentation-instance:\n_target_: ocl.preprocessing.IntegerToOneHotMask\nbatch_transform: false\n02b_format_consistency:\n_target_: ocl.transforms.Map\ntransform:\n_target_: torchvision.transforms.Compose\ntransforms:\n# Create segmentation mask.\n- _target_: ocl.preprocessing.VOCInstanceMasksToDenseMasks\ninstance_mask_key: segmentation-instance\nclass_mask_key: segmentation-class\nclasses_key: instance_category\n- _target_: ocl.preprocessing.RenameFields\nmapping:\nsegmentation-instance: instance_mask\nfields:\n- segmentation-instance\n- segmentation-class\n- image\nbatch_transform: false\n03a_preprocessing:\n_target_: ocl.transforms.Map\ntransform:\n_target_: torchvision.transforms.Compose\ntransforms:\n# This is not needed for VOC.\n# - _target_: ocl.preprocessing.InstanceMasksToDenseMasks\n- _target_: ocl.preprocessing.AddSegmentationMaskFromInstanceMask\n# Drop instance_category again as some images do not contain it\n- \"${lambda_fn:'lambda data: {k: v for k, v in data.items() if k != \\\"instance_category\\\"\\\n}'}\"\n- _target_: ocl.preprocessing.AddEmptyMasks\nmask_keys:\n- instance_mask\n- segmentation_mask\nfields:\n- image\n- instance_mask\n- instance_category\nbatch_transform: false\n03b_preprocessing:\n_target_: ocl.transforms.SimpleTransform\ntransforms:\nimage:\n_target_: torchvision.transforms.Compose\ntransforms:\n- _target_: torchvision.transforms.ToTensor\n- _target_: torchvision.transforms.Resize\nsize: 128\ninterpolation: ${torchvision_interpolation_mode:BILINEAR}\n- _target_: torchvision.transforms.CenterCrop\nsize: 128\n- _target_: torchvision.transforms.Normalize\nmean: [0.5, 0.5, 0.5]\nstd: [0.5, 0.5, 0.5]\ninstance_mask:\n_target_: torchvision.transforms.Compose\ntransforms:\n- _target_: ocl.preprocessing.DenseMaskToTensor\n- _target_: ocl.preprocessing.ResizeNearestExact\nsize: 128\n- _target_: torchvision.transforms.CenterCrop\nsize: 128\nsegmentation_mask:\n_target_: torchvision.transforms.Compose\ntransforms:\n- _target_: ocl.preprocessing.DenseMaskToTensor\n- _target_: ocl.preprocessing.ResizeNearestExact\nsize: 128\n- _target_: torchvision.transforms.CenterCrop\nsize: 128\nbatch_transform: false\ntrain_transforms:\n03b_preprocessing:\n_target_: ocl.transforms.SimpleTransform\ntransforms:\nimage:\n_target_: torchvision.transforms.Compose\ntransforms:\n- _target_: torchvision.transforms.ToTensor\n- _target_: torchvision.transforms.Resize\nsize: 128\ninterpolation: ${torchvision_interpolation_mode:BILINEAR}\n- _target_: torchvision.transforms.RandomCrop\nsize: 128\n- _target_: torchvision.transforms.RandomHorizontalFlip\n- _target_: torchvision.transforms.Normalize\nmean: [0.5, 0.5, 0.5]\nstd: [0.5, 0.5, 0.5]\nbatch_transform: false\n</code></pre>"},{"location":"configs/experiment/slot_attention/cater10/","title":"configs/experiment/slot_attention/cater10.yaml","text":"<pre><code># @package _global_\n# Configuration to exactly reproduce unsupervised object recognition of the original slot attention\n# paper.\ndefaults:\n- /experiment/slot_attention/_base  # (1)!\n- /dataset: cater_image  # (2)!\n- /experiment/slot_attention/_preprocessing_cater # (3)!\n- /experiment/slot_attention/_metrics_clevr # (4)!\n- _self_\ntrainer:\ndevices: 8\nmax_steps: 500000\nmax_epochs:\ndataset:\nnum_workers: 4\nbatch_size: 8\nmodels:\nconditioning:\nn_slots: 11\n</code></pre> <ol> <li>/experiment/slot_attention/_base</li> <li>/dataset/cater_image</li> <li>/experiment/slot_attention/_preprocessing_cater</li> <li>/experiment/slot_attention/_metrics_clevr</li> </ol>"},{"location":"configs/experiment/slot_attention/cater6/","title":"configs/experiment/slot_attention/cater6.yaml","text":"<pre><code># @package _global_\n# Configuration to exactly reproduce unsupervised object recognition of the original slot attention\n# paper.\ndefaults:\n- /experiment/slot_attention/_base  # (1)!\n- /dataset: cater6_image  # (2)!\n- /experiment/slot_attention/_preprocessing_cater # (3)!\n- /experiment/slot_attention/_metrics_clevr # (4)!\n- _self_\ntrainer:\ndevices: 8\nmax_steps: 500000\nmax_epochs:\ndataset:\nnum_workers: 4\nbatch_size: 8\nmodels:\nconditioning:\nn_slots: 7\n</code></pre> <ol> <li>/experiment/slot_attention/_base</li> <li>/dataset/cater6_image</li> <li>/experiment/slot_attention/_preprocessing_cater</li> <li>/experiment/slot_attention/_metrics_clevr</li> </ol>"},{"location":"configs/experiment/slot_attention/clevr10/","title":"configs/experiment/slot_attention/clevr10.yaml","text":"<pre><code># @package _global_\ndefaults:\n- /experiment/slot_attention/_base  # (1)!\n- /dataset: clevr  # (2)!\n- /experiment/slot_attention/_preprocessing_clevr # (3)!\n- /experiment/slot_attention/_metrics_clevr # (4)!\n- _self_\n# The following parameters assume training on 8 GPUs, leading to an effective batch size of 64.\ntrainer:\ndevices: 8\nmax_steps: 500000\nmax_epochs:\ndataset:\nnum_workers: 4\nbatch_size: 8\nmodels:\nconditioning:\nn_slots: 11\n</code></pre> <ol> <li>/experiment/slot_attention/_base</li> <li>/dataset/clevr</li> <li>/experiment/slot_attention/_preprocessing_clevr</li> <li>/experiment/slot_attention/_metrics_clevr</li> </ol>"},{"location":"configs/experiment/slot_attention/clevr10_lds/","title":"configs/experiment/slot_attention/clevr10_lds.yaml","text":"<pre><code># @package _global_\ndefaults:\n- /experiment/slot_attention/clevr10 # (1)!\n- _self_\nlosses:\nlds:\n_target_: routed.ocl.losses.LatentDupplicateSuppressionLoss\nweight: 500.0\ngrouping_path: perceptual_grouping\n</code></pre> <ol> <li>/experiment/slot_attention/clevr10</li> </ol>"},{"location":"configs/experiment/slot_attention/clevr6/","title":"configs/experiment/slot_attention/clevr6.yaml","text":"<pre><code># @package _global_\n# Configuration to exactly reproduce unsupervised object recognition of the original slot attention\n# paper.\ndefaults:\n- /experiment/slot_attention/_base  # (1)!\n- /dataset: clevr6  # (2)!\n- /experiment/slot_attention/_preprocessing_clevr # (3)!\n- /experiment/slot_attention/_metrics_clevr # (4)!\n- _self_\n# The following parameters assume training on 8 GPUs, leading to an effective batch size of 64.\ntrainer:\ndevices: 8\nmax_steps: 500000\nmax_epochs:\ndataset:\nnum_workers: 4\nbatch_size: 8\nmodels:\nconditioning:\nn_slots: 7\n</code></pre> <ol> <li>/experiment/slot_attention/_base</li> <li>/dataset/clevr6</li> <li>/experiment/slot_attention/_preprocessing_clevr</li> <li>/experiment/slot_attention/_metrics_clevr</li> </ol>"},{"location":"configs/experiment/slot_attention/coco/","title":"configs/experiment/slot_attention/coco.yaml","text":"<pre><code># @package _global_\ndefaults:\n- /experiment/slot_attention/_base_large  # (1)!\n- /dataset: coco  # (2)!\n- /experiment/slot_attention/_preprocessing_coco # (3)!\n- /experiment/slot_attention/_metrics_coco # (4)!\n- _self_\ntrainer:\ndevices: 8\nmax_steps: 500000\nmax_epochs:\ngradient_clip_val: 1.0\ndataset:\nnum_workers: 4\nbatch_size: 8\nmodels:\nconditioning:\n_target_: routed.ocl.conditioning.RandomConditioning\nobject_dim: 256\nn_slots: 11\nbatch_size_path: input.batch_size\n</code></pre> <ol> <li>/experiment/slot_attention/_base_large</li> <li>/dataset/coco</li> <li>/experiment/slot_attention/_preprocessing_coco</li> <li>/experiment/slot_attention/_metrics_coco</li> </ol>"},{"location":"configs/experiment/slot_attention/movi_c/","title":"configs/experiment/slot_attention/movi_c.yaml","text":"<pre><code># @package _global_\ndefaults:\n- /experiment/slot_attention/_base_large  # (1)!\n- /dataset: movi_c_image  # (2)!\n- /experiment/slot_attention/_preprocessing_movi # (3)!\n- /experiment/slot_attention/_metrics_clevr # (4)!\n- _self_\ntrainer:\ndevices: 8\nmax_steps: 500000\nmax_epochs:\ngradient_clip_val: 1.0\ndataset:\nnum_workers: 4\nbatch_size: 8\nmodels:\nconditioning:\n_target_: routed.ocl.conditioning.RandomConditioning\nobject_dim: 256\nn_slots: 11\nbatch_size_path: input.batch_size\n</code></pre> <ol> <li>/experiment/slot_attention/_base_large</li> <li>/dataset/movi_c_image</li> <li>/experiment/slot_attention/_preprocessing_movi</li> <li>/experiment/slot_attention/_metrics_clevr</li> </ol>"},{"location":"configs/experiment/slot_attention/movi_c_optical_flow/","title":"configs/experiment/slot_attention/movi_c_optical_flow.yaml","text":"<pre><code># @package _global_\n# Configuration to exactly reproduce unsupervised object recognition of the original slot attention\n# paper.\ndefaults:\n- /experiment/slot_attention/_base_optical_flow  # (1)!\n- /dataset: movi_c_image  # (2)!\n- /experiment/slot_attention/_preprocessing_movi # (3)!\n- _self_\ntrainer:\ndevices: 8\nmax_steps: 500000\nmax_epochs:\ndataset:\nnum_workers: 4\nbatch_size: 8\n# Additionally sample frames from flow_backward.\ntrain_transforms:\n02_sample_frames:\nfields: [image, backward_flow]\n03_preprocessing:\ntransforms:\n# Addtionally preprocess backward_flow.\n# TODO(flwenzel): Normalize flow?\nbackward_flow:\n_target_: torchvision.transforms.Compose\ntransforms:\n- _target_: ocl.preprocessing.FlowToTensor\n- _target_: torchvision.transforms.Resize\nsize: 128\neval_transforms:\n02_sample_frames:\nfields: [image, mask, backward_flow]\n03_preprocessing:\ntransforms:\n# Addtionally preprocess backward_flow.\nbackward_flow:\n_target_: torchvision.transforms.Compose\ntransforms:\n- _target_: ocl.preprocessing.FlowToTensor\n- _target_: torchvision.transforms.Resize\nsize: 128\nmodels:\nconditioning:\nn_slots: 7\n</code></pre> <ol> <li>/experiment/slot_attention/_base_optical_flow</li> <li>/dataset/movi_c_image</li> <li>/experiment/slot_attention/_preprocessing_movi</li> </ol>"},{"location":"configs/experiment/slot_attention/movi_e/","title":"configs/experiment/slot_attention/movi_e.yaml","text":"<pre><code># @package _global_\ndefaults:\n- /experiment/slot_attention/_base_large  # (1)!\n- /dataset: movi_e_image  # (2)!\n- /experiment/slot_attention/_preprocessing_movi # (3)!\n- /experiment/slot_attention/_metrics_clevr # (4)!\n- _self_\ntrainer:\ndevices: 8\nmax_steps: 500000\nmax_epochs:\ngradient_clip_val: 1.0\ndataset:\nnum_workers: 4\nbatch_size: 8\nmodels:\nconditioning:\n_target_: routed.ocl.conditioning.RandomConditioning\nobject_dim: 256\nn_slots: 24\nbatch_size_path: input.batch_size\n</code></pre> <ol> <li>/experiment/slot_attention/_base_large</li> <li>/dataset/movi_e_image</li> <li>/experiment/slot_attention/_preprocessing_movi</li> <li>/experiment/slot_attention/_metrics_clevr</li> </ol>"},{"location":"configs/experiment/slot_attention/voc2012_trainaug/","title":"configs/experiment/slot_attention/voc2012_trainaug.yaml","text":"<pre><code># @package _global_\ndefaults:\n- /experiment/slot_attention/_base_large  # (1)!\n- /dataset: voc2012_trainaug  # (2)!\n- /experiment/slot_attention/_preprocessing_voc2012_trainaug # (3)!\n- /experiment/slot_attention/_metrics_coco # (4)!\n- _self_\ntrainer:\ndevices: 8\nmax_steps: 500000\nmax_epochs:\ngradient_clip_val: 1.0\ncheck_val_every_n_epoch: 50\ndataset:\nnum_workers: 4\nbatch_size: 8\nmodels:\nconditioning:\n_target_: routed.ocl.conditioning.RandomConditioning\nobject_dim: 256\nn_slots: 7\nbatch_size_path: input.batch_size\n</code></pre> <ol> <li>/experiment/slot_attention/_base_large</li> <li>/dataset/voc2012_trainaug</li> <li>/experiment/slot_attention/_preprocessing_voc2012_trainaug</li> <li>/experiment/slot_attention/_metrics_coco</li> </ol>"},{"location":"tutorial/","title":"Tutorial","text":"<p>This tutorial should give a brief overview on how to install OCLF, use it to run experiments and how to create your own experiments within OCLF.</p>"},{"location":"tutorial/datasets/","title":"Datasets","text":"<p>Most datasets are not suited for large-scale multi-GPU and multi-node training by default, as they are either composed of many small files.  In OCLF datasets are generally stored in the webdataset format which bundles together files into tar archives.  In order to use the unmodified configurations in oclf it is thus required to first convert the datasets into this format which is explained below.</p>"},{"location":"tutorial/datasets/#webdatasets-torchdata-datapipes","title":"Webdatasets / torchdata datapipes","text":"<p>We provide scripts to easily download and convert the datasets used in the codebase into the right format. These scripts can be found in the subfolder <code>scripts/datasets</code> and come with their own set of dependencies. We recommend to install these is a separate virtual environment as they will not be needed for training models but solely for the dataset generation.  When poetry is installed you can simply install the dependencies using using the following commands:</p> <pre><code>cd scripts/datasets\npoetry install\n</code></pre> Installation issues with <code>pycocotools</code> <p>If you encounter any issue when installing <code>pycocotools</code> please be sure to have a compiler (for instance <code>gcc</code>) and the python development headers for your python version installed.  On rhel based systems this would look something like this:</p> <pre><code>sudo yum install gcc python-devel\n</code></pre> <p>If the python version you are using is installed via pyenv you might need to set environment variables such as <code>CPATH</code> for the correct headers to be found.</p> <p>After the dependencies are installed you can convert a dataset by calling the <code>download_and_convert.sh</code> script with the name of the dataset you would like to convert.  It will download the necessary data to <code>data</code> and store converted versions of the data in the <code>output</code> directory.  For example, to create a webdataset version of the <code>movi_c</code> dataset run the following command:</p> <pre><code>bash download_and_convert.sh movi_c\n</code></pre>"},{"location":"tutorial/datasets/#changing-the-dataset-path","title":"Changing the dataset path","text":"<p>As shown below, the dataset configurations instantiate a WebdatasetDataModule with the parameters <code>train_shards</code>, <code>train_size</code>, <code>val_shards</code>, etc..</p> configs/dataset/clevr.yaml<pre><code># Image dataset CLEVR based on https://github.com/deepmind/multi_object_datasets .\n_target_: ocl.datasets.WebdatasetDataModule\ntrain_shards: \"${oc.env:DATASET_PREFIX}/clevr_with_masks/train/shard-{000000..000114}.tar\"\ntrain_size: 70000\nval_shards: \"${oc.env:DATASET_PREFIX}/clevr_with_masks/val/shard-{000000..000024}.tar\"\nval_size: 15000\ntest_shards: \"${oc.env:DATASET_PREFIX}/clevr_with_masks/test/shard-{000000..000024}.tar\"\ntest_size: 15000\n</code></pre> <p>The shard paths contain a resolver <code>${oc.env:DATASET_PREFIX}</code> which looks up the value of the environment variable <code>DATASET_PREFIX</code>.</p> <p>Thus, by setting the environment variable <code>DATASET_PREFIX</code> the path from which the datasets are read can be changed. WebdatasetDataModule supports multiple protocols for reading data from cloud storage or from local paths.  For further information take a look at torchdata.datapipes.iter.FSSpecFileOpener.</p> <p>Alternatively, you can of course create your own configuration file for the dataset or replace the path of one of the predefined datasets directly in the existing configuration files.  For more information on using your own configuration files, see Configuration.</p>"},{"location":"tutorial/datasets/#custom-datasets","title":"Custom datasets","text":"<p>OCLF command line tools are invariant to the exact dataset specification, thus you can define your own dataset by implementing a pytorch lightning datamodule.</p> <p>After creating your own datamodule you can simply add a dataset to OCLF by creating an appropriate configuration files.  Please check out Configuration for further information.</p>"},{"location":"tutorial/datasets/#dataset-transforms","title":"Dataset transforms","text":"<p>Preprocessing steps are applied using dataset Transforms which operate on the level of a pytorch data IterDataPipe.  The transforms are provided to the WebdatasetDataModule via the constructor arguments <code>train_transforms</code> and <code>eval_transforms</code>.  In OCLF each IterDataPipe yields (potentially nested) dictionaries where each represents an individual element or batch of the dataset. Further, each transform takes a IterDataPipe as input to their call method and returns a IterDataPipe.  Check out Transforms to see which transformations are available.</p> <p>Importantly, each Transform has a property <code>is_batch_transform</code> which determines if it should be applied prior to or after batching. This determines if the input will be a dictionary containing a single element or a dictionary with concatenated tensors from multiple elements.  In general, it is beneficial for performance to use batch transformation whenever possible.</p> <p>Often, a transformation does not need to be applied to the whole dictionary, but only to a single input element (for instance an image) of the input dict. Code that cover such functionality is stored in ocl.preprocessing.  Generally, any standard preprocessing function can be used when combined with the right transform.  For instance, if the input image should be converted to a tensor, resized and normalized the following configuration will do the trick:</p> <pre><code>dataset:\ntrain_transforms:\nresize_and_normalize:\n_target_: ocl.transforms.SimpleTransform\ntransforms:\nimage:\n_target_: torchvision.transforms.Compose\ntransforms:\n- _target_: torchvision.transforms.ToTensor\n- _target_: torchvision.transforms.Resize\nsize: 128\n- _target_: torchvision.transforms.Normalize\nmean: [0.5, 0.5, 0.5]\nstd: [0.5, 0.5, 0.5]\nbatch_transform: false\n</code></pre>"},{"location":"tutorial/evaluation/","title":"Evaluation","text":""},{"location":"tutorial/experiments/","title":"Experiments","text":"<p>A experiment is a configuration that is applied to the global configuration tree by adding <code># @package _global_</code> to the beginning of the configuration file.  A experiment is thus intended to define dataset, model, losses and metrics that should be used during a training run.  The options which can be configured in a training run are defined in the base configuration training_config and shown below for convenience.</p>"},{"location":"tutorial/experiments/#ocl.cli.train.TrainingConfig","title":"<code>ocl.cli.train.TrainingConfig</code>  <code>dataclass</code>","text":"<p>Configuration of a training run.</p> <p>For losses, metrics and visualizations it can be of use to utilize the routed module as these are simply provided with a dictionary of all model inputs and outputs.</p> <p>Attributes:</p> Name Type Description <code>dataset</code> <code>Any</code> <p>The pytorch lightning datamodule that will be used for training</p> <code>models</code> <code>Any</code> <p>Either a dictionary of torch.nn.Modules which will be interpreted as a Combined model or a torch.nn.Module itself that accepts a dictionary as input.</p> <code>optimizers</code> <code>Dict[str, Any]</code> <p>Dictionary of functools.partial wrapped optimizers or OptimizationWrapper instances</p> <code>losses</code> <code>Dict[str, Any]</code> <p>Dict of callables that return scalar values which will be summed to compute a total loss.  Typically should contain routed versions of callables.</p> <code>visualizations</code> <code>Dict[str, Any]</code> <p>Dictionary of visualizations.  Typically should contain routed versions of visualizations.</p> <code>trainer</code> <code>TrainerConf</code> <p>Pytorch lightning trainer</p> <code>training_vis_frequency</code> <code>Optional[int]</code> <p>Number of optimization steps between generation and storage of visualizations.</p> <code>training_metrics</code> <code>Optional[Dict[str, Any]]</code> <p>Dictionary of torchmetrics that should be used to log training progress. Typically should contain routed versions of torchmetrics.</p> <code>evaluation_metrics</code> <code>Optional[Dict[str, Any]]</code> <p>Dictionary of torchmetrics that should be used to log progress on evaluation splits of the data.  Typically should contain routed versions of Torchmetrics.</p> <code>load_checkpoint</code> <code>Optional[str]</code> <p>Path to checkpoint file that should be loaded prior to starting training.</p> <code>seed</code> <code>Optional[int]</code> <p>Seed used to ensure reproducability.</p> <code>experiment</code> <code>Dict[str, Any]</code> <p>Dictionary with arbitrary additional information.  Useful when building configurations as it can be used as central point for a single parameter that might influence multiple model components.</p> Source code in <code>ocl/cli/train.py</code> <pre><code>@dataclasses.dataclass\nclass TrainingConfig:\n\"\"\"Configuration of a training run.\n    For losses, metrics and visualizations it can be of use to utilize the\n    [routed][] module as these are simply provided with a dictionary of all\n    model inputs and outputs.\n    Attributes:\n        dataset: The pytorch lightning datamodule that will be used for training\n        models: Either a dictionary of [torch.nn.Module][]s which will be interpreted\n            as a [Combined][ocl.utils.routing.Combined] model or a [torch.nn.Module][] itself\n            that accepts a dictionary as input.\n        optimizers: Dictionary of [functools.partial][] wrapped optimizers or\n            [OptimizationWrapper][ocl.optimization.OptimizationWrapper] instances\n        losses: Dict of callables that return scalar values which will be summed to\n            compute a total loss.  Typically should contain [routed][] versions of callables.\n        visualizations: Dictionary of [visualizations][ocl.visualizations].  Typically\n            should contain [routed][] versions of visualizations.\n        trainer: Pytorch lightning trainer\n        training_vis_frequency: Number of optimization steps between generation and\n            storage of visualizations.\n        training_metrics: Dictionary of torchmetrics that should be used to log training progress.\n            Typically should contain [routed][] versions of torchmetrics.\n        evaluation_metrics: Dictionary of torchmetrics that should be used to log progress on\n            evaluation splits of the data.  Typically should contain [routed][] versions of\n            Torchmetrics.\n        load_checkpoint: Path to checkpoint file that should be loaded prior to starting training.\n        seed: Seed used to ensure reproducability.\n        experiment: Dictionary with arbitrary additional information.  Useful when building\n            configurations as it can be used as central point for a single parameter that might\n            influence multiple model components.\n    \"\"\"\ndataset: Any\nmodels: Any  # When provided with dict wrap in `utils.Combined`, otherwise interpret as model.\noptimizers: Dict[str, Any]\nlosses: Dict[str, Any]\nvisualizations: Dict[str, Any] = dataclasses.field(default_factory=dict)\ntrainer: TrainerConf = dataclasses.field(default_factory=lambda: TrainerConf())\ntraining_vis_frequency: Optional[int] = None\ntraining_metrics: Optional[Dict[str, Any]] = None\nevaluation_metrics: Optional[Dict[str, Any]] = None\nload_checkpoint: Optional[str] = None\nseed: Optional[int] = None\nexperiment: Dict[str, Any] = dataclasses.field(default_factory=lambda: {\"callbacks\": {}})\n</code></pre>"},{"location":"tutorial/experiments/#using-routed-classes","title":"Using routed classes","text":"<p>Some elements of the training config (especially, losses, metrics and visualizations) expect dictionary elements to be able to handle a whole dictionary that contains all information of the forward pass.  Instead of coding up this support explicitly in your metric and loss implementations, it is recommended to used routed subclasses of your code.  This is allows using external code for example from pytorch or torchmetrics.  Below you see an example of this</p> configs/experiments/my_test_experiment.yaml<pre><code>training_metrics:\nclassification_accuracy:\n_target_: routed.torchmetrics.BinaryAccuracy\npreds_path: my_model.prediction\ntarget_path: inputs.target\nlosses:\nbce:\n_target_: routed.torch.nn.BCEWithLogitsLoss\ninput_path: my_model.prediction\ntarget_path: inputs.target\n</code></pre> <p>For further information take a look at Models/How does this work? and the routed module.</p>"},{"location":"tutorial/experiments/#creating-your-own-experiments-example","title":"Creating your own experiments - Example","text":"<p>Below an example of how it looks to adapt an existing experiment configuration /experiment/slot_attention/movi_c to additionally reconstruct an optical flow signal.</p> configs/experiment/examples/composition.yaml"},{"location":"tutorial/installation/","title":"Installation","text":"<p>You can install OCLF either in a development setup using poetry or as a dependency in your own project via pip.  As OCLF relies heavily on configuration files for running experiments it is easiest to start with a development setup as this allows configuration files to be inspected and edited in place for rapid prototyping.  In contrast, if you want to keep your configurations separate from OCLF a installation of OCLF as a dependency might be the better way to go for you.</p>"},{"location":"tutorial/installation/#development-installation","title":"Development installation","text":"<p>Installing OCLF requires at least python3.8. Installation can be done using poetry.  After installing <code>poetry</code>, check out the repo and setup a development environment:</p> <pre><code>git clone https://github.com/amazon-science/object-centric-learning-framework.git\ncd object-centric-learning-framework\npoetry install # Optionally add -E &lt;extra&gt; for each extra that should be installed\n</code></pre> <p>Valid extras are <code>timm</code> for access to timm models for feature extraction and <code>clip</code> for access to OpenAI's clip model.  For instance <code>poetry install -E timm -E clip</code> installs both.</p> <p>Poetry will create a separate virtual environment where the projects dependencies are installed.  It can be accessed using <code>poetry shell</code> or <code>poetry run</code>.  Please see the poetry docs for further information on using poetry.</p>"},{"location":"tutorial/installation/#installation-as-a-dependency","title":"Installation as a dependency","text":"<p>It is also possible to install OCLF as a dependency for your project via pip for this simply run</p> <pre><code>pip3 install \"git+https://github.com/amazon-science/object-centric-learning-framework.git\"\n</code></pre> <p>this might take a while as pip tries to resolve the dependencies specified in the OCLF <code>pyproject.toml</code> file whereas poetry directly uses locked dependencies which were determined in a previous run and added to the repository.</p> <p>With OCLF installed as a dependency you cannot directly explore configurations that are part of OCLF or edit them. Nevertheless, you can access OCLF components via the python API and run experiments using by adding your own configurations.  For further information check out Usage/Separate codebase.</p>"},{"location":"tutorial/models/","title":"Models","text":"<p>A model in OCLF can be defined in one of two ways:</p> <ol> <li>A Combined model which is initialized using a     dict</li> <li>A code model which works similar to a regular pytorch model familiar from     other frameworks</li> </ol>"},{"location":"tutorial/models/#combined-model","title":"Combined model","text":"<p>Combined models are different from how models are typically written in pytorch. The main difference is that routing of information is not performed in code, but instead in the configuration.  This is useful if you might need access to different inputs dependent on the exact submodule you are using.</p> <p>For example, assume you want to train slot attention on images, but instead of using the typical random initialization for slots you would like to condition each slot on the center of mass of each object (similar to what is done in SAVi). In this case it would be necessary to either create special handling in the main model for it to understand which type of conditioning module you are using (random or bbox-based conditioning) and then forward the correct inputs to the module.  This introduces unnecessary dependencies between the code of the model and the conditioning approach used which we would ideally like to avoid.</p> <p>Additionally, models defined in code cannot be composed.  Thus if a representation should be used for multiple prediction endpoints, these need to be defined in code.  If new endpoints are added the code needs to be changed and additional clauses for handling subsets of the functionality need to be introduced.</p> <p>Combined models simply specify individual modules or parts of a model as entries in a dictionary.  For instance (below uses hydras instantiate notation):</p> Example model<pre><code>models:\n_target_: ocl.utils.routing.Combined\nfeature_extractor:\n_target_: routed.my.feature.extractor\nmy_parameter_1: test\ninput_path: input.image\ngrouping:\n_target_: routed.my.grouping.module\nmy_parameter_2: \"some other value\"\ninput_features_path: feature_extractor\n</code></pre> <p>Which translates into python code similar to:</p> <pre><code>models = ocl.utils.routing.Combined(\nfeature_extractor=routed.my_module.TestModule(\nmy_parameter=\"test\",\ninput_path=\"input.image\"\n),\ngrouping=routed.my.grouping.module(\nmy_parameter_2=\"some other value\",\ninput_features_path=\"feature_extractor\"\n)\n)\n</code></pre>"},{"location":"tutorial/models/#how-does-this-work","title":"How does this work?","text":"<p>The Combined model will go through the two modules (<code>feature_extractor</code> and <code>grouping</code>) in order, execute them and store the return value of each module under the key same key as the module itself in a dictionary.  This dictionary is initialized with one entry <code>input</code>, which contains the input data and is updated with additional values as the modules are executed.</p> <p>Yet, how do the modules access the right inputs? This possible due to the magic of the routed package which automatically subclasses any module being imported from it's path and adds routing parameters to its constructor.  In particular, it examines the signature of the <code>forward</code>, <code>update</code> or <code>__call__</code> method of the class that should be routed and adds <code>&lt;method_argument_name&gt;_path</code> to the constructor of the class.  This allows the routed version of the class to simply be called using dictionary that is being expanded with each module call.  Thus each module will be able to access outputs of modules which have been called before itself.  The path is dot-separated and internally uses the get_tree_element implementation to derive elements from the dict.  It thus is also possible to select elements from nested dictionaries, lists, or even dataclasses.  Please check out the documentation of routed for more information.</p>"},{"location":"tutorial/models/#recurrent-model-components","title":"Recurrent model components","text":"<p>If parts of the model need to be applied individually over time the special Recurrent module can be used to implement this. It allows defining the model components that should be applied over time (or in fact any other axis) and takes as arguments the axis along which the input should be sliced, which input tensors should be sliced and how the initial input should be constructed.</p> <p>To access the output of the previous iteration, the entry <code>previous_output</code> can be used.  An example of applying the Recurrent can be found in /configs/experiment/SAVi/cater.yaml.</p> <p>Of course, alternatively the functionality of handling higher dimensional data can also be implemented in the module itself.</p>"},{"location":"tutorial/models/#regular-pytorch-model","title":"Regular pytorch model","text":"<p>A model can also be defined in a similarly to regular pytorch models by implementing (potentially only parts of) the routing in code.  One such example is shown in ocl.models.savi.SAVi.  Here the model simply accepts the input dictionary as input and internally routes information however desired.</p>"},{"location":"tutorial/usage/","title":"Usage","text":"<p>OCLF relies heavily on configuration via the hydra configuration framework and the command line interface of OCLF scripts supports all overrides that hydra supports.  Thus is is always possible to change any value that can be defined in the configuration.  As all models and experiments are constructed with this in mind it is essentially possible to override any parameter via the command line.  The basic syntax of overriding parameters is explained here.</p> <p>OCLF provides three command line scripts: ocl_train for training models, ocl_eval for evaluating them and ocl_compute_dataset_size for deriving the number of instances in a dataset for correct progress monitoring.</p> <p>Note</p> <p>The commands below assume you are running OCLF in a development installation and not as a package dependency.  They thus are prefixed with the <code>poetry run</code> command such that the command is run in the virtual environment of the project.  In a package dependency type installation this prefix is not needed.</p>"},{"location":"tutorial/usage/#running-a-simple-experiment","title":"Running a simple experiment","text":"<p>For training a model in OCLF a experiment configuration is needed which defines, model, dataset and training parameters.  Existing experiment configurations are stored in the <code>configs/experiment</code> path of the repository and can be selected using the command line.  For instance, in order to run an training experiment based on the experiment configuration file <code>configs/experiment/slot_attention/movi_c.yaml</code> we can call the following command:</p> <pre><code>poetry run ocl_train +experiment=slot_attention/movi_c\n</code></pre> <p>Using the cli we can change any parameter of this run. For instance we could use</p> <p><pre><code>poetry run ocl_train +experiment=slot_attention/movi_c dataset.batch_size=64\n</code></pre> to train with a larger batch size.</p> <p>In order to see all parameters that can be overwritten consult the experiment config or have a look at the complete configuration by adding the flag <code>--cfg</code> as shown below.</p> <pre><code>poetry run ocl_train +experiment=slot_attention/movi_c --cfg\n</code></pre>"},{"location":"tutorial/usage/#experiment-outputs","title":"Experiment outputs","text":"<p>For all configuration that are part of OCLF, the result is saved in a timestamped subdirectory in <code>outputs/&lt;experiment_name&gt;</code>, i.e. <code>outputs/slot_attention/movi_c/&lt;date&gt;_&lt;time&gt;</code> in the above case. The prefix path <code>outputs</code> can be configured using the <code>experiment.root_output_path</code> variable.  This behavior implemented in the <code>configs/experiment/_output_path.yaml</code>.</p>"},{"location":"tutorial/usage/#combining-configurations","title":"Combining configurations","text":"<p>Some settings might only be applicable in certain contexts, for instance when running a model on a cluster or when running a debug run.  These can simply be grouped into a configuration file in the <code>configs</code> folder.  One such example is shown below.</p> configs/tricks/debug.yaml<pre><code># @package _global_\n# Settings that implement a debug mode for quick testing locally on CPU.\ntrainer:\ndevices: 1\naccelerator: cpu\nfast_dev_run: 5  # Only run 5 batches.\ndataset:\nbatch_size: 2    # Use small batch size to speed things up.\nnum_workers: 0\nshuffle_buffer_size: 1\n</code></pre> <p>The previous call to run a training experiment can then be augmented to</p> <pre><code>poetry run ocl_train +experiment=slot_attention/movi_c +tricks=debug\n</code></pre>"},{"location":"tutorial/usage/#separate-codebase","title":"Separate codebase","text":"<p>When running OCLF cli tools installed as dependencies, one does not have direct access to the <code>configs</code> folder.  Nevertheless, it is possible to add your own configurations by additionally passing <code>--config-dir</code> on the command line. This will augment the provided configuration folder to the configuration search path.  You can thus still access configurations that are provided with OCLF. An example is shown below.</p> my_configs/tricks/debug.yaml<pre><code># @package _global_\n# Settings that implement a debug mode for quick testing locally on CPU.\ntrainer:\ndevices: 1\naccelerator: cpu\nfast_dev_run: 5  # Only run 5 batches.\ndataset:\nbatch_size: 2    # Use small batch size to speed things up.\nnum_workers: 0\nshuffle_buffer_size: 1\n</code></pre> <pre><code>poetry run ocl_train +experiment=slot_attention/movi_c +tricks=debug  --config-dir my_configs\n</code></pre>"}]}